{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arviz.data.base import generate_dims_coords\n",
    "from arviz.data import load_arviz_data\n",
    "from arviz.data.converters import convert_to_inference_data\n",
    "from line_profiler import LineProfiler\n",
    "import numpy as np\n",
    "from numpy import array, average, dot\n",
    "import numba\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import warnings\n",
    "from arviz.plots.kdeplot import _fast_kde_2d as f2\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import xarray as xr\n",
    "import timeit\n",
    "from scipy.signal import gaussian, convolve, convolve2d  # pylint: disable=no-name-in-module\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import OrderedDict,defaultdict\n",
    "from collections.abc import Sequence\n",
    "from copy import copy as ccopy, deepcopy\n",
    "from datetime import datetime\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000435 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/data/base.py\n",
      "Function: generate_dims_coords at line 30\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    30                                           def generate_dims_coords(shape, var_name, dims=None, coords=None, default_dims=None):\n",
      "    31                                               \"\"\"Generate default dimensions and coordinates for a variable.\n",
      "    32                                           \n",
      "    33                                               Parameters\n",
      "    34                                               ----------\n",
      "    35                                               shape : tuple[int]\n",
      "    36                                                   Shape of the variable\n",
      "    37                                               var_name : str\n",
      "    38                                                   Name of the variable. Used in the default name, if necessary\n",
      "    39                                               dims : list\n",
      "    40                                                   List of dimensions for the variable\n",
      "    41                                               coords : dict[str] -> list[str]\n",
      "    42                                                   Map of dimensions to coordinates\n",
      "    43                                               default_dims : list[str]\n",
      "    44                                                   Dimensions that do not apply to the variable's shape\n",
      "    45                                           \n",
      "    46                                               Returns\n",
      "    47                                               -------\n",
      "    48                                               list[str]\n",
      "    49                                                   Default dims\n",
      "    50                                               dict[str] -> list[str]\n",
      "    51                                                   Default coords\n",
      "    52                                               \"\"\"\n",
      "    53         1          6.0      6.0      1.4      if default_dims is None:\n",
      "    54         1          3.0      3.0      0.7          default_dims = []\n",
      "    55         1          2.0      2.0      0.5      if dims is None:\n",
      "    56         1          3.0      3.0      0.7          dims = []\n",
      "    57         1          9.0      9.0      2.1      if len([dim for dim in dims if dim not in default_dims]) > len(shape):\n",
      "    58                                                   warnings.warn(\n",
      "    59                                                       (\n",
      "    60                                                           \"In variable {var_name}, there are \"\n",
      "    61                                                           + \"more dims ({dims_len}) given than exist ({shape_len}). \"\n",
      "    62                                                           + \"Passed array should have shape (chains, draws, *shape)\"\n",
      "    63                                                       ).format(var_name=var_name, dims_len=len(dims), shape_len=len(shape)),\n",
      "    64                                                       SyntaxWarning,\n",
      "    65                                                   )\n",
      "    66         1          3.0      3.0      0.7      if coords is None:\n",
      "    67         1          3.0      3.0      0.7          coords = {}\n",
      "    68                                           \n",
      "    69         1         76.0     76.0     17.5      coords = deepcopy(coords)\n",
      "    70         1         26.0     26.0      6.0      dims = deepcopy(dims)\n",
      "    71                                           \n",
      "    72         4         12.0      3.0      2.8      for idx, dim_len in enumerate(shape):\n",
      "    73         3          8.0      2.7      1.8          if (len(dims) < idx + 1) or (dims[idx] is None):\n",
      "    74         3         22.0      7.3      5.1              dim_name = \"{var_name}_dim_{idx}\".format(var_name=var_name, idx=idx)\n",
      "    75         3          9.0      3.0      2.1              if len(dims) < idx + 1:\n",
      "    76         3          8.0      2.7      1.8                  dims.append(dim_name)\n",
      "    77                                                       else:\n",
      "    78                                                           dims[idx] = dim_name\n",
      "    79         3        121.0     40.3     27.8          dim_name = dims[idx]\n",
      "    80         3          9.0      3.0      2.1          if dim_name not in coords:\n",
      "    81         3         85.0     28.3     19.5              coords[dim_name] = np.arange(dim_len)\n",
      "    82         1         28.0     28.0      6.4      coords = {key: coord for key, coord in coords.items() if any(key == dim for dim in dims)}\n",
      "    83         1          2.0      2.0      0.5      return dims, coords\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(generate_dims_coords)\n",
    "wrapper((500,600,80), 'x')\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def range_(x):\n",
    "    return np.arange(x)\n",
    "\n",
    "\n",
    "def range_jit(x):\n",
    "    return np.arange(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 15.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.15 µs ± 4.96 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit range_(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 µs ± 165 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit range_jit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dims_coords(shape, var_name, dims=None, coords=None, default_dims=None):\n",
    "    if default_dims is None:\n",
    "        default_dims = []\n",
    "    if dims is None:\n",
    "        dims = []\n",
    "    if len([dim for dim in dims if dim not in default_dims]) > len(shape):\n",
    "        warnings.warn(\n",
    "            (\n",
    "                \"In variable {var_name}, there are \"\n",
    "                + \"more dims ({dims_len}) given than exist ({shape_len}). \"\n",
    "                + \"Passed array should have shape (chains, draws, *shape)\"\n",
    "            ).format(var_name=var_name, dims_len=len(dims), shape_len=len(shape)),\n",
    "            SyntaxWarning,\n",
    "        )\n",
    "    if coords is None:\n",
    "        coords = {}\n",
    "\n",
    "    coords = deepcopy(coords)\n",
    "    dims = deepcopy(dims)\n",
    "\n",
    "    for idx, dim_len in enumerate(shape):\n",
    "        if (len(dims) < idx + 1) or (dims[idx] is None):\n",
    "            dim_name = \"{var_name}_dim_{idx}\".format(var_name=var_name, idx=idx)\n",
    "            if len(dims) < idx + 1:\n",
    "                dims.append(dim_name)\n",
    "            else:\n",
    "                dims[idx] = dim_name\n",
    "        dim_name = dims[idx]\n",
    "        if dim_name not in coords:\n",
    "            coords[dim_name] = np.arange(dim_len)\n",
    "    coords = {key: coord for key, coord in coords.items() if any(key == dim for dim in dims)}\n",
    "    return dims, coords\n",
    "\n",
    "\n",
    "\n",
    "def generate_dims_coords_jit(shape, var_name, dims=None, coords=None, default_dims=None):\n",
    "    if default_dims is None:\n",
    "        default_dims = []\n",
    "    if dims is None:\n",
    "        dims = []\n",
    "    if len([dim for dim in dims if dim not in default_dims]) > len(shape):\n",
    "        warnings.warn(\n",
    "            (\n",
    "                \"In variable {var_name}, there are \"\n",
    "                + \"more dims ({dims_len}) given than exist ({shape_len}). \"\n",
    "                + \"Passed array should have shape (chains, draws, *shape)\"\n",
    "            ).format(var_name=var_name, dims_len=len(dims), shape_len=len(shape)),\n",
    "            SyntaxWarning,\n",
    "        )\n",
    "    if coords is None:\n",
    "        coords = {}\n",
    "\n",
    "    coords = deepcopy(coords)\n",
    "    dims = deepcopy(dims)\n",
    "    \n",
    "    for idx, dim_len in enumerate(shape):\n",
    "        if (len(dims) < idx + 1) or (dims[idx] is None):\n",
    "            dim_name = \"{var_name}_dim_{idx}\".format(var_name=var_name, idx=idx)\n",
    "            if len(dims) < idx + 1:\n",
    "                dims.append(dim_name)\n",
    "            else:\n",
    "                dims[idx] = dim_name\n",
    "        dim_name = dims[idx]\n",
    "        if dim_name not in coords:\n",
    "            coords[dim_name] = range_(dim_len)\n",
    "    coords = {key: coord for key, coord in coords.items() if any(key == dim for dim in dims)}\n",
    "    return dims, coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(forceobj=True)\n",
    "def jit_loop(dims,coords,shape,var_name):\n",
    "    for idx,dim_len in enumerate(shape):\n",
    "        if (len(dims)<idx+1) or (dims[idx] is None):\n",
    "            dim_name = \"{var_name}_dim_{idx}\".format(var_name=var_name, idx=idx)\n",
    "            if len(dims)<idx+1:\n",
    "                dims.append(dim_name)\n",
    "            else:\n",
    "                dims[idx] = dim_name\n",
    "        dim_name = dims[idx]\n",
    "        if dim_name not in coords:\n",
    "            coords[dim_name] = range_(dim_len)\n",
    "        return coords, dims\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.7 µs ± 4.57 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_dims_coords((10000,100,10,5), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.7 µs ± 4.51 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_dims_coords_jit((10000,100,10,5), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.5 µs ± 2.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_dims_coords((10,), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 µs ± 944 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit generate_dims_coords_jit((10,), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def one_de(x):\n",
    "    return np.atleast_1d(x)\n",
    "\n",
    "@numba.njit\n",
    "def two_de(x):\n",
    "    return np.atleast_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_data_array(ary, *, var_name=\"data\", coords=None, dims=None):\n",
    "    # manage and transform copies\n",
    "    default_dims = [\"chain\", \"draw\"]\n",
    "    ary = np.atleast_2d(ary)\n",
    "    n_chains, n_samples, *shape = ary.shape\n",
    "    if n_chains > n_samples:\n",
    "        warnings.warn(\n",
    "            \"More chains ({n_chains}) than draws ({n_samples}). \"\n",
    "            \"Passed array should have shape (chains, draws, *shape)\".format(\n",
    "                n_chains=n_chains, n_samples=n_samples\n",
    "            ),\n",
    "            SyntaxWarning,\n",
    "        )\n",
    "\n",
    "    dims, coords = generate_dims_coords(\n",
    "        shape, var_name, dims=dims, coords=coords, default_dims=default_dims\n",
    "    )\n",
    "\n",
    "    # reversed order for default dims: 'chain', 'draw'\n",
    "    if \"draw\" not in dims:\n",
    "        dims = [\"draw\"] + dims\n",
    "    if \"chain\" not in dims:\n",
    "        dims = [\"chain\"] + dims\n",
    "\n",
    "    if \"chain\" not in coords:\n",
    "        coords[\"chain\"] = np.arange(n_chains)\n",
    "    if \"draw\" not in coords:\n",
    "        coords[\"draw\"] = np.arange(n_samples)\n",
    "\n",
    "    # filter coords based on the dims\n",
    "    coords = {key: xr.IndexVariable((key,), data=coords[key]) for key in dims}\n",
    "    return xr.DataArray(ary, coords=coords, dims=dims)\n",
    "\n",
    "\n",
    "def numpy_to_data_array_jit(ary, *, var_name=\"data\", coords=None, dims=None):\n",
    "    # manage and transform copies\n",
    "    default_dims = [\"chain\", \"draw\"]\n",
    "    ary = two_de(ary)\n",
    "    n_chains, n_samples, *shape = ary.shape\n",
    "    if n_chains > n_samples:\n",
    "        warnings.warn(\n",
    "            \"More chains ({n_chains}) than draws ({n_samples}). \"\n",
    "            \"Passed array should have shape (chains, draws, *shape)\".format(\n",
    "                n_chains=n_chains, n_samples=n_samples\n",
    "            ),\n",
    "            SyntaxWarning,\n",
    "        )\n",
    "\n",
    "    dims, coords = generate_dims_coords(\n",
    "        shape, var_name, dims=dims, coords=coords, default_dims=default_dims\n",
    "    )\n",
    "\n",
    "    # reversed order for default dims: 'chain', 'draw'\n",
    "    if \"draw\" not in dims:\n",
    "        dims = [\"draw\"] + dims\n",
    "    if \"chain\" not in dims:\n",
    "        dims = [\"chain\"] + dims\n",
    "\n",
    "    if \"chain\" not in coords:\n",
    "        coords[\"chain\"] = range_(n_chains)\n",
    "    if \"draw\" not in coords:\n",
    "        coords[\"draw\"] = range_(n_samples)\n",
    "\n",
    "    # filter coords based on the dims\n",
    "    coords = {key: xr.IndexVariable((key,), data=coords[key]) for key in dims}\n",
    "    return xr.DataArray(ary, coords=coords, dims=dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000,100)\n",
    "linear = np.random.randn(1000000)\n",
    "small = np.random.randn(100,100)\n",
    "small_linear = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SyntaxWarning: More chains (10000) than draws (100). Passed array should have shape (chains, draws, *shape)\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497 µs ± 82.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: SyntaxWarning: More chains (10000) than draws (100). Passed array should have shape (chains, draws, *shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 µs ± 62.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array_jit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.69 ms ± 396 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array_jit(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 µs ± 5.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array(small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 µs ± 36.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array_jit(small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 µs ± 37.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array(small_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_to_data_array_jit(small_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very Similar Performance. Up for reconsideration'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Very Similar Performance. Up for reconsideration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to dataset bottleneck ---> numpy_to_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Converters'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Converters\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.002719 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/data/converters.py\n",
      "Function: convert_to_inference_data at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           def convert_to_inference_data(obj, *, group=\"posterior\", coords=None, dims=None, **kwargs):\n",
      "    17                                               r\"\"\"Convert a supported object to an InferenceData object.\n",
      "    18                                           \n",
      "    19                                               This function sends `obj` to the right conversion function. It is idempotent,\n",
      "    20                                               in that it will return arviz.InferenceData objects unchanged.\n",
      "    21                                           \n",
      "    22                                               Parameters\n",
      "    23                                               ----------\n",
      "    24                                               obj : dict, str, np.ndarray, xr.Dataset, pystan fit, pymc3 trace\n",
      "    25                                                   A supported object to convert to InferenceData:\n",
      "    26                                                       | InferenceData: returns unchanged\n",
      "    27                                                       | str: Attempts to load the cmdstan csv or netcdf dataset from disk\n",
      "    28                                                       | pystan fit: Automatically extracts data\n",
      "    29                                                       | cmdstanpy fit: Automatically extracts data\n",
      "    30                                                       | cmdstan csv-list: Automatically extracts data\n",
      "    31                                                       | pymc3 trace: Automatically extracts data\n",
      "    32                                                       | emcee sampler: Automatically extracts data\n",
      "    33                                                       | pyro MCMC: Automatically extracts data\n",
      "    34                                                       | xarray.Dataset: adds to InferenceData as only group\n",
      "    35                                                       | dict: creates an xarray dataset as the only group\n",
      "    36                                                       | numpy array: creates an xarray dataset as the only group, gives the\n",
      "    37                                                                    array an arbitrary name\n",
      "    38                                               group : str\n",
      "    39                                                   If `obj` is a dict or numpy array, assigns the resulting xarray\n",
      "    40                                                   dataset to this group. Default: \"posterior\".\n",
      "    41                                               coords : dict[str, iterable]\n",
      "    42                                                   A dictionary containing the values that are used as index. The key\n",
      "    43                                                   is the name of the dimension, the values are the index values.\n",
      "    44                                               dims : dict[str, List(str)]\n",
      "    45                                                   A mapping from variables to a list of coordinate names for the variable\n",
      "    46                                               kwargs\n",
      "    47                                                   Rest of the supported keyword arguments transferred to conversion function.\n",
      "    48                                           \n",
      "    49                                               Returns\n",
      "    50                                               -------\n",
      "    51                                               InferenceData\n",
      "    52                                               \"\"\"\n",
      "    53         1          3.0      3.0      0.1      kwargs[group] = obj\n",
      "    54         1          2.0      2.0      0.1      kwargs[\"coords\"] = coords\n",
      "    55         1          2.0      2.0      0.1      kwargs[\"dims\"] = dims\n",
      "    56                                           \n",
      "    57                                               # Cases that convert to InferenceData\n",
      "    58         1          5.0      5.0      0.2      if isinstance(obj, InferenceData):\n",
      "    59                                                   return obj\n",
      "    60         1          2.0      2.0      0.1      elif isinstance(obj, str):\n",
      "    61                                                   if obj.endswith(\".csv\"):\n",
      "    62                                                       if group == \"sample_stats\":\n",
      "    63                                                           kwargs[\"posterior\"] = kwargs.pop(group)\n",
      "    64                                                       elif group == \"sample_stats_prior\":\n",
      "    65                                                           kwargs[\"prior\"] = kwargs.pop(group)\n",
      "    66                                                       return from_cmdstan(**kwargs)\n",
      "    67                                                   else:\n",
      "    68                                                       return InferenceData.from_netcdf(obj)\n",
      "    69                                               elif (\n",
      "    70         1          3.0      3.0      0.1          obj.__class__.__name__ in {\"StanFit4Model\", \"RunSet\"}\n",
      "    71         1          4.0      4.0      0.1          or obj.__class__.__module__ == \"stan.fit\"\n",
      "    72                                               ):\n",
      "    73                                                   if group == \"sample_stats\":\n",
      "    74                                                       kwargs[\"posterior\"] = kwargs.pop(group)\n",
      "    75                                                   elif group == \"sample_stats_prior\":\n",
      "    76                                                       kwargs[\"prior\"] = kwargs.pop(group)\n",
      "    77                                                   if obj.__class__.__name__ == \"RunSet\":\n",
      "    78                                                       return from_cmdstanpy(**kwargs)\n",
      "    79                                                   else:  # pystan or pystan3\n",
      "    80                                                       return from_pystan(**kwargs)\n",
      "    81         1          2.0      2.0      0.1      elif obj.__class__.__name__ == \"MultiTrace\":  # ugly, but doesn't make PyMC3 a requirement\n",
      "    82                                                   return from_pymc3(trace=kwargs.pop(group), **kwargs)\n",
      "    83         1          2.0      2.0      0.1      elif obj.__class__.__name__ == \"EnsembleSampler\":  # ugly, but doesn't make emcee a requirement\n",
      "    84                                                   return from_emcee(sampler=kwargs.pop(group), **kwargs)\n",
      "    85         1          2.0      2.0      0.1      elif obj.__class__.__name__ == \"MCMC\" and obj.__class__.__module__.startswith(\"pyro\"):\n",
      "    86                                                   return from_pyro(posterior=kwargs.pop(group), **kwargs)\n",
      "    87                                           \n",
      "    88                                               # Cases that convert to xarray\n",
      "    89         1        102.0    102.0      3.8      if isinstance(obj, xr.Dataset):\n",
      "    90                                                   dataset = obj\n",
      "    91         1          3.0      3.0      0.1      elif isinstance(obj, dict):\n",
      "    92                                                   dataset = dict_to_dataset(obj, coords=coords, dims=dims)\n",
      "    93         1          3.0      3.0      0.1      elif isinstance(obj, np.ndarray):\n",
      "    94         1       2566.0   2566.0     94.4          dataset = dict_to_dataset({\"x\": obj}, coords=coords, dims=dims)\n",
      "    95                                               elif isinstance(obj, (list, tuple)) and isinstance(obj[0], str) and obj[0].endswith(\".csv\"):\n",
      "    96                                                   if group == \"sample_stats\":\n",
      "    97                                                       kwargs[\"posterior\"] = kwargs.pop(group)\n",
      "    98                                                   elif group == \"sample_stats_prior\":\n",
      "    99                                                       kwargs[\"prior\"] = kwargs.pop(group)\n",
      "   100                                                   return from_cmdstan(**kwargs)\n",
      "   101                                               else:\n",
      "   102                                                   allowable_types = (\n",
      "   103                                                       \"xarray dataset\",\n",
      "   104                                                       \"dict\",\n",
      "   105                                                       \"netcdf file\",\n",
      "   106                                                       \"numpy array\",\n",
      "   107                                                       \"pystan fit\",\n",
      "   108                                                       \"pymc3 trace\",\n",
      "   109                                                       \"emcee fit\",\n",
      "   110                                                       \"pyro mcmc fit\",\n",
      "   111                                                       \"cmdstan fit csv\",\n",
      "   112                                                       \"cmdstanpy fit\",\n",
      "   113                                                   )\n",
      "   114                                                   raise ValueError(\n",
      "   115                                                       \"Can only convert {} to InferenceData, not {}\".format(\n",
      "   116                                                           \", \".join(allowable_types), obj.__class__.__name__\n",
      "   117                                                       )\n",
      "   118                                                   )\n",
      "   119                                           \n",
      "   120         1         18.0     18.0      0.7      return InferenceData(**{group: dataset})\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/data/base.py:124: SyntaxWarning: More chains (10000) than draws (100). Passed array should have shape (chains, draws, *shape)\n",
      "  SyntaxWarning,\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(convert_to_inference_data)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck is dict to dataset. Refer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"DATASETS.PY'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"DATASETS.PY\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to improve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"io_dict\"\"'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"io_dict\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'\"\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck is dict to dataset. Refer above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"io_netcdf'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"io_netcdf\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottlenecks---->Inference data and convert_to_inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inference_data'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Inference_data\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceData:\n",
    "    \"\"\"Container for accessing netCDF files using xarray.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize InferenceData object from keyword xarray datasets.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        InferenceData(posterior=posterior, prior=prior)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        kwargs :\n",
    "            Keyword arguments of xarray datasets\n",
    "        \"\"\"\n",
    "        self._groups = []\n",
    "        for key, dataset in kwargs.items():\n",
    "            if dataset is None:\n",
    "                continue\n",
    "            elif not isinstance(dataset, xr.Dataset):\n",
    "                raise ValueError(\n",
    "                    \"Arguments to InferenceData must be xarray Datasets \"\n",
    "                    '(argument \"{}\" was type \"{}\")'.format(key, type(dataset))\n",
    "                )\n",
    "            setattr(self, key, dataset)\n",
    "            self._groups.append(key)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Make string representation of object.\"\"\"\n",
    "        return \"Inference data with groups:\\n\\t> {options}\".format(\n",
    "            options=\"\\n\\t> \".join(self._groups)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_netcdf(filename):\n",
    "        \"\"\"Initialize object from a netcdf file.\n",
    "\n",
    "        Expects that the file will have groups, each of which can be loaded by xarray.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            location of netcdf file\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        InferenceData object\n",
    "        \"\"\"\n",
    "        groups = {}\n",
    "        with nc.Dataset(filename, mode=\"r\") as data:\n",
    "            data_groups = list(data.groups)\n",
    "\n",
    "        for group in data_groups:\n",
    "            with xr.open_dataset(filename, group=group) as data:\n",
    "                groups[group] = data\n",
    "        return InferenceData(**groups)\n",
    "\n",
    "    def to_netcdf(self, filename, compress=True):\n",
    "        \"\"\"Write InferenceData to file using netcdf4.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            Location to write to\n",
    "        compress : bool\n",
    "            Whether to compress result. Note this saves disk space, but may make\n",
    "            saving and loading somewhat slower (default: True).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Location of netcdf file\n",
    "        \"\"\"\n",
    "        mode = \"w\"  # overwrite first, then append\n",
    "        if self._groups:  # check's whether a group is present or not.\n",
    "            for group in self._groups:\n",
    "                data = getattr(self, group)\n",
    "                kwargs = {}\n",
    "                if compress:\n",
    "                    kwargs[\"encoding\"] = {var_name: {\"zlib\": True} for var_name in data.variables}\n",
    "                data.to_netcdf(filename, mode=mode, group=group, **kwargs)\n",
    "                data.close()\n",
    "                mode = \"a\"\n",
    "        else:  # creates a netcdf file for an empty InferenceData object.\n",
    "            empty_netcdf_file = nc.Dataset(filename, mode=\"w\", format=\"NETCDF4\")\n",
    "            empty_netcdf_file.close()\n",
    "        return filename\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Concatenate two InferenceData objects.\"\"\"\n",
    "        return concat(self, other, copy=True, inplace=False)\n",
    "\n",
    "    def sel(self, inplace=True, **kwargs):\n",
    "        \"\"\"Perform an xarray selection on all groups.\n",
    "\n",
    "        Loops over all groups to perform Dataset.sel(key=item)\n",
    "        for every kwarg if key is a dimension of the dataset.\n",
    "        The selection is performed inplace.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inplace : bool\n",
    "            If True, modify the InferenceData object inplace, otherwise, return the modified copy.\n",
    "        **kwargs : mapping\n",
    "            It must be accepted by Dataset.sel()\n",
    "        \"\"\"\n",
    "        out = self if inplace else deepcopy(self)\n",
    "        for group in self._groups:\n",
    "            dataset = getattr(self, group)\n",
    "            valid_keys = set(kwargs.keys()).intersection(dataset.dims)\n",
    "            dataset = dataset.sel(**{key: kwargs[key] for key in valid_keys})\n",
    "            setattr(out, group, dataset)\n",
    "        if inplace:\n",
    "            return None\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "@numba.jit(forceobj=True)\n",
    "def concat(*args, dim=None, copy=True, inplace=False, reset_dim=True):\n",
    "    \"\"\"Concatenate InferenceData objects.\n",
    "\n",
    "    Concatenates over `group`, `chain` or `draw`.\n",
    "    By default concatenates over unique groups.\n",
    "    To concatenate over `chain` or `draw` function\n",
    "    needs identical groups and variables.\n",
    "\n",
    "    The `variables` in the `data` -group are merged if `dim` are not found.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : InferenceData\n",
    "        Variable length InferenceData list or\n",
    "        Sequence of InferenceData.\n",
    "    dim : str, optional\n",
    "        If defined, concatenated over the defined dimension.\n",
    "        Dimension which is concatenated. If None, concatenates over\n",
    "        unique groups.\n",
    "    copy : bool\n",
    "        If True, groups are copied to the new InferenceData object.\n",
    "        Used only if `dim` is None.\n",
    "    inplace : bool\n",
    "        If True, merge args to first object.\n",
    "    reset_dim : bool\n",
    "        Valid only if dim is not None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    InferenceData\n",
    "        A new InferenceData object by default.\n",
    "        When `inplace==True` merge args to first arg and return `None`\n",
    "    \"\"\"\n",
    "    # pylint: disable=undefined-loop-variable, too-many-nested-blocks\n",
    "    if len(args) == 0:\n",
    "        if inplace:\n",
    "            return\n",
    "        return InferenceData()\n",
    "\n",
    "    if len(args) == 1 and isinstance(args[0], Sequence):\n",
    "        args = args[0]\n",
    "\n",
    "    # assert that all args are InferenceData\n",
    "    for i, arg in enumerate(args):\n",
    "        if not isinstance(arg, InferenceData):\n",
    "            raise TypeError(\n",
    "                \"Concatenating is supported only\"\n",
    "                \"between InferenceData objects. Input arg {} is {}\".format(i, type(arg))\n",
    "            )\n",
    "\n",
    "    if dim is not None and dim.lower() not in {\"group\", \"chain\", \"draw\"}:\n",
    "        msg = \"Invalid `dim`: {}. Valid `dim` are {}\".format(dim, '{\"group\", \"chain\", \"draw\"}')\n",
    "        raise TypeError(msg)\n",
    "    dim = dim.lower() if dim is not None else dim\n",
    "\n",
    "    if len(args) == 1 and isinstance(args[0], InferenceData):\n",
    "        if inplace:\n",
    "            return None\n",
    "        else:\n",
    "            if copy:\n",
    "                return deepcopy(args[0])\n",
    "            else:\n",
    "                return args[0]\n",
    "\n",
    "    current_time = str(datetime.now())\n",
    "\n",
    "    if not inplace:\n",
    "        # Keep order for python 3.5\n",
    "        inference_data_dict = OrderedDict()\n",
    "\n",
    "    if dim is None:\n",
    "        arg0 = args[0]\n",
    "        arg0_groups = ccopy(arg0._groups)\n",
    "        args_groups = dict()\n",
    "        # check if groups are independent\n",
    "        # Concat over unique groups\n",
    "        for arg in args[1:]:\n",
    "            for group in arg._groups:\n",
    "                if group in args_groups or group in arg0_groups:\n",
    "                    msg = (\n",
    "                        \"Concatenating overlapping groups is not supported unless `dim` is defined.\"\n",
    "                    )\n",
    "                    msg += \" Valid dimensions are `chain` and `draw`.\"\n",
    "                    raise TypeError(msg)\n",
    "            group_data = getattr(arg, group)\n",
    "            args_groups[group] = deepcopy(group_data) if copy else group_data\n",
    "        # add arg0 to args_groups if inplace is False\n",
    "        if not inplace:\n",
    "            for group in arg0_groups:\n",
    "                group_data = getattr(arg0, group)\n",
    "                args_groups[group] = deepcopy(group_data) if copy else group_data\n",
    "\n",
    "        basic_order = [\n",
    "            \"posterior\",\n",
    "            \"posterior_predictive\",\n",
    "            \"sample_stats\",\n",
    "            \"prior\",\n",
    "            \"prior_predictive\",\n",
    "            \"sample_stats_prior\",\n",
    "            \"observed_data\",\n",
    "        ]\n",
    "        other_groups = [group for group in args_groups if group not in basic_order]\n",
    "\n",
    "        for group in basic_order + other_groups:\n",
    "            if group not in args_groups:\n",
    "                continue\n",
    "            if inplace:\n",
    "                arg0._groups.append(group)\n",
    "                setattr(arg0, group, args_groups[group])\n",
    "            else:\n",
    "                inference_data_dict[group] = args_groups[group]\n",
    "        if inplace:\n",
    "            other_groups = [\n",
    "                group for group in arg0_groups if group not in basic_order\n",
    "            ] + other_groups\n",
    "            sorted_groups = [group for group in basic_order + other_groups if group in arg0._groups]\n",
    "            setattr(arg0, \"_groups\", sorted_groups)\n",
    "    else:\n",
    "        arg0 = args[0]\n",
    "        arg0_groups = arg0._groups\n",
    "        for arg in args[1:]:\n",
    "            for group0 in arg0_groups:\n",
    "                if group0 not in arg._groups:\n",
    "                    if group0 == \"observed_data\":\n",
    "                        continue\n",
    "                    msg = \"Mismatch between the groups.\"\n",
    "                    raise TypeError(msg)\n",
    "            for group in arg._groups:\n",
    "                if group != \"observed_data\":\n",
    "                    # assert that groups are equal\n",
    "                    if group not in arg0_groups:\n",
    "                        msg = \"Mismatch between the groups.\"\n",
    "                        raise TypeError(msg)\n",
    "\n",
    "                    # assert that variables are equal\n",
    "                    group_data = getattr(arg, group)\n",
    "                    group_vars = group_data.data_vars\n",
    "\n",
    "                    if not inplace and group in inference_data_dict:\n",
    "                        group0_data = inference_data_dict[group]\n",
    "                    else:\n",
    "                        group0_data = getattr(arg0, group)\n",
    "                    group0_vars = group0_data.data_vars\n",
    "\n",
    "                    for var in group0_vars:\n",
    "                        if var not in group_vars:\n",
    "                            msg = \"Mismatch between the variables.\"\n",
    "                            raise TypeError(msg)\n",
    "\n",
    "                    for var in group_vars:\n",
    "                        if var not in group0_vars:\n",
    "                            msg = \"Mismatch between the variables.\"\n",
    "                            raise TypeError(msg)\n",
    "                        var_dims = getattr(group_data, var).dims\n",
    "                        var0_dims = getattr(group0_data, var).dims\n",
    "                        if var_dims != var0_dims:\n",
    "                            msg = \"Mismatch between the dimensions.\"\n",
    "                            raise TypeError(msg)\n",
    "\n",
    "                        if dim not in var_dims or dim not in var0_dims:\n",
    "                            msg = \"Dimension {} missing.\".format(dim)\n",
    "                            raise TypeError(msg)\n",
    "\n",
    "                    # xr.concat\n",
    "                    concatenated_group = xr.concat((group_data, group0_data), dim=dim)\n",
    "                    if reset_dim:\n",
    "                        concatenated_group[dim] = range(concatenated_group[dim].size)\n",
    "\n",
    "                    # handle attrs\n",
    "                    if hasattr(group0_data, \"attrs\"):\n",
    "                        group0_attrs = deepcopy(getattr(group0_data, \"attrs\"))\n",
    "                    else:\n",
    "                        group0_attrs = OrderedDict()\n",
    "\n",
    "                    if hasattr(group_data, \"attrs\"):\n",
    "                        group_attrs = getattr(group_data, \"attrs\")\n",
    "                    else:\n",
    "                        group_attrs = dict()\n",
    "\n",
    "                    # gather attrs results to group0_attrs\n",
    "                    for attr_key, attr_values in group_attrs.items():\n",
    "                        group0_attr_values = group0_attrs.get(attr_key, None)\n",
    "                        equality = attr_values == group0_attr_values\n",
    "                        if hasattr(equality, \"__iter__\"):\n",
    "                            equality = np.all(equality)\n",
    "                        if equality:\n",
    "                            continue\n",
    "                        # handle special cases:\n",
    "                        if attr_key in (\"created_at\", \"previous_created_at\"):\n",
    "                            # check the defaults\n",
    "                            if not hasattr(group0_attrs, \"previous_created_at\"):\n",
    "                                group0_attrs[\"previous_created_at\"] = []\n",
    "                                if group0_attr_values is not None:\n",
    "                                    group0_attrs[\"previous_created_at\"].append(group0_attr_values)\n",
    "                            # check previous values\n",
    "                            if attr_key == \"previous_created_at\":\n",
    "                                if not isinstance(attr_values, list):\n",
    "                                    attr_values = [attr_values]\n",
    "                                group0_attrs[\"previous_created_at\"].extend(attr_values)\n",
    "                                continue\n",
    "                            # update \"created_at\"\n",
    "                            if group0_attr_values != current_time:\n",
    "                                group0_attrs[attr_key] = current_time\n",
    "                            group0_attrs[\"previous_created_at\"].append(attr_values)\n",
    "\n",
    "                        elif attr_key in group0_attrs:\n",
    "                            combined_key = \"combined_{}\".format(attr_key)\n",
    "                            if combined_key not in group0_attrs:\n",
    "                                group0_attrs[combined_key] = [group0_attr_values]\n",
    "                            group0_attrs[combined_key].append(attr_values)\n",
    "                        else:\n",
    "                            group0_attrs[attr_key] = attr_values\n",
    "                    # update attrs\n",
    "                    setattr(concatenated_group, \"attrs\", group0_attrs)\n",
    "\n",
    "                    if inplace:\n",
    "                        setattr(arg0, group, concatenated_group)\n",
    "                    else:\n",
    "                        inference_data_dict[group] = concatenated_group\n",
    "                else:\n",
    "                    # observed_data\n",
    "                    if group not in arg0_groups:\n",
    "                        setattr(arg0, group, deepcopy(group_data) if copy else group_data)\n",
    "                        arg0._groups.append(group)\n",
    "                        continue\n",
    "\n",
    "                    # assert that variables are equal\n",
    "                    group_data = getattr(arg, group)\n",
    "                    group_vars = group_data.data_vars\n",
    "\n",
    "                    group0_data = getattr(arg0, group)\n",
    "                    if not inplace:\n",
    "                        group0_data = deepcopy(group0_data)\n",
    "                    group0_vars = group0_data.data_vars\n",
    "\n",
    "                    for var in group_vars:\n",
    "                        if var not in group0_vars:\n",
    "                            var_data = getattr(group_data, var)\n",
    "                            arg0.observed_data[var] = var_data\n",
    "                        else:\n",
    "                            var_data = getattr(group_data, var)\n",
    "                            var0_data = getattr(group0_data, var)\n",
    "                            if dim in var_data.dims and dim in var0_data.dims:\n",
    "                                concatenated_var = xr.concat((group_data, group0_data), dim=dim)\n",
    "                                group0_data[var] = concatenated_var\n",
    "\n",
    "                    # handle attrs\n",
    "                    if hasattr(group0_data, \"attrs\"):\n",
    "                        group0_attrs = getattr(group0_data, \"attrs\")\n",
    "                    else:\n",
    "                        group0_attrs = OrderedDict()\n",
    "\n",
    "                    if hasattr(group_data, \"attrs\"):\n",
    "                        group_attrs = getattr(group_data, \"attrs\")\n",
    "                    else:\n",
    "                        group_attrs = dict()\n",
    "\n",
    "                    # gather attrs results to group0_attrs\n",
    "                    for attr_key, attr_values in group_attrs.items():\n",
    "                        group0_attr_values = group0_attrs.get(attr_key, None)\n",
    "                        equality = attr_values == group0_attr_values\n",
    "                        if hasattr(equality, \"__iter__\"):\n",
    "                            equality = np.all(equality)\n",
    "                        if equality:\n",
    "                            continue\n",
    "                        # handle special cases:\n",
    "                        if attr_key in (\"created_at\", \"previous_created_at\"):\n",
    "                            # check the defaults\n",
    "                            if not hasattr(group0_attrs, \"previous_created_at\"):\n",
    "                                group0_attrs[\"previous_created_at\"] = []\n",
    "                                if group0_attr_values is not None:\n",
    "                                    group0_attrs[\"previous_created_at\"].append(group0_attr_values)\n",
    "                            # check previous values\n",
    "                            if attr_key == \"previous_created_at\":\n",
    "                                if not isinstance(attr_values, list):\n",
    "                                    attr_values = [attr_values]\n",
    "                                group0_attrs[\"previous_created_at\"].extend(attr_values)\n",
    "                                continue\n",
    "                            # update \"created_at\"\n",
    "                            if group0_attr_values != current_time:\n",
    "                                group0_attrs[attr_key] = current_time\n",
    "                            group0_attrs[\"previous_created_at\"].append(attr_values)\n",
    "\n",
    "                        elif attr_key in group0_attrs:\n",
    "                            combined_key = \"combined_{}\".format(attr_key)\n",
    "                            if combined_key not in group0_attrs:\n",
    "                                group0_attrs[combined_key] = [group0_attr_values]\n",
    "                            group0_attrs[combined_key].append(attr_values)\n",
    "\n",
    "                        else:\n",
    "                            group0_attrs[attr_key] = attr_values\n",
    "                    # update attrs\n",
    "                    setattr(group0_data, \"attrs\", group0_attrs)\n",
    "\n",
    "                    if inplace:\n",
    "                        setattr(arg0, group, group0_data)\n",
    "                    else:\n",
    "                        inference_data_dict[group] = group0_data\n",
    "\n",
    "    return None if inplace else InferenceData(**inference_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from emcee jit the loop and stack b/w lines 180 and 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io pyro jit the loops in var names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def ex_jit(x):\n",
    "    return np.expand_dims(x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 µs ± 152 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  ex_jit(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 µs ± 114 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expand_dims(c, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# njit expand dims in io_tfp and everywhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44 µs ± 229 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.atleast_1d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473 ns ± 18.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit one_de(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 µs ± 15 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.atleast_2d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 ns ± 40.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit two_de(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for io_pymc3 jit the expand-dims and at-least one d methods. Loop jitting can also be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frompystan work with the last 4 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystan import stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_42c0ed8065b95a4b695f212f17d17458 NOW.\n"
     ]
    }
   ],
   "source": [
    "model_code = '''\n",
    "... parameters {\n",
    "...   real y[2];\n",
    "... }\n",
    "... model {\n",
    "...   y[1] ~ normal(0, 1);\n",
    "...   y[2] ~ double_exponential(0, 2);\n",
    "... }'''\n",
    "fit1 = stan(model_code=model_code, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_42c0ed8065b95a4b695f212f17d17458.\n",
       "4 chains, each with iter=100; warmup=50; thin=1; \n",
       "post-warmup draws per chain=50, total post-warmup draws=200.\n",
       "\n",
       "       mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "y[0]  -0.02    0.07    1.0  -2.06  -0.75  -0.09   0.66   2.17    200   0.99\n",
       "y[1]  -0.39    0.37   3.02  -8.11  -1.95  -0.12   1.45   5.08     65    1.0\n",
       "lp__  -1.63     0.2   1.25  -4.53  -2.36  -1.29  -0.71  -0.13     41   1.06\n",
       "\n",
       "Samples were drawn using NUTS at Sun Jul 28 13:04:36 2019.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def check_dim(x):\n",
    "    return np.prod(np.asarray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_draws(fit, variables=None, ignore=None):\n",
    "    \"\"\"Extract draws from PyStan fit.\"\"\"\n",
    "    if ignore is None:\n",
    "        ignore = []\n",
    "    if fit.mode == 1:\n",
    "        msg = \"Model in mode 'test_grad'. Sampling is not conducted.\"\n",
    "        raise AttributeError(msg)\n",
    "\n",
    "    if fit.mode == 2 or fit.sim.get(\"samples\") is None:\n",
    "        msg = \"Fit doesn't contain samples.\"\n",
    "        raise AttributeError(msg)\n",
    "\n",
    "    dtypes = infer_dtypes(fit)\n",
    "\n",
    "    if variables is None:\n",
    "        variables = fit.sim[\"pars_oi\"]\n",
    "    elif isinstance(variables, str):\n",
    "        variables = [variables]\n",
    "    variables = list(variables)\n",
    "\n",
    "    for var, dim in zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]):\n",
    "        if var in variables and np.prod(dim) == 0:\n",
    "            del variables[variables.index(var)]\n",
    "\n",
    "    ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\n",
    "    nchain = len(fit.sim[\"samples\"])\n",
    "\n",
    "    # check if the values are in 0-based (<=2.17) or 1-based indexing (>=2.18)\n",
    "    shift = 1\n",
    "    if any(dim and np.prod(dim) != 0 for dim in fit.sim[\"dims_oi\"]):\n",
    "        # choose variable with lowest number of dims > 1\n",
    "        par_idx = min(\n",
    "            (dim, i) for i, dim in enumerate(fit.sim[\"dims_oi\"]) if (dim and np.prod(dim) != 0)\n",
    "        )[1]\n",
    "        offset = int(sum(map(np.product, fit.sim[\"dims_oi\"][:par_idx])))\n",
    "        par_offset = int(np.product(fit.sim[\"dims_oi\"][par_idx]))\n",
    "        par_keys = fit.sim[\"fnames_oi\"][offset : offset + par_offset]\n",
    "        shift = len(par_keys)\n",
    "        for item in par_keys:\n",
    "            _, shape = item.replace(\"]\", \"\").split(\"[\")\n",
    "            shape_idx_min = min(int(shape_value) for shape_value in shape.split(\",\"))\n",
    "            if shape_idx_min < shift:\n",
    "                shift = shape_idx_min\n",
    "        # If shift is higher than 1, this will probably mean that Stan\n",
    "        # has implemented sparse structure (saves only non-zero parts),\n",
    "        # but let's hope that dims are still corresponding the full shape\n",
    "        shift = int(min(shift, 1))\n",
    "\n",
    "    var_keys = OrderedDict((var, []) for var in fit.sim[\"pars_oi\"])\n",
    "    for key in fit.sim[\"fnames_oi\"]:\n",
    "        var, *tails = key.split(\"[\")\n",
    "        loc = [Ellipsis]\n",
    "        for tail in tails:\n",
    "            loc = []\n",
    "            for i in tail[:-1].split(\",\"):\n",
    "                loc.append(int(i) - shift)\n",
    "        var_keys[var].append((key, loc))\n",
    "\n",
    "    shapes = dict(zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]))\n",
    "\n",
    "    variables = [var for var in variables if var not in ignore]\n",
    "\n",
    "    data = OrderedDict()\n",
    "\n",
    "    for var in variables:\n",
    "        if var in data:\n",
    "            continue\n",
    "        keys_locs = var_keys.get(var, [(var, [Ellipsis])])\n",
    "        shape = shapes.get(var, [])\n",
    "        dtype = dtypes.get(var)\n",
    "\n",
    "        ndraw = max(ndraws)\n",
    "        ary_shape = [nchain, ndraw] + shape\n",
    "        ary = np.empty(ary_shape, dtype=dtype, order=\"F\")\n",
    "        for chain, (pyholder, ndraw) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\n",
    "            axes = [chain, slice(None)]\n",
    "            for key, loc in keys_locs:\n",
    "                ary_slice = tuple(axes + loc)\n",
    "                ary[ary_slice] = pyholder.chains[key][-ndraw:]\n",
    "        data[var] = ary\n",
    "    return data\n",
    "\n",
    "def get_draws_jit(fit, variables=None, ignore=None):\n",
    "    \"\"\"Extract draws from PyStan fit.\"\"\"\n",
    "    if ignore is None:\n",
    "        ignore = []\n",
    "    if fit.mode == 1:\n",
    "        msg = \"Model in mode 'test_grad'. Sampling is not conducted.\"\n",
    "        raise AttributeError(msg)\n",
    "\n",
    "    if fit.mode == 2 or fit.sim.get(\"samples\") is None:\n",
    "        msg = \"Fit doesn't contain samples.\"\n",
    "        raise AttributeError(msg)\n",
    "\n",
    "    dtypes = infer_dtypes(fit)\n",
    "\n",
    "    if variables is None:\n",
    "        variables = fit.sim[\"pars_oi\"]\n",
    "    elif isinstance(variables, str):\n",
    "        variables = [variables]\n",
    "    variables = list(variables)\n",
    "\n",
    "    for var, dim in zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]):\n",
    "        if var in variables and np.prod(dim) == 0:\n",
    "            del variables[variables.index(var)]\n",
    "\n",
    "    ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\n",
    "    nchain = len(fit.sim[\"samples\"])\n",
    "\n",
    "    # check if the values are in 0-based (<=2.17) or 1-based indexing (>=2.18)\n",
    "    shift = 1\n",
    "    if any(dim and np.prod(dim) != 0 for dim in fit.sim[\"dims_oi\"]):\n",
    "        # choose variable with lowest number of dims > 1\n",
    "        par_idx = min(\n",
    "            (dim, i) for i, dim in enumerate(fit.sim[\"dims_oi\"]) if (dim and np.prod(dim) != 0)\n",
    "        )[1]\n",
    "        offset = int(sum(map(np.product, fit.sim[\"dims_oi\"][:par_idx])))\n",
    "        par_offset = int(np.product(fit.sim[\"dims_oi\"][par_idx]))\n",
    "        par_keys = fit.sim[\"fnames_oi\"][offset : offset + par_offset]\n",
    "        shift = len(par_keys)\n",
    "        for item in par_keys:\n",
    "            _, shape = item.replace(\"]\", \"\").split(\"[\")\n",
    "            shape_idx_min = min(int(shape_value) for shape_value in shape.split(\",\"))\n",
    "            if shape_idx_min < shift:\n",
    "                shift = shape_idx_min\n",
    "        # If shift is higher than 1, this will probably mean that Stan\n",
    "        # has implemented sparse structure (saves only non-zero parts),\n",
    "        # but let's hope that dims are still corresponding the full shape\n",
    "        shift = int(min(shift, 1))\n",
    "\n",
    "    var_keys = OrderedDict((var, []) for var in fit.sim[\"pars_oi\"])\n",
    "    for key in fit.sim[\"fnames_oi\"]:\n",
    "        var, *tails = key.split(\"[\")\n",
    "        loc = [Ellipsis]\n",
    "        for tail in tails:\n",
    "            loc = []\n",
    "            for i in tail[:-1].split(\",\"):\n",
    "                loc.append(int(i) - shift)\n",
    "        var_keys[var].append((key, loc))\n",
    "\n",
    "    shapes = dict(zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]))\n",
    "\n",
    "    variables = [var for var in variables if var not in ignore]\n",
    "\n",
    "    data = OrderedDict()\n",
    "    for var in variables:\n",
    "        if var in data:\n",
    "            continue\n",
    "        keys_locs = var_keys.get(var, [(var, [Ellipsis])])\n",
    "        shape = shapes.get(var, [])\n",
    "        dtype = dtypes.get(var)\n",
    "\n",
    "        ndraw = max(ndraws)\n",
    "        ary_shape = [nchain, ndraw] + shape\n",
    "        ary = np.empty(ary_shape, dtype=dtype, order=\"F\")\n",
    "        for chain, (pyholder, ndraw) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\n",
    "            axes = [chain, slice(None)]\n",
    "            for key, loc in keys_locs:\n",
    "                ary_slice = tuple(axes + loc)\n",
    "                ary[ary_slice] = pyholder.chains[key][-ndraw:]\n",
    "        data[var] = ary\n",
    "    return data\n",
    "\n",
    "@numba.jit(forceobj=True)\n",
    "def loop_lifter(data,variables,var_keys,shapes,dtypes,ndraws,fit,nchain):\n",
    "    for var in variables:\n",
    "        if var in data:\n",
    "            continue\n",
    "        keys_locs = var_keys.get(var, [(var, [Ellipsis])])\n",
    "        shape = shapes.get(var, [])\n",
    "        dtype = dtypes.get(var)\n",
    "\n",
    "        ndraw = max(ndraws)\n",
    "        ary_shape = [nchain, ndraw] + shape\n",
    "        ary = np.empty(ary_shape, dtype=dtype, order=\"F\")\n",
    "        for chain, (pyholder, ndraw) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\n",
    "            axes = [chain, slice(None)]\n",
    "            for key, loc in keys_locs:\n",
    "                ary_slice = tuple(axes + loc)\n",
    "                ary[ary_slice] = pyholder.chains[key][-ndraw:]\n",
    "        data[var] = ary\n",
    "    return data\n",
    "    \n",
    "def infer_dtypes(fit, model=None):\n",
    "    \"\"\"Infer dtypes from Stan model code.\n",
    "\n",
    "    Function strips out generated quantities block and searchs for `int`\n",
    "    dtypes after stripping out comments inside the block.\n",
    "    \"\"\"\n",
    "    pattern_remove_comments = re.compile(\n",
    "        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"', re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "    stan_integer = r\"int\"\n",
    "    stan_limits = r\"(?:\\<[^\\>]+\\>)*\"  # ignore group: 0 or more <....>\n",
    "    stan_param = r\"([^;=\\s\\[]+)\"  # capture group: ends= \";\", \"=\", \"[\" or whitespace\n",
    "    stan_ws = r\"\\s*\"  # 0 or more whitespace\n",
    "    pattern_int = re.compile(\n",
    "        \"\".join((stan_integer, stan_ws, stan_limits, stan_ws, stan_param)), re.IGNORECASE\n",
    "    )\n",
    "    if model is None:\n",
    "        stan_code = fit.get_stancode()\n",
    "        model_pars = fit.model_pars\n",
    "    else:\n",
    "        stan_code = model.program_code\n",
    "        model_pars = fit.param_names\n",
    "    # remove deprecated comments\n",
    "    stan_code = \"\\n\".join(\n",
    "        line if \"#\" not in line else line[: line.find(\"#\")] for line in stan_code.splitlines()\n",
    "    )\n",
    "    stan_code = re.sub(pattern_remove_comments, \"\", stan_code)\n",
    "    stan_code = stan_code.split(\"generated quantities\")[-1]\n",
    "    dtypes = re.findall(pattern_int, stan_code)\n",
    "    dtypes = {item.strip(): \"int\" for item in dtypes if item.strip() in model_pars}\n",
    "    return dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 µs ± 22.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_draws(fit=fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no point jitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.099299 s\n",
      "File: <ipython-input-54-3c8d3439d7c5>\n",
      "Function: get_draws at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def get_draws(fit, variables=None, ignore=None):\n",
      "     2                                               \"\"\"Extract draws from PyStan fit.\"\"\"\n",
      "     3         1          8.0      8.0      0.0      if ignore is None:\n",
      "     4         1          6.0      6.0      0.0          ignore = []\n",
      "     5         1          7.0      7.0      0.0      if fit.mode == 1:\n",
      "     6                                                   msg = \"Model in mode 'test_grad'. Sampling is not conducted.\"\n",
      "     7                                                   raise AttributeError(msg)\n",
      "     8                                           \n",
      "     9         1          8.0      8.0      0.0      if fit.mode == 2 or fit.sim.get(\"samples\") is None:\n",
      "    10                                                   msg = \"Fit doesn't contain samples.\"\n",
      "    11                                                   raise AttributeError(msg)\n",
      "    12                                           \n",
      "    13         1      97513.0  97513.0     98.2      dtypes = infer_dtypes(fit)\n",
      "    14                                           \n",
      "    15         1         10.0     10.0      0.0      if variables is None:\n",
      "    16         1         13.0     13.0      0.0          variables = fit.sim[\"pars_oi\"]\n",
      "    17                                               elif isinstance(variables, str):\n",
      "    18                                                   variables = [variables]\n",
      "    19         1         12.0     12.0      0.0      variables = list(variables)\n",
      "    20                                           \n",
      "    21         3         32.0     10.7      0.0      for var, dim in zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]):\n",
      "    22         2        243.0    121.5      0.2          if var in variables and np.prod(dim) == 0:\n",
      "    23                                                       del variables[variables.index(var)]\n",
      "    24                                           \n",
      "    25         1         20.0     20.0      0.0      ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\n",
      "    26         1          9.0      9.0      0.0      nchain = len(fit.sim[\"samples\"])\n",
      "    27                                           \n",
      "    28                                               # check if the values are in 0-based (<=2.17) or 1-based indexing (>=2.18)\n",
      "    29         1          7.0      7.0      0.0      shift = 1\n",
      "    30         1         78.0     78.0      0.1      if any(dim and np.prod(dim) != 0 for dim in fit.sim[\"dims_oi\"]):\n",
      "    31                                                   # choose variable with lowest number of dims > 1\n",
      "    32         1          8.0      8.0      0.0          par_idx = min(\n",
      "    33         1         69.0     69.0      0.1              (dim, i) for i, dim in enumerate(fit.sim[\"dims_oi\"]) if (dim and np.prod(dim) != 0)\n",
      "    34         1          8.0      8.0      0.0          )[1]\n",
      "    35         1         18.0     18.0      0.0          offset = int(sum(map(np.product, fit.sim[\"dims_oi\"][:par_idx])))\n",
      "    36         1        101.0    101.0      0.1          par_offset = int(np.product(fit.sim[\"dims_oi\"][par_idx]))\n",
      "    37         1         11.0     11.0      0.0          par_keys = fit.sim[\"fnames_oi\"][offset : offset + par_offset]\n",
      "    38         1          8.0      8.0      0.0          shift = len(par_keys)\n",
      "    39         3         23.0      7.7      0.0          for item in par_keys:\n",
      "    40         2         25.0     12.5      0.0              _, shape = item.replace(\"]\", \"\").split(\"[\")\n",
      "    41         2         34.0     17.0      0.0              shape_idx_min = min(int(shape_value) for shape_value in shape.split(\",\"))\n",
      "    42         2         14.0      7.0      0.0              if shape_idx_min < shift:\n",
      "    43         1          7.0      7.0      0.0                  shift = shape_idx_min\n",
      "    44                                                   # If shift is higher than 1, this will probably mean that Stan\n",
      "    45                                                   # has implemented sparse structure (saves only non-zero parts),\n",
      "    46                                                   # but let's hope that dims are still corresponding the full shape\n",
      "    47         1         10.0     10.0      0.0          shift = int(min(shift, 1))\n",
      "    48                                           \n",
      "    49         1         25.0     25.0      0.0      var_keys = OrderedDict((var, []) for var in fit.sim[\"pars_oi\"])\n",
      "    50         4         29.0      7.2      0.0      for key in fit.sim[\"fnames_oi\"]:\n",
      "    51         3         33.0     11.0      0.0          var, *tails = key.split(\"[\")\n",
      "    52         3         22.0      7.3      0.0          loc = [Ellipsis]\n",
      "    53         5         36.0      7.2      0.0          for tail in tails:\n",
      "    54         2         14.0      7.0      0.0              loc = []\n",
      "    55         4         34.0      8.5      0.0              for i in tail[:-1].split(\",\"):\n",
      "    56         2         19.0      9.5      0.0                  loc.append(int(i) - shift)\n",
      "    57         3         25.0      8.3      0.0          var_keys[var].append((key, loc))\n",
      "    58                                           \n",
      "    59         1         16.0     16.0      0.0      shapes = dict(zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]))\n",
      "    60                                           \n",
      "    61         1         15.0     15.0      0.0      variables = [var for var in variables if var not in ignore]\n",
      "    62                                           \n",
      "    63         1          9.0      9.0      0.0      data = OrderedDict()\n",
      "    64                                           \n",
      "    65         3         21.0      7.0      0.0      for var in variables:\n",
      "    66         2         14.0      7.0      0.0          if var in data:\n",
      "    67                                                       continue\n",
      "    68         2         18.0      9.0      0.0          keys_locs = var_keys.get(var, [(var, [Ellipsis])])\n",
      "    69         2         16.0      8.0      0.0          shape = shapes.get(var, [])\n",
      "    70         2         15.0      7.5      0.0          dtype = dtypes.get(var)\n",
      "    71                                           \n",
      "    72         2         19.0      9.5      0.0          ndraw = max(ndraws)\n",
      "    73         2         16.0      8.0      0.0          ary_shape = [nchain, ndraw] + shape\n",
      "    74         2         40.0     20.0      0.0          ary = np.empty(ary_shape, dtype=dtype, order=\"F\")\n",
      "    75        10         83.0      8.3      0.1          for chain, (pyholder, ndraw) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\n",
      "    76         8         64.0      8.0      0.1              axes = [chain, slice(None)]\n",
      "    77        20        143.0      7.2      0.1              for key, loc in keys_locs:\n",
      "    78        12         97.0      8.1      0.1                  ary_slice = tuple(axes + loc)\n",
      "    79        12        153.0     12.8      0.2                  ary[ary_slice] = pyholder.chains[key][-ndraw:]\n",
      "    80         2         14.0      7.0      0.0          data[var] = ary\n",
      "    81                                           \n",
      "    82         1          7.0      7.0      0.0      return data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(get_draws)\n",
    "wrapper(fit1)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def check_dim(x,y):\n",
    "    return np.product(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(1000000)\n",
    "y=np.random.randn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################cmdstan########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_dataframes(dfs):\n",
    "    col_groups = defaultdict(list)\n",
    "    columns = dfs[0].columns\n",
    "    for col in columns:\n",
    "        key, *loc = col.split(\".\")\n",
    "        loc = tuple(int(i) - 1 for i in loc)\n",
    "        col_groups[key].append((col, loc))\n",
    "\n",
    "    chains = len(dfs)\n",
    "    draws = len(dfs[0])\n",
    "    sample = {}\n",
    "    for key, cols_locs in col_groups.items():\n",
    "        ndim = np.array([loc for _, loc in cols_locs]).max(0) + 1\n",
    "        sample[key] = np.full((chains, draws, *ndim), np.nan)\n",
    "        for col, loc in cols_locs:\n",
    "            for chain_id, df in enumerate(dfs):\n",
    "                draw = df[col].values\n",
    "                if loc == ():\n",
    "                    sample[key][chain_id, :] = draw\n",
    "                else:\n",
    "                    axis1_all = range(sample[key].shape[1])\n",
    "                    slicer = (chain_id, axis1_all, *loc)\n",
    "                    sample[key][slicer] = draw\n",
    "    return sample\n",
    "\n",
    "def _unpack_dataframes_jit(dfs):\n",
    "    col_groups = defaultdict(list)\n",
    "    columns = dfs[0].columns\n",
    "    for col in columns:\n",
    "        key, *loc = col.split(\".\")\n",
    "        loc = tuple(int(i) - 1 for i in loc)\n",
    "        col_groups[key].append((col, loc))\n",
    "    sample = looper_jit(col_groups,columns,key,loc,)\n",
    "    return sample\n",
    "\n",
    "@numba.jit(nopython=False,forceobj=True)\n",
    "def looper_jit(col_groups,columns,key,loc):\n",
    "    chains = len(dfs)\n",
    "    draws = len(dfs[0])\n",
    "    sample = {}\n",
    "\n",
    "    for key, cols_locs in col_groups.items():\n",
    "        locs = []\n",
    "        for _,loc in cols_locs:\n",
    "            locs.append(loc)\n",
    "        \n",
    "        ndim = np.array(locs).max(0) + 1\n",
    "        sample[key] = np.full((chains, draws, *ndim),np.nan)\n",
    "        for col, loc in cols_locs:\n",
    "            for chain_id, df in enumerate(dfs):\n",
    "                draw = df[col].values\n",
    "                if loc == ():\n",
    "                    sample[key][chain_id, :] = draw\n",
    "                else:\n",
    "                    axis1_all = range(sample[key].shape[1])\n",
    "                    slicer = (chain_id, axis1_all, *loc)\n",
    "                    sample[key][slicer] = draw\n",
    "    return sample\n",
    "\n",
    "@numba.njit\n",
    "def full(shape):\n",
    "    return np.full(shape,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({'1234': np.random.randn(10000),\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 µs ± 2.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _unpack_dataframes(dfs=dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-5b087d73eede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_unpack_dataframes_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-287-c33eb37ac94b>\u001b[0m in \u001b[0;36m_unpack_dataframes_jit\u001b[0;34m(dfs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcol_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlooper_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (0,) "
     ]
    }
   ],
   "source": [
    "_unpack_dataframes_jit(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jitting is throwing erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpack_frame(data, columns, valid_cols):\n",
    "    draws, chains, *_ = data.shape\n",
    "\n",
    "    column_groups = defaultdict(list)\n",
    "    column_locs = defaultdict(list)\n",
    "    for i, col in enumerate(columns):\n",
    "        col_base, *col_tail = col.split(\".\")\n",
    "        if len(col_tail):\n",
    "            column_groups[col_base].append(tuple(map(int, col_tail)))\n",
    "        column_locs[col_base].append(i)\n",
    "    dims = {}\n",
    "    for colname, col_dims in column_groups.items():\n",
    "        dims[colname] = tuple(np.array(col_dims).max(0))\n",
    "    sample = {}\n",
    "    valid_base_cols = []\n",
    "    for col in valid_cols:\n",
    "        base_col, *_ = col.split(\".\")\n",
    "        if base_col not in valid_base_cols:\n",
    "            valid_base_cols.append(base_col)\n",
    "\n",
    "    # extract each wanted parameter to ndarray with correct shape\n",
    "    for key in valid_base_cols:\n",
    "        ndim = dims.get(key, None)\n",
    "        shape_location = column_groups.get(key, None)\n",
    "        if ndim is not None:\n",
    "            sample[key] = np.full((chains, draws, *ndim), np.nan)\n",
    "        if shape_location is None:\n",
    "            # reorder draw, chain -> chain, draw\n",
    "            i, = column_locs[key]\n",
    "            sample[key] = np.swapaxes(data[..., i], 0, 1)\n",
    "        else:\n",
    "            for i, shape_loc in zip(column_locs[key], shape_location):\n",
    "                # location to insert extracted array\n",
    "                shape_loc = tuple([Ellipsis] + [j - 1 for j in shape_loc])\n",
    "                # reorder draw, chain -> chain, draw and insert to ndarray\n",
    "                sample[key][shape_loc] = np.swapaxes(data[..., i], 0, 1)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing  to see here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
