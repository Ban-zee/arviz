{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "from arviz.data import convert_to_dataset\n",
    "from arviz.stats.diagnostics import bfmi,_bfmi,geweke as ge,rhat as rh,_ess as es\n",
    "from arviz.stats.diagnostics import _rhat,_split_chains,_z_scale\n",
    "from arviz.stats.diagnostics import _rhat_rank as rk\n",
    "from arviz.stats.diagnostics import ks_summary as ks\n",
    "from arviz.utils import conditional_jit\n",
    "from arviz.stats.stats_utils import not_valid as _not_valid, autocov as _autocov\n",
    "import numpy as np\n",
    "from line_profiler import LineProfiler\n",
    "import numba\n",
    "import pandas as pd\n",
    "from scipy.fftpack import next_fast_len\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy import stats\n",
    "from xarray import apply_ufunc\n",
    "import xarray as xr\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 ms ± 3.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.207935 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: bfmi at line 24\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    24                                           def bfmi(data):\n",
      "    25                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "    26                                           \n",
      "    27                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "    28                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "    29                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "    30                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "    31                                               information.\n",
      "    32                                           \n",
      "    33                                               Parameters\n",
      "    34                                               ----------\n",
      "    35                                               data : obj\n",
      "    36                                                   Any object that can be converted to an az.InferenceData object.\n",
      "    37                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "    38                                                   If InferenceData, energy variable needs to be found.\n",
      "    39                                           \n",
      "    40                                               Returns\n",
      "    41                                               -------\n",
      "    42                                               z : array\n",
      "    43                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "    44                                                   chain in the trace.\n",
      "    45                                               \"\"\"\n",
      "    46         1          6.0      6.0      0.0      if isinstance(data, np.ndarray):\n",
      "    47         1     207929.0 207929.0    100.0          return _bfmi(data)\n",
      "    48                                           \n",
      "    49                                               dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
      "    50                                               if not hasattr(dataset, \"energy\"):\n",
      "    51                                                   raise TypeError(\"Energy variable was not found.\")\n",
      "    52                                               return _bfmi(dataset.energy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.193759 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: _bfmi at line 475\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   475                                           def _bfmi(energy):\n",
      "   476                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "   477                                           \n",
      "   478                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "   479                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "   480                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "   481                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "   482                                               information.\n",
      "   483                                           \n",
      "   484                                               Parameters\n",
      "   485                                               ----------\n",
      "   486                                               energy : NumPy array\n",
      "   487                                                   Should be extracted from a gradient based sampler, such as in Stan or PyMC3. Typically,\n",
      "   488                                                   after converting a trace or fit to InferenceData, the energy will be in\n",
      "   489                                                   `data.sample_stats.energy`.\n",
      "   490                                           \n",
      "   491                                               Returns\n",
      "   492                                               -------\n",
      "   493                                               z : array\n",
      "   494                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "   495                                                   chain in the trace.\n",
      "   496                                               \"\"\"\n",
      "   497         1         17.0     17.0      0.0      energy_mat = np.atleast_2d(energy)\n",
      "   498         1     102648.0 102648.0     53.0      num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
      "   499         1      91073.0  91073.0     47.0      den = np.var(energy_mat, axis=1)\n",
      "   500         1         21.0     21.0      0.0      return num / den\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "def _bfmi_jit(energy):\n",
    "    energy_mat = np.atleast_2d(energy)\n",
    "    if energy_mat.ndim==2:\n",
    "        num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
    "        den = _var_2d(energy_mat)\n",
    "    return num / den\n",
    "\n",
    "def bfmi_new(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return _bfmi_jit(data)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
    "    if not hasattr(dataset, \"energy\"):\n",
    "        raise TypeError(\"Energy variable was not found.\")\n",
    "    return _bfmi_jit(dataset.energy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 ms ± 3.58 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 ms ± 19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.diagnostics.bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.8 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.2 ms ± 2.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 ms ± 37.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 ms ± 4.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Much better improvement on larger datasets. A gain on a few milliseconds on school'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Much better improvement on larger datasets. A gain on a few milliseconds on school'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ks_summary'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ks_summary\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10_00_00,1000)\n",
    "school  = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ks_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 12.386 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ks_summary at line 448\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   448                                           def ks_summary(pareto_tail_indices):\n",
      "   449                                               \"\"\"Display a summary of Pareto tail indices.\n",
      "   450                                           \n",
      "   451                                               Parameters\n",
      "   452                                               ----------\n",
      "   453                                               pareto_tail_indices : array\n",
      "   454                                                 Pareto tail indices.\n",
      "   455                                           \n",
      "   456                                               Returns\n",
      "   457                                               -------\n",
      "   458                                               df_k : dataframe\n",
      "   459                                                 Dataframe containing k diagnostic values.\n",
      "   460                                               \"\"\"\n",
      "   461         1   11673651.0 11673651.0     94.2      kcounts, _ = np.histogram(pareto_tail_indices, bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
      "   462         1     130334.0 130334.0      1.1      kprop = kcounts / len(pareto_tail_indices) * 100\n",
      "   463         1         12.0     12.0      0.0      df_k = pd.DataFrame(\n",
      "   464         1     580598.0 580598.0      4.7          dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
      "   465         1       1305.0   1305.0      0.0      ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
      "   466                                           \n",
      "   467         1         39.0     39.0      0.0      if np.sum(kcounts[1:]) == 0:\n",
      "   468                                                   warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
      "   469         1         14.0     14.0      0.0      elif np.sum(kcounts[2:]) == 0:\n",
      "   470                                                   warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
      "   471                                           \n",
      "   472         1          2.0      2.0      0.0      return df_k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ks)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottleneck ar np.historgram'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bottleneck ar np.historgram'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _histogram(data):\n",
    "    kcounts, _ = np.histogram(data,bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
    "    return kcounts\n",
    "\n",
    "\n",
    "def ks_summary_new(pareto_tail_indices):\n",
    "    kcounts = _histogram(pareto_tail_indices)\n",
    "    kprop = kcounts / len(pareto_tail_indices) * 100\n",
    "    df_k = pd.DataFrame(\n",
    "        dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
    "    ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
    "\n",
    "    if np.sum(kcounts[1:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
    "    elif np.sum(kcounts[2:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
    "\n",
    "    return df_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_</th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-Inf, 0.5]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 0.7]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.7, 1]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, Inf)</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                _  Count   Pct\n",
       "(-Inf, 0.5]  True   True  True\n",
       " (0.5, 0.7]  True   True  True\n",
       "   (0.7, 1]  True   True  True\n",
       "   (1, Inf)  True   True  True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_summary_new(data)==ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 s ± 47.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 s ± 187 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 ms ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 ms ± 248 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 ms ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.87 s ± 43.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHEW. 10 times faster on large datasets. Much better performance on every dataset'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''PHEW. 10 times faster on large datasets. Much better performance on every dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"geweke'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"geweke\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000000)\n",
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "school = school[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 8.58519 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: geweke at line 376\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   376                                           def geweke(ary, first=0.1, last=0.5, intervals=20):\n",
      "   377                                               r\"\"\"Compute z-scores for convergence diagnostics.\n",
      "   378                                           \n",
      "   379                                               Compare the mean of the first % of series with the mean of the last % of series. x is divided\n",
      "   380                                               into a number of segments for which this difference is computed. If the series is converged,\n",
      "   381                                               this score should oscillate between -1 and 1.\n",
      "   382                                           \n",
      "   383                                               Parameters\n",
      "   384                                               ----------\n",
      "   385                                               ary : 1D array-like\n",
      "   386                                                 The trace of some stochastic parameter.\n",
      "   387                                               first : float\n",
      "   388                                                 The fraction of series at the beginning of the trace.\n",
      "   389                                               last : float\n",
      "   390                                                 The fraction of series at the end to be compared with the section\n",
      "   391                                                 at the beginning.\n",
      "   392                                               intervals : int\n",
      "   393                                                 The number of segments.\n",
      "   394                                           \n",
      "   395                                               Returns\n",
      "   396                                               -------\n",
      "   397                                               scores : list [[]]\n",
      "   398                                                 Return a list of [i, score], where i is the starting index for each interval and score the\n",
      "   399                                                 Geweke score on the interval.\n",
      "   400                                           \n",
      "   401                                               Notes\n",
      "   402                                               -----\n",
      "   403                                               The Geweke score on some series x is computed by:\n",
      "   404                                           \n",
      "   405                                                 .. math:: \\frac{E[x_s] - E[x_e]}{\\sqrt{V[x_s] + V[x_e]}}\n",
      "   406                                           \n",
      "   407                                               where :math:`E` stands for the mean, :math:`V` the variance,\n",
      "   408                                               :math:`x_s` a section at the start of the series and\n",
      "   409                                               :math:`x_e` a section at the end of the series.\n",
      "   410                                           \n",
      "   411                                               References\n",
      "   412                                               ----------\n",
      "   413                                               Geweke (1992)\n",
      "   414                                               \"\"\"\n",
      "   415                                               # Filter out invalid intervals\n",
      "   416         3          5.0      1.7      0.0      for interval in (first, last):\n",
      "   417         2          5.0      2.5      0.0          if interval <= 0 or interval >= 1:\n",
      "   418                                                       raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   419         1          2.0      2.0      0.0      if first + last >= 1:\n",
      "   420                                                   raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   421                                           \n",
      "   422                                               # Initialize list of z-scores\n",
      "   423         1          1.0      1.0      0.0      zscores = []\n",
      "   424                                           \n",
      "   425                                               # Last index value\n",
      "   426         1          3.0      3.0      0.0      end = len(ary) - 1\n",
      "   427                                           \n",
      "   428                                               # Start intervals going up to the <last>% of the chain\n",
      "   429         1          2.0      2.0      0.0      last_start_idx = (1 - last) * end\n",
      "   430                                           \n",
      "   431                                               # Calculate starting indices\n",
      "   432         1      19959.0  19959.0      0.2      start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
      "   433                                           \n",
      "   434                                               # Loop over start indices\n",
      "   435       201        703.0      3.5      0.0      for start in start_indices:\n",
      "   436                                                   # Calculate slices\n",
      "   437       200       7251.0     36.3      0.1          first_slice = ary[start : start + int(first * (end - start))]\n",
      "   438       200       3246.0     16.2      0.0          last_slice = ary[int(end - last * (end - start)) :]\n",
      "   439                                           \n",
      "   440       200     905038.0   4525.2     10.5          z_score = first_slice.mean() - last_slice.mean()\n",
      "   441       200    7647501.0  38237.5     89.1          z_score /= np.sqrt(first_slice.var() + last_slice.var())\n",
      "   442                                           \n",
      "   443       200       1245.0      6.2      0.0          zscores.append([start, z_score])\n",
      "   444                                           \n",
      "   445         1        232.0    232.0      0.0      return np.array(zscores)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ge)\n",
    "wrapper(data,0.1,0.5,200)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "\n",
    "@numba.vectorize(nopython=True)   ##Remember to make a conditional_vectorize function\n",
    "def _sqr(a,b):\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "\n",
    "@conditional_jit\n",
    "def geweke_new(ary, first=0.1, last=0.5, intervals=20):\n",
    "    # Filter out invalid intervals\n",
    "    for interval in (first, last):\n",
    "        if interval <= 0 or interval >= 1:\n",
    "            raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "    if first + last >= 1:\n",
    "        raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "\n",
    "    # Initialize list of z-scores\n",
    "    zscores = []\n",
    "\n",
    "    # Last index value\n",
    "    end = len(ary) - 1\n",
    "\n",
    "    # Start intervals going up to the <last>% of the chain\n",
    "    last_start_idx = (1 - last) * end\n",
    "\n",
    "    # Calculate starting indices\n",
    "    start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
    "\n",
    "    # Loop over start indices\n",
    "    for start in start_indices:\n",
    "        # Calculate slices\n",
    "        first_slice = ary[start : start + int(first * (end - start))]\n",
    "        last_slice = ary[int(end - last * (end - start)) :]\n",
    "\n",
    "        z_score = first_slice.mean() - last_slice.mean()\n",
    "        D = _sqr(_var_1d(first_slice), _var_1d(last_slice))\n",
    "        z_score = z_score/D\n",
    "\n",
    "        zscores.append([start, z_score])\n",
    "\n",
    "    return np.array(zscores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 ms ± 4.35 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791 ms ± 46.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 s ± 205 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 s ± 508 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(geweke_new(data,intervals=500), az.geweke(data,intervals=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 ms ± 29.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 ms ± 299 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"GWEKE WORKS WELL'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"GWEKE WORKS WELL\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ess'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ess\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 ms ± 23.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545 ms ± 56.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.504246 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          4.0      4.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          3.0      3.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          2.0      2.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          3.0      3.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          3.0      3.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          2.0      2.0      0.0          \"median\": _ess_median,\n",
      "   180         1          3.0      3.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          3.0      3.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          2.0      2.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          5.0      5.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          3.0      3.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          3.0      3.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          3.0      3.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          4.0      4.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196         1         22.0     22.0      0.0          data = np.atleast_2d(data)\n",
      "   197         1          4.0      4.0      0.0          if len(data.shape) < 3:\n",
      "   198         1          3.0      3.0      0.0              if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203         1     504174.0 504174.0    100.0                  return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211                                               dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212                                               var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214                                               dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216                                               ufunc_kwargs = {\"ravel\": False}\n",
      "   217                                               func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218                                               return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.513004 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          6.0      6.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          4.0      4.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          4.0      4.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          4.0      4.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          4.0      4.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          3.0      3.0      0.0          \"median\": _ess_median,\n",
      "   180         1          4.0      4.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          4.0      4.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          4.0      4.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          6.0      6.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          4.0      4.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          4.0      4.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          4.0      4.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          7.0      7.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196                                                   data = np.atleast_2d(data)\n",
      "   197                                                   if len(data.shape) < 3:\n",
      "   198                                                       if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203                                                           return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211         1       4832.0   4832.0      0.9      dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212         1         24.0     24.0      0.0      var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214         1          4.0      4.0      0.0      dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216         1          4.0      4.0      0.0      ufunc_kwargs = {\"ravel\": False}\n",
      "   217         1          4.0      4.0      0.0      func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218         1     508074.0 508074.0     99.0      return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(dict_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= np.conjugate(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocov(numpy_data,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.034678 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats_utils.py\n",
      "Function: autocov at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           def autocov(ary, axis=-1):\n",
      "    17                                               \"\"\"Compute autocovariance estimates for every lag for the input array.\n",
      "    18                                           \n",
      "    19                                               Parameters\n",
      "    20                                               ----------\n",
      "    21                                               ary : Numpy array\n",
      "    22                                                   An array containing MCMC samples\n",
      "    23                                           \n",
      "    24                                               Returns\n",
      "    25                                               -------\n",
      "    26                                               acov: Numpy array same size as the input array\n",
      "    27                                               \"\"\"\n",
      "    28         1          5.0      5.0      0.0      axis = axis if axis > 0 else len(ary.shape) + axis\n",
      "    29         1          2.0      2.0      0.0      n = ary.shape[axis]\n",
      "    30         1         20.0     20.0      0.1      m = next_fast_len(2 * n)\n",
      "    31                                           \n",
      "    32         1       4028.0   4028.0     11.6      ary = ary - ary.mean(axis, keepdims=True)\n",
      "    33                                           \n",
      "    34                                               # added to silence tuple warning for a submodule\n",
      "    35         1         31.0     31.0      0.1      with warnings.catch_warnings():\n",
      "    36         1         24.0     24.0      0.1          warnings.simplefilter(\"ignore\")\n",
      "    37                                           \n",
      "    38         1       9423.0   9423.0     27.2          ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
      "    39         1       9522.0   9522.0     27.5          ifft_ary *= np.conjugate(ifft_ary)\n",
      "    40                                           \n",
      "    41         1          3.0      3.0      0.0          shape = tuple(\n",
      "    42         1         23.0     23.0      0.1              slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
      "    43                                                   )\n",
      "    44         1       6491.0   6491.0     18.7          cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
      "    45         1       5105.0   5105.0     14.7          cov /= n\n",
      "    46                                           \n",
      "    47         1          1.0      1.0      0.0      return cov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.autocov)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck :: np.fft.rfft and np.conjugate'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Bottleneck :: np.fft.rfft and np.conjugate'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fft(x,m,axis):\n",
    "    return np.fft.rfft(x,n=m,axis=axis)\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def _fft_new(x,m,axis):\n",
    "    N =  np.fft.rfft(x,n=m,axis=axis)\n",
    "    return N\n",
    "\n",
    "def conjuc(data):\n",
    "    return np.conjugate(data)\n",
    "\n",
    "\n",
    "@numba.vectorize\n",
    "def nconjuc(data):\n",
    "    return np.conjugate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8 ms ± 2.26 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft_new(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jitting fft is of no use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.98 ms ± 37.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.conjugate([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.02 ms ± 59 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nconjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov_new(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= nconjuc(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.8 ms ± 399 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit autocov_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.6 ms ± 1.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (az.autocov(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    size = ary.size\n",
    "    rank = stats.rankdata(ary, method=\"average\")\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(ary.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.556993 s\n",
      "File: <ipython-input-62-21c5aaeb33f5>\n",
      "Function: _z_scale at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def _z_scale(ary):\n",
      "     2         1          9.0      9.0      0.0      ary = np.asarray(ary)\n",
      "     3         1          2.0      2.0      0.0      size = ary.size\n",
      "     4         1     384033.0 384033.0     68.9      rank = stats.rankdata(ary, method=\"average\")\n",
      "     5         1     172933.0 172933.0     31.0      z = stats.norm.ppf((rank - 0.5) / size)\n",
      "     6         1         15.0     15.0      0.0      z = z.reshape(ary.shape)\n",
      "     7         1          1.0      1.0      0.0      return z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_z_scale)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankdata_new(arr):\n",
    "    arr = np.ravel(arr)\n",
    "    sorter = np.argsort(arr, kind=\"quicksort\")\n",
    "    inv = np.empty(sorter.size, dtype=np.intp)\n",
    "    inv = inv_sorter(inv,sorter)\n",
    "    arr = arr[sorter]\n",
    "    obs = np.r_[True, arr[1:] != arr[:-1]]\n",
    "    dense = summ(obs)[inv]\n",
    "    count = np.r_[np.nonzero(obs)[0], len(obs)]\n",
    "    return av(count,dense)\n",
    "\n",
    "\n",
    "@numba.njit(cache=True)\n",
    "def inv_sorter(inv,sorter):\n",
    "    inv[sorter] = np.arange(sorter.size)\n",
    "    return inv\n",
    "\n",
    "@numba.njit(cache=True)\n",
    "def av(count, dense):\n",
    "    return .5 * (count[dense] + count[dense - 1] + 1)\n",
    "\n",
    "def av_nojit(count,dense):\n",
    "    return .5 * (count[dense] + count[dense - 1] + 1)\n",
    "\n",
    "\n",
    "def no_jit_inv_sorter(inv,sorter):\n",
    "    inv[sorter] = np.arange(sorter.size)\n",
    "    return inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.1 ms ± 1.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "timeit av(np.random.randn(1000000), np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.3 ms ± 391 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit av_nojit(np.random.randn(1000000), np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   60,  119,  178,  237,  296,  355,  414,  473,  532,  591,\n",
       "        650,  709,  768,  827,  886,  945, 1004, 1063, 1122, 1181, 1240,\n",
       "       1299, 1358, 1417, 1476, 1535, 1594, 1653, 1712, 1771, 1830, 1889,\n",
       "       1948, 2007, 2066, 2125, 2184, 2243, 2302, 2361, 2420, 2479, 2538,\n",
       "       2597, 2656, 2715, 2774, 2833, 2892, 2951, 3010, 3069, 3128, 3187,\n",
       "       3246, 3305, 3364, 3423, 3482, 3541, 3600, 3659, 3718, 3777, 3836,\n",
       "       3895, 3954, 4013, 4072, 4131, 4190, 4249, 4308, 4367, 4426, 4485,\n",
       "       4544, 4603, 4662, 4721, 4780, 4839, 4898, 4957, 5016, 5075, 5134,\n",
       "       5193, 5252, 5311, 5370, 5429, 5488, 5547, 5606, 5665, 5724, 5783,\n",
       "       5842, 5901, 5960, 6019, 6078, 6137, 6196, 6255, 6314, 6373, 6432,\n",
       "       6491, 6550, 6609, 6668, 6727, 6786, 6845, 6904, 6963, 7022, 7081,\n",
       "       7140, 7199, 7258, 7317, 7376, 7435, 7494, 7553, 7612, 7671, 7730,\n",
       "       7789, 7848, 7907, 7966, 8025, 8084, 8143, 8202, 8261, 8320, 8379,\n",
       "       8438, 8497, 8556, 8615, 8674, 8733, 8792, 8851, 8910, 8969, 9028,\n",
       "       9087, 9146, 9205, 9264, 9323, 9382, 9441, 9500, 9559, 9618, 9677,\n",
       "       9736, 9795, 9854, 9913, 9972])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1,10000,np.random.randint(100))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.2 ms ± 288 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit inv_sorter(np.random.randn(1000000),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.4 ms ± 589 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit no_jit_inv_sorter(np.random.randn(1000000),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 8.069 s\n",
      "File: <ipython-input-132-eea37e42983b>\n",
      "Function: rankdata_new at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def rankdata_new(arr):\n",
      "     2         1         32.0     32.0      0.0      arr = np.ravel(arr)\n",
      "     3         1    3173870.0 3173870.0     39.3      sorter = np.argsort(arr, kind=\"quicksort\")\n",
      "     4         1         51.0     51.0      0.0      inv = np.empty(sorter.size, dtype=np.intp)\n",
      "     5         1     430032.0 430032.0      5.3      inv[sorter] = np.arange(sorter.size, dtype=np.intp)\n",
      "     6         1     217964.0 217964.0      2.7      arr = arr[sorter]\n",
      "     7         1      12095.0  12095.0      0.1      obs = np.r_[True, arr[1:] != arr[:-1]]\n",
      "     8         1     268025.0 268025.0      3.3      dense = summ(obs)[inv]\n",
      "     9         1      91675.0  91675.0      1.1      count = np.r_[np.nonzero(obs)[0], len(obs)]\n",
      "    10         1    3875256.0 3875256.0     48.0      return av(count, dense)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(rankdata_new)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ravel(numpy_data)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def summ(x):\n",
    "    return x.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.41 µs ± 44.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summ(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 µs ± 1.04 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cumsum(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(summ(x)==np.cumsum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rankdata_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 ms ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stats.rankdata(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 µs ± 25.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rankdata_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 µs ± 4.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stats.rankdata(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(rankdata_new(school)==stats.rankdata(school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([547064., 106801., 506988., ..., 413241., 492319., 884194.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankdata_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(stats.rankdata(x)==rankdata_new(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    size= ary.size\n",
    "    rank = rankdata_new(ary)\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(ary.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.74 s ± 185 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.93 s ± 109 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_z_scale_new(numpy_data),_z_scale(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 ms ± 1.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 ms ± 934 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''z_scale works well on large sets. It's a bit slow on schools though\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''z_scale works well on large sets. It's a bit slow on schools though'''''''''''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/(len(data)-1)-((a/(len(data)))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "\n",
    "def _rhat_new(ary):\n",
    "    ary = np.asarray(ary, dtype=float)\n",
    "    if _not_valid(ary, check_shape=False):\n",
    "        return np.nan\n",
    "    _, num_samples = ary.shape\n",
    "\n",
    "    # Calculate chain mean\n",
    "    chain_mean = np.mean(ary, axis=1)\n",
    "    # Calculate chain variance\n",
    "    chain_var = _var_2d(ary)\n",
    "    # Calculate between-chain variance\n",
    "    between_chain_variance = num_samples * _var_1d(chain_mean)\n",
    "    # Calculate within-chain variance\n",
    "    within_chain_variance = np.mean(chain_var)\n",
    "    # Estimate of marginal posterior variance\n",
    "    rhat_value = np.sqrt(\n",
    "        (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
    "    )\n",
    "    return rhat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999995592638663"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999995597452329"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.01563316512969"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2579387959289574"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rhat_rank_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    split_ary = _split_chains(ary)\n",
    "    rhat_bulk = _rhat_new(_z_scale_new(split_ary))\n",
    "\n",
    "    split_ary_folded = abs(split_ary - np.median(split_ary))\n",
    "    rhat_tail = _rhat_new(_z_scale_new(split_ary_folded))\n",
    "\n",
    "    rhat_rank = max(rhat_bulk, rhat_tail)\n",
    "    return rhat_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.37711934, 12.14856451, 11.91083021, 12.97533472])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(school,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.43113179, 12.22660757, 11.9633455 , 13.0460773 ])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2579387959289574"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 s ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat_rank_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21 s ± 90.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rk(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.81 ms ± 275 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit _rhat_rank_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.76 ms ± 281 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit rk(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_fold(ary):\n",
    "    \"\"\"Fold and z-scale values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    ary = abs(ary - np.median(ary))\n",
    "    ary = _z_scale_new(ary)\n",
    "    return ary\n",
    "\n",
    "\n",
    "def _rhat_folded_new(ary):\n",
    "    \"\"\"Calculate split-Rhat for folded z-values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    ary = _z_fold(_split_chains(ary))\n",
    "    return _rhat_new(ary)\n",
    "\n",
    "\n",
    "def _rhat_z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_z_scale_new(_split_chains(ary)))\n",
    "\n",
    "\n",
    "def _rhat_split_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_split_chains(ary))\n",
    "\n",
    "\n",
    "def _rhat_identity_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(ary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_new(data, *, var_names=None, method=\"rank\"):\n",
    "\n",
    "    methods = {\n",
    "        \"rank\": _rhat_rank_new,\n",
    "        \"split\": _rhat_split_new,\n",
    "        \"folded\": _rhat_folded_new,\n",
    "        \"z_scale\": _rhat_z_scale_new,\n",
    "        \"identity\": _rhat_identity_new,\n",
    "    }\n",
    "    if method not in methods:\n",
    "        raise TypeError(\n",
    "            \"R-hat method {} not found. Valid methods are:\\n{}\".format(\n",
    "                method, \"\\n    \".join(methods)\n",
    "            )\n",
    "        )\n",
    "    rhat_func = methods[method]\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.atleast_2d(data)\n",
    "        if len(data.shape) < 3:\n",
    "            return rhat_func(data)\n",
    "        else:\n",
    "            msg = (\n",
    "                \"Only uni-dimensional ndarray variables are supported.\"\n",
    "                \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
    "            )\n",
    "            raise TypeError(msg)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"posterior\")\n",
    "    var_names = _var_names(var_names, dataset)\n",
    "\n",
    "    dataset = dataset if var_names is None else dataset[var_names]\n",
    "\n",
    "    ufunc_kwargs = {\"ravel\": False}\n",
    "    func_kwargs = {}\n",
    "    return _wrap_xarray_ufunc(\n",
    "        rhat_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(10000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 ms ± 9.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 ms ± 3.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.65 s ± 43.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.38 s ± 141 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.37 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.06 s ± 718 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 ms ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.1 ms ± 8.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11 ms ± 179 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7 ms ± 71.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 µs ± 2.37 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 ms ± 42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.49 ms ± 72.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35 ms ± 34.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 ms ± 4.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 µs ± 7.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 µs ± 747 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Improvement in rhat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\'ESS'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'ESS\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def loop_lifter(n_chain,n_draw,acov,chain_mean, mean_var, var_plus,rho_hat_t,rho_hat_even,rho_hat_odd,relative):\n",
    "    t = 1\n",
    "    while t < (n_draw - 3) and (rho_hat_even + rho_hat_odd) > 0.0:\n",
    "        rho_hat_even = 1.0 - (mean_var - np.mean(acov[:, t + 1])) / var_plus\n",
    "        rho_hat_odd = 1.0 - (mean_var - np.mean(acov[:, t + 2])) / var_plus\n",
    "        if (rho_hat_even + rho_hat_odd) >= 0:\n",
    "            rho_hat_t[t + 1] = rho_hat_even\n",
    "            rho_hat_t[t + 2] = rho_hat_odd\n",
    "        t += 2\n",
    "\n",
    "    max_t = t - 2\n",
    "    # improve estimation\n",
    "    if rho_hat_even > 0:\n",
    "        rho_hat_t[max_t + 1] = rho_hat_even\n",
    "    # Geyer's initial monotone sequence\n",
    "    t = 1\n",
    "    while t <= max_t - 2:\n",
    "        if (rho_hat_t[t + 1] + rho_hat_t[t + 2]) > (rho_hat_t[t - 1] + rho_hat_t[t]):\n",
    "            rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2.0\n",
    "            rho_hat_t[t + 2] = rho_hat_t[t + 1]\n",
    "        t += 2\n",
    "\n",
    "    ess = n_chain * n_draw\n",
    "    tau_hat = -1.0 + 2.0 * np.sum(rho_hat_t[: max_t + 1]) + np.sum(rho_hat_t[max_t + 1 : max_t + 2])\n",
    "    tau_hat = max(tau_hat, 1 / np.log10(ess))\n",
    "    ess = (1 if relative else ess) / tau_hat\n",
    "    if np.isnan(rho_hat_t).any():\n",
    "        ess = np.nan\n",
    "    return ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _ess_new(ary, relative=False):\n",
    "    \"\"\"Compute the effective sample size for a 2D array.\"\"\"\n",
    "    ary = np.asarray(ary, dtype=float)\n",
    "    if _not_valid(ary, check_shape=False):\n",
    "        return np.nan\n",
    "    if (np.max(ary) - np.min(ary)) < np.finfo(float).resolution:  # pylint: disable=no-member\n",
    "        return ary.size\n",
    "    if len(ary.shape) < 2:\n",
    "        ary = np.atleast_2d(ary)\n",
    "    n_chain, n_draw = ary.shape\n",
    "    acov = _autocov(ary, axis=1)\n",
    "    chain_mean = ary.mean(axis=1)\n",
    "    mean_var = np.mean(acov[:, 0]) * n_draw / (n_draw - 1.0)\n",
    "    var_plus = mean_var * (n_draw - 1.0) / n_draw\n",
    "    if n_chain > 1:\n",
    "        var_plus += np.var(chain_mean, ddof=1)\n",
    "\n",
    "    rho_hat_t = np.zeros(n_draw)\n",
    "    rho_hat_even = 1.0\n",
    "    rho_hat_t[0] = rho_hat_even\n",
    "    rho_hat_odd = 1.0 - (mean_var - np.mean(acov[:, 1])) / var_plus\n",
    "    rho_hat_t[1] = rho_hat_odd\n",
    "    \n",
    "    ess = loop_lifter(n_chain,n_draw,acov,chain_mean, mean_var, var_plus,rho_hat_t,rho_hat_even,rho_hat_odd,relative)\n",
    "    return ess\n",
    "\n",
    "    # Geyer's initial positive sequence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_ess_new(data), es(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.6 ms ± 988 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ess_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.9 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit es(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ess cannot be improved further.There is no point in testing other _ess methods as all of them involve _ess func at one point or another\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
