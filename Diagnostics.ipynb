{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "from arviz.data import convert_to_dataset\n",
    "from arviz.stats.diagnostics import bfmi,_bfmi,geweke as ge,rhat as rh,_ess as es\n",
    "from arviz.stats.diagnostics import _rhat,_split_chains,_z_scale\n",
    "from arviz.stats.diagnostics import _rhat_rank as rk\n",
    "from arviz.stats.diagnostics import ks_summary as ks\n",
    "from arviz.stats.diagnostics import _mc_error as me, _mcse_mean, _mcse_quantile, _mcse_sd as msd,_ess_mean,_ess_sd\n",
    "from arviz.utils import conditional_jit\n",
    "from arviz.stats.stats_utils import not_valid as _not_valid, autocov as _autocov\n",
    "import numpy as np\n",
    "from line_profiler import LineProfiler\n",
    "import numba\n",
    "import pandas as pd\n",
    "from scipy.fftpack import next_fast_len\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy import stats\n",
    "from xarray import apply_ufunc\n",
    "import xarray as xr\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 ms ± 10.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.253433 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: bfmi at line 24\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    24                                           def bfmi(data):\n",
      "    25                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "    26                                           \n",
      "    27                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "    28                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "    29                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "    30                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "    31                                               information.\n",
      "    32                                           \n",
      "    33                                               Parameters\n",
      "    34                                               ----------\n",
      "    35                                               data : obj\n",
      "    36                                                   Any object that can be converted to an az.InferenceData object.\n",
      "    37                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "    38                                                   If InferenceData, energy variable needs to be found.\n",
      "    39                                           \n",
      "    40                                               Returns\n",
      "    41                                               -------\n",
      "    42                                               z : array\n",
      "    43                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "    44                                                   chain in the trace.\n",
      "    45                                               \"\"\"\n",
      "    46         1          6.0      6.0      0.0      if isinstance(data, np.ndarray):\n",
      "    47         1     253427.0 253427.0    100.0          return _bfmi(data)\n",
      "    48                                           \n",
      "    49                                               dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
      "    50                                               if not hasattr(dataset, \"energy\"):\n",
      "    51                                                   raise TypeError(\"Energy variable was not found.\")\n",
      "    52                                               return _bfmi(dataset.energy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.242757 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: _bfmi at line 475\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   475                                           def _bfmi(energy):\n",
      "   476                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "   477                                           \n",
      "   478                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "   479                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "   480                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "   481                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "   482                                               information.\n",
      "   483                                           \n",
      "   484                                               Parameters\n",
      "   485                                               ----------\n",
      "   486                                               energy : NumPy array\n",
      "   487                                                   Should be extracted from a gradient based sampler, such as in Stan or PyMC3. Typically,\n",
      "   488                                                   after converting a trace or fit to InferenceData, the energy will be in\n",
      "   489                                                   `data.sample_stats.energy`.\n",
      "   490                                           \n",
      "   491                                               Returns\n",
      "   492                                               -------\n",
      "   493                                               z : array\n",
      "   494                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "   495                                                   chain in the trace.\n",
      "   496                                               \"\"\"\n",
      "   497         1         24.0     24.0      0.0      energy_mat = np.atleast_2d(energy)\n",
      "   498         1     130948.0 130948.0     53.9      num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
      "   499         1     111761.0 111761.0     46.0      den = np.var(energy_mat, axis=1)\n",
      "   500         1         24.0     24.0      0.0      return num / den\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "def _bfmi_jit(energy):\n",
    "    energy_mat = np.atleast_2d(energy)\n",
    "    if energy_mat.ndim==2:\n",
    "        num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
    "        den = _var_2d(energy_mat)\n",
    "    return num / den\n",
    "\n",
    "def bfmi_new(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return _bfmi_jit(data)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
    "    if not hasattr(dataset, \"energy\"):\n",
    "        raise TypeError(\"Energy variable was not found.\")\n",
    "    return _bfmi_jit(dataset.energy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.diagnostics.bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.3 ms ± 565 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 ms ± 1.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 ms ± 855 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Much better improvement on larger datasets. A gain on a few milliseconds on school'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Much better improvement on larger datasets. A gain on a few milliseconds on school'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ks_summary'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ks_summary\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10_00_00,1000)\n",
    "school  = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ks_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 11.8666 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ks_summary at line 448\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   448                                           def ks_summary(pareto_tail_indices):\n",
      "   449                                               \"\"\"Display a summary of Pareto tail indices.\n",
      "   450                                           \n",
      "   451                                               Parameters\n",
      "   452                                               ----------\n",
      "   453                                               pareto_tail_indices : array\n",
      "   454                                                 Pareto tail indices.\n",
      "   455                                           \n",
      "   456                                               Returns\n",
      "   457                                               -------\n",
      "   458                                               df_k : dataframe\n",
      "   459                                                 Dataframe containing k diagnostic values.\n",
      "   460                                               \"\"\"\n",
      "   461         1   11859142.0 11859142.0     99.9      kcounts, _ = np.histogram(pareto_tail_indices, bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
      "   462         1         34.0     34.0      0.0      kprop = kcounts / len(pareto_tail_indices) * 100\n",
      "   463         1         18.0     18.0      0.0      df_k = pd.DataFrame(\n",
      "   464         1       4406.0   4406.0      0.0          dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
      "   465         1       2927.0   2927.0      0.0      ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
      "   466                                           \n",
      "   467         1         74.0     74.0      0.0      if np.sum(kcounts[1:]) == 0:\n",
      "   468                                                   warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
      "   469         1         33.0     33.0      0.0      elif np.sum(kcounts[2:]) == 0:\n",
      "   470                                                   warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
      "   471                                           \n",
      "   472         1          4.0      4.0      0.0      return df_k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ks)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottleneck ar np.historgram'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bottleneck ar np.historgram'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _histogram(data):\n",
    "    kcounts, _ = np.histogram(data,bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
    "    return kcounts\n",
    "\n",
    "\n",
    "def ks_summary_new(pareto_tail_indices):\n",
    "    kcounts = _histogram(pareto_tail_indices)\n",
    "    kprop = kcounts / len(pareto_tail_indices) * 100\n",
    "    df_k = pd.DataFrame(\n",
    "        dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
    "    ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
    "\n",
    "    if np.sum(kcounts[1:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
    "    elif np.sum(kcounts[2:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
    "\n",
    "    return df_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_</th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-Inf, 0.5]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 0.7]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.7, 1]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, Inf)</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                _  Count   Pct\n",
       "(-Inf, 0.5]  True   True  True\n",
       " (0.5, 0.7]  True   True  True\n",
       "   (0.7, 1]  True   True  True\n",
       "   (1, Inf)  True   True  True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_summary_new(data)==ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 s ± 27.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 s ± 50.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.42 ms ± 31.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 ms ± 24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 s ± 70.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHEW. 10 times faster on large datasets. Much better performance on every dataset'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''PHEW. 10 times faster on large datasets. Much better performance on every dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"geweke'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"geweke\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000000)\n",
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "school = school[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 8.24005 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: geweke at line 376\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   376                                           def geweke(ary, first=0.1, last=0.5, intervals=20):\n",
      "   377                                               r\"\"\"Compute z-scores for convergence diagnostics.\n",
      "   378                                           \n",
      "   379                                               Compare the mean of the first % of series with the mean of the last % of series. x is divided\n",
      "   380                                               into a number of segments for which this difference is computed. If the series is converged,\n",
      "   381                                               this score should oscillate between -1 and 1.\n",
      "   382                                           \n",
      "   383                                               Parameters\n",
      "   384                                               ----------\n",
      "   385                                               ary : 1D array-like\n",
      "   386                                                 The trace of some stochastic parameter.\n",
      "   387                                               first : float\n",
      "   388                                                 The fraction of series at the beginning of the trace.\n",
      "   389                                               last : float\n",
      "   390                                                 The fraction of series at the end to be compared with the section\n",
      "   391                                                 at the beginning.\n",
      "   392                                               intervals : int\n",
      "   393                                                 The number of segments.\n",
      "   394                                           \n",
      "   395                                               Returns\n",
      "   396                                               -------\n",
      "   397                                               scores : list [[]]\n",
      "   398                                                 Return a list of [i, score], where i is the starting index for each interval and score the\n",
      "   399                                                 Geweke score on the interval.\n",
      "   400                                           \n",
      "   401                                               Notes\n",
      "   402                                               -----\n",
      "   403                                               The Geweke score on some series x is computed by:\n",
      "   404                                           \n",
      "   405                                                 .. math:: \\frac{E[x_s] - E[x_e]}{\\sqrt{V[x_s] + V[x_e]}}\n",
      "   406                                           \n",
      "   407                                               where :math:`E` stands for the mean, :math:`V` the variance,\n",
      "   408                                               :math:`x_s` a section at the start of the series and\n",
      "   409                                               :math:`x_e` a section at the end of the series.\n",
      "   410                                           \n",
      "   411                                               References\n",
      "   412                                               ----------\n",
      "   413                                               Geweke (1992)\n",
      "   414                                               \"\"\"\n",
      "   415                                               # Filter out invalid intervals\n",
      "   416         3         16.0      5.3      0.0      for interval in (first, last):\n",
      "   417         2          8.0      4.0      0.0          if interval <= 0 or interval >= 1:\n",
      "   418                                                       raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   419         1          4.0      4.0      0.0      if first + last >= 1:\n",
      "   420                                                   raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   421                                           \n",
      "   422                                               # Initialize list of z-scores\n",
      "   423         1          3.0      3.0      0.0      zscores = []\n",
      "   424                                           \n",
      "   425                                               # Last index value\n",
      "   426         1          5.0      5.0      0.0      end = len(ary) - 1\n",
      "   427                                           \n",
      "   428                                               # Start intervals going up to the <last>% of the chain\n",
      "   429         1          4.0      4.0      0.0      last_start_idx = (1 - last) * end\n",
      "   430                                           \n",
      "   431                                               # Calculate starting indices\n",
      "   432         1        294.0    294.0      0.0      start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
      "   433                                           \n",
      "   434                                               # Loop over start indices\n",
      "   435       201        599.0      3.0      0.0      for start in start_indices:\n",
      "   436                                                   # Calculate slices\n",
      "   437       200       6601.0     33.0      0.1          first_slice = ary[start : start + int(first * (end - start))]\n",
      "   438       200       2658.0     13.3      0.0          last_slice = ary[int(end - last * (end - start)) :]\n",
      "   439                                           \n",
      "   440       200     916452.0   4582.3     11.1          z_score = first_slice.mean() - last_slice.mean()\n",
      "   441       200    7312083.0  36560.4     88.7          z_score /= np.sqrt(first_slice.var() + last_slice.var())\n",
      "   442                                           \n",
      "   443       200       1170.0      5.8      0.0          zscores.append([start, z_score])\n",
      "   444                                           \n",
      "   445         1        155.0    155.0      0.0      return np.array(zscores)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ge)\n",
    "wrapper(data,0.1,0.5,200)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "\n",
    "@numba.vectorize(nopython=True)   ##Remember to make a conditional_vectorize function\n",
    "def _sqr(a,b):\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "\n",
    "@conditional_jit\n",
    "def geweke_new(ary, first=0.1, last=0.5, intervals=20):\n",
    "    # Filter out invalid intervals\n",
    "    for interval in (first, last):\n",
    "        if interval <= 0 or interval >= 1:\n",
    "            raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "    if first + last >= 1:\n",
    "        raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "\n",
    "    # Initialize list of z-scores\n",
    "    zscores = []\n",
    "\n",
    "    # Last index value\n",
    "    end = len(ary) - 1\n",
    "\n",
    "    # Start intervals going up to the <last>% of the chain\n",
    "    last_start_idx = (1 - last) * end\n",
    "\n",
    "    # Calculate starting indices\n",
    "    start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
    "\n",
    "    # Loop over start indices\n",
    "    for start in start_indices:\n",
    "        # Calculate slices\n",
    "        first_slice = ary[start : start + int(first * (end - start))]\n",
    "        last_slice = ary[int(end - last * (end - start)) :]\n",
    "\n",
    "        z_score = first_slice.mean() - last_slice.mean()\n",
    "        D = _sqr(_var_1d(first_slice), _var_1d(last_slice))\n",
    "        z_score = z_score/D\n",
    "\n",
    "        zscores.append([start, z_score])\n",
    "\n",
    "    return np.array(zscores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 ms ± 10.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882 ms ± 41.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74 s ± 67.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 s ± 518 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(geweke_new(data,intervals=500), az.geweke(data,intervals=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 ms ± 138 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 ms ± 94.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"GWEKE WORKS WELL'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"GWEKE WORKS WELL\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ess'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ess\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 ms ± 29.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.566543 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          3.0      3.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          2.0      2.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          2.0      2.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          1.0      1.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          1.0      1.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          2.0      2.0      0.0          \"median\": _ess_median,\n",
      "   180         1          1.0      1.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          1.0      1.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          1.0      1.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          2.0      2.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          2.0      2.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          2.0      2.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          2.0      2.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          3.0      3.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196         1         16.0     16.0      0.0          data = np.atleast_2d(data)\n",
      "   197         1          3.0      3.0      0.0          if len(data.shape) < 3:\n",
      "   198         1          2.0      2.0      0.0              if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203         1     566497.0 566497.0    100.0                  return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211                                               dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212                                               var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214                                               dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216                                               ufunc_kwargs = {\"ravel\": False}\n",
      "   217                                               func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218                                               return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.590619 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          3.0      3.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          2.0      2.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          1.0      1.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          1.0      1.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          1.0      1.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          1.0      1.0      0.0          \"median\": _ess_median,\n",
      "   180         1          2.0      2.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          2.0      2.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          1.0      1.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          3.0      3.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          2.0      2.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          2.0      2.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          1.0      1.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          3.0      3.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196                                                   data = np.atleast_2d(data)\n",
      "   197                                                   if len(data.shape) < 3:\n",
      "   198                                                       if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203                                                           return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211         1       3609.0   3609.0      0.6      dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212         1         21.0     21.0      0.0      var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214         1          4.0      4.0      0.0      dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216         1          4.0      4.0      0.0      ufunc_kwargs = {\"ravel\": False}\n",
      "   217         1          4.0      4.0      0.0      func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218         1     586952.0 586952.0     99.4      return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(dict_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= np.conjugate(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocov(numpy_data,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.0412 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats_utils.py\n",
      "Function: autocov at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           def autocov(ary, axis=-1):\n",
      "    17                                               \"\"\"Compute autocovariance estimates for every lag for the input array.\n",
      "    18                                           \n",
      "    19                                               Parameters\n",
      "    20                                               ----------\n",
      "    21                                               ary : Numpy array\n",
      "    22                                                   An array containing MCMC samples\n",
      "    23                                           \n",
      "    24                                               Returns\n",
      "    25                                               -------\n",
      "    26                                               acov: Numpy array same size as the input array\n",
      "    27                                               \"\"\"\n",
      "    28         1          6.0      6.0      0.0      axis = axis if axis > 0 else len(ary.shape) + axis\n",
      "    29         1          2.0      2.0      0.0      n = ary.shape[axis]\n",
      "    30         1         23.0     23.0      0.1      m = next_fast_len(2 * n)\n",
      "    31                                           \n",
      "    32         1       4058.0   4058.0      9.8      ary = ary - ary.mean(axis, keepdims=True)\n",
      "    33                                           \n",
      "    34                                               # added to silence tuple warning for a submodule\n",
      "    35         1         37.0     37.0      0.1      with warnings.catch_warnings():\n",
      "    36         1         29.0     29.0      0.1          warnings.simplefilter(\"ignore\")\n",
      "    37                                           \n",
      "    38         1      13288.0  13288.0     32.3          ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
      "    39         1      11970.0  11970.0     29.1          ifft_ary *= np.conjugate(ifft_ary)\n",
      "    40                                           \n",
      "    41         1          4.0      4.0      0.0          shape = tuple(\n",
      "    42         1         47.0     47.0      0.1              slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
      "    43                                                   )\n",
      "    44         1       6593.0   6593.0     16.0          cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
      "    45         1       5140.0   5140.0     12.5          cov /= n\n",
      "    46                                           \n",
      "    47         1          3.0      3.0      0.0      return cov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.autocov)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck :: np.fft.rfft and np.conjugate'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Bottleneck :: np.fft.rfft and np.conjugate'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fft(x,m,axis):\n",
    "    return np.fft.rfft(x,n=m,axis=axis)\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def _fft_new(x,m,axis):\n",
    "    N =  np.fft.rfft(x,n=m,axis=axis)\n",
    "    return N\n",
    "\n",
    "def conjuc(data):\n",
    "    return np.conjugate(data)\n",
    "\n",
    "\n",
    "@numba.vectorize\n",
    "def nconjuc(data):\n",
    "    return np.conjugate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 ms ± 883 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 ms ± 728 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft_new(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jitting fft is of no use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.91 ms ± 70.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.conjugate([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.88 ms ± 103 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nconjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov_new(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= nconjuc(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 ms ± 689 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit autocov_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 ms ± 10.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (az.autocov(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    size = ary.size\n",
    "    rank = stats.rankdata(ary, method=\"average\")\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(ary.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.536373 s\n",
      "File: <ipython-input-62-21c5aaeb33f5>\n",
      "Function: _z_scale at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def _z_scale(ary):\n",
      "     2         1         16.0     16.0      0.0      ary = np.asarray(ary)\n",
      "     3         1          4.0      4.0      0.0      size = ary.size\n",
      "     4         1     361312.0 361312.0     67.4      rank = stats.rankdata(ary, method=\"average\")\n",
      "     5         1     175021.0 175021.0     32.6      z = stats.norm.ppf((rank - 0.5) / size)\n",
      "     6         1         18.0     18.0      0.0      z = z.reshape(ary.shape)\n",
      "     7         1          2.0      2.0      0.0      return z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_z_scale)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def summ(x):\n",
    "    return x.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankdata_new(arr):\n",
    "    arr = np.ravel(arr)\n",
    "    sorter = np.argsort(arr, kind=\"quicksort\")\n",
    "    inv = np.empty(sorter.size, dtype=np.intp)\n",
    "    inv = inv_sorter(inv,sorter)\n",
    "    arr = arr[sorter]\n",
    "    obs = np.r_[True, arr[1:] != arr[:-1]]\n",
    "    dense = summ(obs)[inv]\n",
    "    count = np.r_[np.nonzero(obs)[0], len(obs)]\n",
    "    return av(count,dense)\n",
    "\n",
    "\n",
    "@numba.njit(cache=True)\n",
    "def inv_sorter(inv,sorter):\n",
    "    inv[sorter] = np.arange(sorter.size)\n",
    "    return inv\n",
    "\n",
    "@numba.njit(cache=True)\n",
    "def av(count, dense):\n",
    "    return .5 * (count[dense] + count[dense - 1] + 1)\n",
    "\n",
    "def av_nojit(count,dense):\n",
    "    return .5 * (count[dense] + count[dense - 1] + 1)\n",
    "\n",
    "\n",
    "def no_jit_inv_sorter(inv,sorter):\n",
    "    inv[sorter] = np.arange(sorter.size)\n",
    "    return inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.2 ms ± 2.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit av(np.random.randn(1000000), np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 ms ± 4.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit av_nojit(np.random.randn(1000000), np.random.randint(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   81,  161,  241,  321,  401,  481,  561,  641,  721,  801,\n",
       "        881,  961, 1041, 1121, 1201, 1281, 1361, 1441, 1521, 1601, 1681,\n",
       "       1761, 1841, 1921, 2001, 2081, 2161, 2241, 2321, 2401, 2481, 2561,\n",
       "       2641, 2721, 2801, 2881, 2961, 3041, 3121, 3201, 3281, 3361, 3441,\n",
       "       3521, 3601, 3681, 3761, 3841, 3921, 4001, 4081, 4161, 4241, 4321,\n",
       "       4401, 4481, 4561, 4641, 4721, 4801, 4881, 4961, 5041, 5121, 5201,\n",
       "       5281, 5361, 5441, 5521, 5601, 5681, 5761, 5841, 5921, 6001, 6081,\n",
       "       6161, 6241, 6321, 6401, 6481, 6561, 6641, 6721, 6801, 6881, 6961,\n",
       "       7041, 7121, 7201, 7281, 7361, 7441, 7521, 7601, 7681, 7761, 7841,\n",
       "       7921, 8001, 8081, 8161, 8241, 8321, 8401, 8481, 8561, 8641, 8721,\n",
       "       8801, 8881, 8961, 9041, 9121, 9201, 9281, 9361, 9441, 9521, 9601,\n",
       "       9681, 9761, 9841, 9921])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1,10000,np.random.randint(100))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 1.83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "timeit inv_sorter(np.random.randn(1000000),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 3.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit no_jit_inv_sorter(np.random.randn(1000000),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 1.5518 s\n",
      "File: <ipython-input-65-669031228bc7>\n",
      "Function: rankdata_new at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def rankdata_new(arr):\n",
      "     2         1         27.0     27.0      0.0      arr = np.ravel(arr)\n",
      "     3         1     268656.0 268656.0     17.3      sorter = np.argsort(arr, kind=\"quicksort\")\n",
      "     4         1         27.0     27.0      0.0      inv = np.empty(sorter.size, dtype=np.intp)\n",
      "     5         1     296502.0 296502.0     19.1      inv = inv_sorter(inv,sorter)\n",
      "     6         1      18109.0  18109.0      1.2      arr = arr[sorter]\n",
      "     7         1       1492.0   1492.0      0.1      obs = np.r_[True, arr[1:] != arr[:-1]]\n",
      "     8         1     355038.0 355038.0     22.9      dense = summ(obs)[inv]\n",
      "     9         1       3801.0   3801.0      0.2      count = np.r_[np.nonzero(obs)[0], len(obs)]\n",
      "    10         1     608149.0 608149.0     39.2      return av(count,dense)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(rankdata_new)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ravel(numpy_data)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.36 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "15.1 µs ± 12.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summ(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1 µs ± 878 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cumsum(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(summ(x)==np.cumsum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 ms ± 13.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rankdata_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 ms ± 7.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stats.rankdata(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 µs ± 34.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rankdata_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 µs ± 11.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stats.rankdata(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(rankdata_new(school)==stats.rankdata(school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([889571., 101192., 740473., ..., 706380., 974936., 150401.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankdata_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(stats.rankdata(x)==rankdata_new(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    size= ary.size\n",
    "    rank = rankdata_new(ary)\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(ary.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 ms ± 36.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 ms ± 47.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_z_scale_new(numpy_data),_z_scale(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 2.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 ms ± 11.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 ms ± 5.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms ± 1.78 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''z_scale works well on large sets. It's a bit slow on schools though\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''z_scale works well on large sets. It's a bit slow on schools though'''''''''''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/(len(data))-((a/(len(data)))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "\n",
    "def _rhat_new(ary):\n",
    "    ary = np.asarray(ary, dtype=float)\n",
    "    if _not_valid(ary, check_shape=False):\n",
    "        return np.nan\n",
    "    _, num_samples = ary.shape\n",
    "\n",
    "    # Calculate chain mean\n",
    "    chain_mean = np.mean(ary, axis=1)\n",
    "    # Calculate chain variance\n",
    "    chain_var = _var_2d(ary)\n",
    "    # Calculate between-chain variance\n",
    "    between_chain_variance = num_samples * _var_1d(chain_mean)\n",
    "    # Calculate within-chain variance\n",
    "    within_chain_variance = np.mean(chain_var)\n",
    "    # Estimate of marginal posterior variance\n",
    "    rhat_value = np.sqrt(\n",
    "        (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
    "    )\n",
    "    return rhat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999988804978854"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999993309362801"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.01563316512969"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0115252895535172"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rhat_rank_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    split_ary = _split_chains(ary)\n",
    "    rhat_bulk = _rhat_new(_z_scale_new(split_ary))\n",
    "\n",
    "    split_ary_folded = abs(split_ary - np.median(split_ary))\n",
    "    rhat_tail = _rhat_new(_z_scale_new(split_ary_folded))\n",
    "\n",
    "    rhat_rank = max(rhat_bulk, rhat_tail)\n",
    "    return rhat_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.37711934, 12.14856451, 11.91083021, 12.97533472])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(school,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.37711934, 12.14856451, 11.91083021, 12.97533472])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0115252895535172"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.01563316512969"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rhat(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 s ± 53.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat_rank_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 s ± 16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rk(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.43 ms ± 49.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit _rhat_rank_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.84 ms ± 76.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit rk(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_fold(ary):\n",
    "    \"\"\"Fold and z-scale values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    ary = abs(ary - np.median(ary))\n",
    "    ary = _z_scale_new(ary)\n",
    "    return ary\n",
    "\n",
    "\n",
    "def _rhat_folded_new(ary):\n",
    "    \"\"\"Calculate split-Rhat for folded z-values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    ary = _z_fold(_split_chains(ary))\n",
    "    return _rhat_new(ary)\n",
    "\n",
    "\n",
    "def _rhat_z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_z_scale_new(_split_chains(ary)))\n",
    "\n",
    "\n",
    "def _rhat_split_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_split_chains(ary))\n",
    "\n",
    "\n",
    "def _rhat_identity_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(ary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_new(data, *, var_names=None, method=\"rank\"):\n",
    "\n",
    "    methods = {\n",
    "        \"rank\": _rhat_rank_new,\n",
    "        \"split\": _rhat_split_new,\n",
    "        \"folded\": _rhat_folded_new,\n",
    "        \"z_scale\": _rhat_z_scale_new,\n",
    "        \"identity\": _rhat_identity_new,\n",
    "    }\n",
    "    if method not in methods:\n",
    "        raise TypeError(\n",
    "            \"R-hat method {} not found. Valid methods are:\\n{}\".format(\n",
    "                method, \"\\n    \".join(methods)\n",
    "            )\n",
    "        )\n",
    "    rhat_func = methods[method]\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.atleast_2d(data)\n",
    "        if len(data.shape) < 3:\n",
    "            return rhat_func(data)\n",
    "        else:\n",
    "            msg = (\n",
    "                \"Only uni-dimensional ndarray variables are supported.\"\n",
    "                \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
    "            )\n",
    "            raise TypeError(msg)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"posterior\")\n",
    "    var_names = _var_names(var_names, dataset)\n",
    "\n",
    "    dataset = dataset if var_names is None else dataset[var_names]\n",
    "\n",
    "    ufunc_kwargs = {\"ravel\": False}\n",
    "    func_kwargs = {}\n",
    "    return _wrap_xarray_ufunc(\n",
    "        rhat_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(10000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 s ± 1.14 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 s ± 967 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 ms ± 8.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 ms ± 3.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.06 s ± 113 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.76 s ± 93.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69 s ± 116 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.63 s ± 155 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ms ± 5.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.5 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.74 ms ± 138 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.65 ms ± 169 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 µs ± 34.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 µs ± 4.97 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 ms ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 ms ± 85 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 ms ± 56 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19 ms ± 37.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 µs ± 28.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(school, method='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 µs ± 3.88 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(school, method='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Improvement in rhat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\'ESS'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'ESS\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def loop_lifter(n_chain,n_draw,acov,chain_mean, mean_var, var_plus,rho_hat_t,rho_hat_even,rho_hat_odd,relative):\n",
    "    t = 1\n",
    "    while t < (n_draw - 3) and (rho_hat_even + rho_hat_odd) > 0.0:\n",
    "        rho_hat_even = 1.0 - (mean_var - np.mean(acov[:, t + 1])) / var_plus\n",
    "        rho_hat_odd = 1.0 - (mean_var - np.mean(acov[:, t + 2])) / var_plus\n",
    "        if (rho_hat_even + rho_hat_odd) >= 0:\n",
    "            rho_hat_t[t + 1] = rho_hat_even\n",
    "            rho_hat_t[t + 2] = rho_hat_odd\n",
    "        t += 2\n",
    "\n",
    "    max_t = t - 2\n",
    "    # improve estimation\n",
    "    if rho_hat_even > 0:\n",
    "        rho_hat_t[max_t + 1] = rho_hat_even\n",
    "    # Geyer's initial monotone sequence\n",
    "    t = 1\n",
    "    while t <= max_t - 2:\n",
    "        if (rho_hat_t[t + 1] + rho_hat_t[t + 2]) > (rho_hat_t[t - 1] + rho_hat_t[t]):\n",
    "            rho_hat_t[t + 1] = (rho_hat_t[t - 1] + rho_hat_t[t]) / 2.0\n",
    "            rho_hat_t[t + 2] = rho_hat_t[t + 1]\n",
    "        t += 2\n",
    "\n",
    "    ess = n_chain * n_draw\n",
    "    tau_hat = -1.0 + 2.0 * np.sum(rho_hat_t[: max_t + 1]) + np.sum(rho_hat_t[max_t + 1 : max_t + 2])\n",
    "    tau_hat = max(tau_hat, 1 / np.log10(ess))\n",
    "    ess = (1 if relative else ess) / tau_hat\n",
    "    if np.isnan(rho_hat_t).any():\n",
    "        ess = np.nan\n",
    "    return ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _ess_new(ary, relative=False):\n",
    "    \"\"\"Compute the effective sample size for a 2D array.\"\"\"\n",
    "    ary = np.asarray(ary, dtype=float)\n",
    "    if _not_valid(ary, check_shape=False):\n",
    "        return np.nan\n",
    "    if (np.max(ary) - np.min(ary)) < np.finfo(float).resolution:  # pylint: disable=no-member\n",
    "        return ary.size\n",
    "    if len(ary.shape) < 2:\n",
    "        ary = np.atleast_2d(ary)\n",
    "    n_chain, n_draw = ary.shape\n",
    "    acov = _autocov(ary, axis=1)\n",
    "    chain_mean = ary.mean(axis=1)\n",
    "    mean_var = np.mean(acov[:, 0]) * n_draw / (n_draw - 1.0)\n",
    "    var_plus = mean_var * (n_draw - 1.0) / n_draw\n",
    "    if n_chain > 1:\n",
    "        var_plus += np.var(chain_mean, ddof=1)\n",
    "\n",
    "    rho_hat_t = np.zeros(n_draw)\n",
    "    rho_hat_even = 1.0\n",
    "    rho_hat_t[0] = rho_hat_even\n",
    "    rho_hat_odd = 1.0 - (mean_var - np.mean(acov[:, 1])) / var_plus\n",
    "    rho_hat_t[1] = rho_hat_odd\n",
    "    \n",
    "    ess = loop_lifter(n_chain,n_draw,acov,chain_mean, mean_var, var_plus,rho_hat_t,rho_hat_even,rho_hat_odd,relative)\n",
    "    return ess\n",
    "\n",
    "    # Geyer's initial positive sequence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_ess_new(data), es(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.6 ms ± 369 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ess_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.9 ms ± 601 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit es(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ess cannot be improved further.There is no point in testing other _ess methods as all of them involve _ess func at one point or another\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000)\n",
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.07305 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: _mcse_mean at line 788\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   788                                           def _mcse_mean(ary):\n",
      "   789                                               \"\"\"Compute the Markov Chain mean error.\"\"\"\n",
      "   790         1          8.0      8.0      0.0      ary = np.asarray(ary)\n",
      "   791         1       1765.0   1765.0      2.4      if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=1)):\n",
      "   792                                                   return np.nan\n",
      "   793         1      64440.0  64440.0     88.2      ess = _ess_mean(ary)\n",
      "   794         1       6828.0   6828.0      9.3      sd = np.std(ary, ddof=1)\n",
      "   795         1          8.0      8.0      0.0      mcse_mean_value = sd / np.sqrt(ess)\n",
      "   796         1          1.0      1.0      0.0      return mcse_mean_value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_mcse_mean)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.76 ms ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit np.sqrt(_var_2d(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8 ms ± 507 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit np.std(data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.allclose(np.sqrt(_var_2d(data)),np.std(data,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mcse_mean_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=1)):\n",
    "        return np.nan\n",
    "    ess = _ess_mean(ary)\n",
    "    ary = np.ravel(ary)\n",
    "    sd = np.sqrt(_var_1d(ary))\n",
    "    mcse_mean_value = sd / np.sqrt(ess)\n",
    "    return mcse_mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005144.2729617809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000005000003886"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mcse_mean(data)/_mcse_mean_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.70242922210593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.000250093789082"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mcse_mean(school)/_mcse_mean_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988702489207812"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(_var_1d(np.ravel(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988707483562937"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4858944648051677"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(_var_1d(np.ravel(school)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4867662653602105"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(school,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "1005144.2729617809\n",
      "70.2 ms ± 7.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.2 ms ± 5.81 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_mean_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58 ms ± 128 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_mean_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "194.70242922210593\n",
      "2.9 ms ± 334 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_mean(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mcse_sd_new(ary):\n",
    "    \"\"\"Compute the Markov Chain sd error.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=1)):\n",
    "        return np.nan\n",
    "    ess = _ess_sd(ary)\n",
    "    ary = np.ravel(ary)\n",
    "    sd = np.sqrt(_var_1d(ary))\n",
    "    fac_mcse_sd = np.sqrt(np.exp(1) * (1 - 1 / ess) ** (ess - 1) - 1)\n",
    "    mcse_sd_value = sd * fac_mcse_sd\n",
    "    return mcse_sd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007052742425925257"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mcse_sd_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000705274595229921"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1856619288742948"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mcse_sd_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18570836176957523"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msd(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms ± 3.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_sd_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ms ± 4.88 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit msd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 ms ± 100 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mcse_sd_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ms ± 90 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit msd(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _circfuncs_common(samples, high, low):\n",
    "    samples = np.asarray(samples)\n",
    "    if samples.size == 0:\n",
    "        return np.nan, np.nan\n",
    "    return samples, angle(samples, low,high,np.pi)\n",
    "\n",
    "@numba.vectorize(nopython=True)\n",
    "def angle(samples,low,high,pi=np.pi):\n",
    "    ang = (samples - low)*2.*pi / (high - low)\n",
    "    return ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _circular_standard_deviation(samples, high=2*np.pi, low=0, axis=None):\n",
    "    pi = np.pi\n",
    "    samples, ang = _circfuncs_common(samples, high, low)\n",
    "    S = np.sin(ang).mean(axis=axis)\n",
    "    C = np.cos(ang).mean(axis=axis)\n",
    "    R = np.hypot(S, C)\n",
    "    return ((high - low)/2.0/pi) * np.sqrt(-2*np.log(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.246811 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: _mc_error at line 820\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   820                                           def _mc_error(ary, batches=5, circular=False):\n",
      "   821                                               \"\"\"Calculate the simulation standard error, accounting for non-independent samples.\n",
      "   822                                           \n",
      "   823                                               The trace is divided into batches, and the standard deviation of the batch\n",
      "   824                                               means is calculated.\n",
      "   825                                           \n",
      "   826                                               Parameters\n",
      "   827                                               ----------\n",
      "   828                                               ary : Numpy array\n",
      "   829                                                   An array containing MCMC samples\n",
      "   830                                               batches : integer\n",
      "   831                                                   Number of batches\n",
      "   832                                               circular : bool\n",
      "   833                                                   Whether to compute the error taking into account `ary` is a circular variable\n",
      "   834                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   835                                           \n",
      "   836                                               Returns\n",
      "   837                                               -------\n",
      "   838                                               mc_error : float\n",
      "   839                                                   Simulation standard error\n",
      "   840                                               \"\"\"\n",
      "   841      1001       2026.0      2.0      0.8      if ary.ndim > 1:\n",
      "   842                                           \n",
      "   843         1         10.0     10.0      0.0          dims = np.shape(ary)\n",
      "   844         1       8743.0   8743.0      3.5          trace = np.transpose([t.ravel() for t in ary])\n",
      "   845                                           \n",
      "   846         1         20.0     20.0      0.0          return np.reshape([_mc_error(t, batches) for t in trace], dims[1:])\n",
      "   847                                           \n",
      "   848                                               else:\n",
      "   849      1000      50257.0     50.3     20.4          if _not_valid(ary, check_shape=False):\n",
      "   850                                                       return np.nan\n",
      "   851      1000       1903.0      1.9      0.8          if batches == 1:\n",
      "   852                                                       if circular:\n",
      "   853                                                           std = stats.circstd(ary, high=np.pi, low=-np.pi)\n",
      "   854                                                       else:\n",
      "   855                                                           std = np.std(ary)\n",
      "   856                                                       return std / np.sqrt(len(ary))\n",
      "   857                                           \n",
      "   858      1000      53048.0     53.0     21.5          batched_traces = np.resize(ary, (batches, int(len(ary) / batches)))\n",
      "   859                                           \n",
      "   860      1000       1750.0      1.8      0.7          if circular:\n",
      "   861                                                       means = stats.circmean(batched_traces, high=np.pi, low=-np.pi, axis=1)\n",
      "   862                                                       std = stats.circstd(means, high=np.pi, low=-np.pi)\n",
      "   863                                                   else:\n",
      "   864      1000      44530.0     44.5     18.0              means = np.mean(batched_traces, 1)\n",
      "   865      1000      78666.0     78.7     31.9              std = np.std(means)\n",
      "   866                                           \n",
      "   867      1000       5858.0      5.9      2.4          return std / np.sqrt(batches)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(me)\n",
    "wrapper(data,20,True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mc_error_new(ary, batches=5, circular=False):\n",
    "    if ary.ndim > 1:\n",
    "\n",
    "        dims = np.shape(ary)\n",
    "        trace = np.transpose([t.ravel() for t in ary])\n",
    "\n",
    "        return np.reshape([_mc_error(t, batches) for t in trace], dims[1:])\n",
    "\n",
    "    else:\n",
    "        if _not_valid(ary, check_shape=False):\n",
    "            return np.nan\n",
    "        if batches == 1:\n",
    "            if circular:\n",
    "                std = _circular_standard_deviation(ary, high=np.pi, low=-np.pi)\n",
    "            else:\n",
    "                std = np.sqrt(_var_1d(ary))\n",
    "            return std / np.sqrt(len(ary))\n",
    "\n",
    "        batched_traces = np.resize(ary, (batches, int(len(ary) / batches)))\n",
    "\n",
    "        if circular:\n",
    "            means = stats.circmean(batched_traces, high=np.pi, low=-np.pi, axis=1)\n",
    "            std = _circular_standard_deviation(means, high=np.pi, low=-np.pi)\n",
    "        else:\n",
    "            means = np.mean(batched_traces, 1)\n",
    "            std = np.sqrt(_var_1d(means))\n",
    "\n",
    "        return std / np.sqrt(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms ± 11.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mc_error_new(data, circular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 ms ± 3.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit me(data, circular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.6 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _mc_error_new(school, circular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.8 ms ± 4.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit me(school, circular=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################DONE####################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
