{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "from arviz.data import convert_to_dataset\n",
    "from arviz.stats.diagnostics import bfmi,_bfmi,geweke as ge,rhat as rh\n",
    "from arviz.stats.diagnostics import _rhat,_split_chains,_z_scale\n",
    "from arviz.stats.diagnostics import _rhat_rank as rk\n",
    "from arviz.stats.diagnostics import ks_summary as ks\n",
    "from arviz.utils import conditional_jit\n",
    "from arviz.stats.stats_utils import not_valid as _not_valid\n",
    "import numpy as np\n",
    "from line_profiler import LineProfiler\n",
    "import numba\n",
    "import pandas as pd\n",
    "from scipy.fftpack import next_fast_len\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy import stats\n",
    "from xarray import apply_ufunc\n",
    "import xarray as xr\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 2.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.223165 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: bfmi at line 24\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    24                                           def bfmi(data):\n",
      "    25                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "    26                                           \n",
      "    27                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "    28                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "    29                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "    30                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "    31                                               information.\n",
      "    32                                           \n",
      "    33                                               Parameters\n",
      "    34                                               ----------\n",
      "    35                                               data : obj\n",
      "    36                                                   Any object that can be converted to an az.InferenceData object.\n",
      "    37                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "    38                                                   If InferenceData, energy variable needs to be found.\n",
      "    39                                           \n",
      "    40                                               Returns\n",
      "    41                                               -------\n",
      "    42                                               z : array\n",
      "    43                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "    44                                                   chain in the trace.\n",
      "    45                                               \"\"\"\n",
      "    46         1          5.0      5.0      0.0      if isinstance(data, np.ndarray):\n",
      "    47         1     223160.0 223160.0    100.0          return _bfmi(data)\n",
      "    48                                           \n",
      "    49                                               dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
      "    50                                               if not hasattr(dataset, \"energy\"):\n",
      "    51                                                   raise TypeError(\"Energy variable was not found.\")\n",
      "    52                                               return _bfmi(dataset.energy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.24117 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: _bfmi at line 475\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   475                                           def _bfmi(energy):\n",
      "   476                                               r\"\"\"Calculate the estimated Bayesian fraction of missing information (BFMI).\n",
      "   477                                           \n",
      "   478                                               BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n",
      "   479                                               information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n",
      "   480                                               values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may\n",
      "   481                                               change. See http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html for more\n",
      "   482                                               information.\n",
      "   483                                           \n",
      "   484                                               Parameters\n",
      "   485                                               ----------\n",
      "   486                                               energy : NumPy array\n",
      "   487                                                   Should be extracted from a gradient based sampler, such as in Stan or PyMC3. Typically,\n",
      "   488                                                   after converting a trace or fit to InferenceData, the energy will be in\n",
      "   489                                                   `data.sample_stats.energy`.\n",
      "   490                                           \n",
      "   491                                               Returns\n",
      "   492                                               -------\n",
      "   493                                               z : array\n",
      "   494                                                   The Bayesian fraction of missing information of the model and trace. One element per\n",
      "   495                                                   chain in the trace.\n",
      "   496                                               \"\"\"\n",
      "   497         1         25.0     25.0      0.0      energy_mat = np.atleast_2d(energy)\n",
      "   498         1     143070.0 143070.0     59.3      num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
      "   499         1      98051.0  98051.0     40.7      den = np.var(energy_mat, axis=1)\n",
      "   500         1         24.0     24.0      0.0      return num / den\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_bfmi)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "def _bfmi_jit(energy):\n",
    "    energy_mat = np.atleast_2d(energy)\n",
    "    if energy_mat.ndim==2:\n",
    "        num = np.square(np.diff(energy_mat, axis=1)).mean(axis=1)  # pylint: disable=no-member\n",
    "        den = _var_2d(energy_mat)\n",
    "    return num / den\n",
    "\n",
    "def bfmi_new(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return _bfmi_jit(data)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"sample_stats\")\n",
    "    if not hasattr(dataset, \"energy\"):\n",
    "        raise TypeError(\"Energy variable was not found.\")\n",
    "    return _bfmi_jit(dataset.energy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 ms ± 3.24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 ms ± 2.03 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.diagnostics.bfmi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.5 ms ± 407 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5 ms ± 638 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(np.random.randn(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.5 ms ± 707 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bfmi_new(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.9 ms ± 917 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.bfmi(az.load_arviz_data(\"centered_eight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Much better improvement on larger datasets. A gain on a few milliseconds on school'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Much better improvement on larger datasets. A gain on a few milliseconds on school'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ks_summary'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ks_summary\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10_00_00,1000)\n",
    "school  = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ks_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 11.1265 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ks_summary at line 448\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   448                                           def ks_summary(pareto_tail_indices):\n",
      "   449                                               \"\"\"Display a summary of Pareto tail indices.\n",
      "   450                                           \n",
      "   451                                               Parameters\n",
      "   452                                               ----------\n",
      "   453                                               pareto_tail_indices : array\n",
      "   454                                                 Pareto tail indices.\n",
      "   455                                           \n",
      "   456                                               Returns\n",
      "   457                                               -------\n",
      "   458                                               df_k : dataframe\n",
      "   459                                                 Dataframe containing k diagnostic values.\n",
      "   460                                               \"\"\"\n",
      "   461         1   11083377.0 11083377.0     99.6      kcounts, _ = np.histogram(pareto_tail_indices, bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
      "   462         1         22.0     22.0      0.0      kprop = kcounts / len(pareto_tail_indices) * 100\n",
      "   463         1         13.0     13.0      0.0      df_k = pd.DataFrame(\n",
      "   464         1      41746.0  41746.0      0.4          dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
      "   465         1       1242.0   1242.0      0.0      ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
      "   466                                           \n",
      "   467         1         38.0     38.0      0.0      if np.sum(kcounts[1:]) == 0:\n",
      "   468                                                   warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
      "   469         1         13.0     13.0      0.0      elif np.sum(kcounts[2:]) == 0:\n",
      "   470                                                   warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
      "   471                                           \n",
      "   472         1          1.0      1.0      0.0      return df_k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ks)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottleneck ar np.historgram'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bottleneck ar np.historgram'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _histogram(data):\n",
    "    kcounts, _ = np.histogram(data,bins=[-np.Inf, 0.5, 0.7, 1, np.Inf])\n",
    "    return kcounts\n",
    "\n",
    "\n",
    "def ks_summary_new(pareto_tail_indices):\n",
    "    kcounts = _histogram(pareto_tail_indices)\n",
    "    kprop = kcounts / len(pareto_tail_indices) * 100\n",
    "    df_k = pd.DataFrame(\n",
    "        dict(_=[\"(good)\", \"(ok)\", \"(bad)\", \"(very bad)\"], Count=kcounts, Pct=kprop)\n",
    "    ).rename(index={0: \"(-Inf, 0.5]\", 1: \" (0.5, 0.7]\", 2: \"   (0.7, 1]\", 3: \"   (1, Inf)\"})\n",
    "\n",
    "    if np.sum(kcounts[1:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are good (k < 0.5)\")\n",
    "    elif np.sum(kcounts[2:]) == 0:\n",
    "        warnings.warn(\"All Pareto k estimates are ok (k < 0.7)\")\n",
    "\n",
    "    return df_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_</th>\n",
       "      <th>Count</th>\n",
       "      <th>Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-Inf, 0.5]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.5, 0.7]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.7, 1]</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, Inf)</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                _  Count   Pct\n",
       "(-Inf, 0.5]  True   True  True\n",
       " (0.5, 0.7]  True   True  True\n",
       "   (0.7, 1]  True   True  True\n",
       "   (1, Inf)  True   True  True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_summary_new(data)==ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34 s ± 6.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8 s ± 12.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 ms ± 4.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.49 ms ± 5.98 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 ms ± 2.79 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks_summary_new(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 s ± 6.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ks(np.random.randn(10000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHEW. 10 times faster on large datasets. Much better performance on every dataset'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''PHEW. 10 times faster on large datasets. Much better performance on every dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"geweke'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"geweke\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(10000000)\n",
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "school = school[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 6.81279 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: geweke at line 376\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   376                                           def geweke(ary, first=0.1, last=0.5, intervals=20):\n",
      "   377                                               r\"\"\"Compute z-scores for convergence diagnostics.\n",
      "   378                                           \n",
      "   379                                               Compare the mean of the first % of series with the mean of the last % of series. x is divided\n",
      "   380                                               into a number of segments for which this difference is computed. If the series is converged,\n",
      "   381                                               this score should oscillate between -1 and 1.\n",
      "   382                                           \n",
      "   383                                               Parameters\n",
      "   384                                               ----------\n",
      "   385                                               ary : 1D array-like\n",
      "   386                                                 The trace of some stochastic parameter.\n",
      "   387                                               first : float\n",
      "   388                                                 The fraction of series at the beginning of the trace.\n",
      "   389                                               last : float\n",
      "   390                                                 The fraction of series at the end to be compared with the section\n",
      "   391                                                 at the beginning.\n",
      "   392                                               intervals : int\n",
      "   393                                                 The number of segments.\n",
      "   394                                           \n",
      "   395                                               Returns\n",
      "   396                                               -------\n",
      "   397                                               scores : list [[]]\n",
      "   398                                                 Return a list of [i, score], where i is the starting index for each interval and score the\n",
      "   399                                                 Geweke score on the interval.\n",
      "   400                                           \n",
      "   401                                               Notes\n",
      "   402                                               -----\n",
      "   403                                               The Geweke score on some series x is computed by:\n",
      "   404                                           \n",
      "   405                                                 .. math:: \\frac{E[x_s] - E[x_e]}{\\sqrt{V[x_s] + V[x_e]}}\n",
      "   406                                           \n",
      "   407                                               where :math:`E` stands for the mean, :math:`V` the variance,\n",
      "   408                                               :math:`x_s` a section at the start of the series and\n",
      "   409                                               :math:`x_e` a section at the end of the series.\n",
      "   410                                           \n",
      "   411                                               References\n",
      "   412                                               ----------\n",
      "   413                                               Geweke (1992)\n",
      "   414                                               \"\"\"\n",
      "   415                                               # Filter out invalid intervals\n",
      "   416         3          8.0      2.7      0.0      for interval in (first, last):\n",
      "   417         2          6.0      3.0      0.0          if interval <= 0 or interval >= 1:\n",
      "   418                                                       raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   419         1          3.0      3.0      0.0      if first + last >= 1:\n",
      "   420                                                   raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
      "   421                                           \n",
      "   422                                               # Initialize list of z-scores\n",
      "   423         1          2.0      2.0      0.0      zscores = []\n",
      "   424                                           \n",
      "   425                                               # Last index value\n",
      "   426         1          4.0      4.0      0.0      end = len(ary) - 1\n",
      "   427                                           \n",
      "   428                                               # Start intervals going up to the <last>% of the chain\n",
      "   429         1          3.0      3.0      0.0      last_start_idx = (1 - last) * end\n",
      "   430                                           \n",
      "   431                                               # Calculate starting indices\n",
      "   432         1        179.0    179.0      0.0      start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
      "   433                                           \n",
      "   434                                               # Loop over start indices\n",
      "   435       201        432.0      2.1      0.0      for start in start_indices:\n",
      "   436                                                   # Calculate slices\n",
      "   437       200       4454.0     22.3      0.1          first_slice = ary[start : start + int(first * (end - start))]\n",
      "   438       200       1845.0      9.2      0.0          last_slice = ary[int(end - last * (end - start)) :]\n",
      "   439                                           \n",
      "   440       200     730166.0   3650.8     10.7          z_score = first_slice.mean() - last_slice.mean()\n",
      "   441       200    6074652.0  30373.3     89.2          z_score /= np.sqrt(first_slice.var() + last_slice.var())\n",
      "   442                                           \n",
      "   443       200        888.0      4.4      0.0          zscores.append([start, z_score])\n",
      "   444                                           \n",
      "   445         1        148.0    148.0      0.0      return np.array(zscores)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(ge)\n",
    "wrapper(data,0.1,0.5,200)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@conditional_jit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "\n",
    "@numba.vectorize(nopython=True)   ##Remember to make a conditional_vectorize function\n",
    "def _sqr(a,b):\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "\n",
    "@conditional_jit\n",
    "def geweke_new(ary, first=0.1, last=0.5, intervals=20):\n",
    "    # Filter out invalid intervals\n",
    "    for interval in (first, last):\n",
    "        if interval <= 0 or interval >= 1:\n",
    "            raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "    if first + last >= 1:\n",
    "        raise ValueError(\"Invalid intervals for Geweke convergence analysis\", (first, last))\n",
    "\n",
    "    # Initialize list of z-scores\n",
    "    zscores = []\n",
    "\n",
    "    # Last index value\n",
    "    end = len(ary) - 1\n",
    "\n",
    "    # Start intervals going up to the <last>% of the chain\n",
    "    last_start_idx = (1 - last) * end\n",
    "\n",
    "    # Calculate starting indices\n",
    "    start_indices = np.linspace(0, last_start_idx, num=intervals, endpoint=True, dtype=int)\n",
    "\n",
    "    # Loop over start indices\n",
    "    for start in start_indices:\n",
    "        # Calculate slices\n",
    "        first_slice = ary[start : start + int(first * (end - start))]\n",
    "        last_slice = ary[int(end - last * (end - start)) :]\n",
    "\n",
    "        z_score = first_slice.mean() - last_slice.mean()\n",
    "        D = _sqr(_var_1d(first_slice), _var_1d(last_slice))\n",
    "        z_score = z_score/D\n",
    "\n",
    "        zscores.append([start, z_score])\n",
    "\n",
    "    return np.array(zscores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 ms ± 727 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 ms ± 8.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.38 s ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2 s ± 270 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(data,intervals=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(geweke_new(data,intervals=500), az.geweke(data,intervals=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 ms ± 13.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geweke_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 ms ± 57.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.geweke(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"GWEKE WORKS WELL'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"GWEKE WORKS WELL\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ess'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ess\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475 ms ± 11.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 ms ± 4.64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.ess(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.50198 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          5.0      5.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          3.0      3.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          2.0      2.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          3.0      3.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          3.0      3.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          2.0      2.0      0.0          \"median\": _ess_median,\n",
      "   180         1          2.0      2.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          2.0      2.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          3.0      3.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          4.0      4.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          3.0      3.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          3.0      3.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          2.0      2.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          6.0      6.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196         1         26.0     26.0      0.0          data = np.atleast_2d(data)\n",
      "   197         1          5.0      5.0      0.0          if len(data.shape) < 3:\n",
      "   198         1          2.0      2.0      0.0              if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203         1     501904.0 501904.0    100.0                  return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211                                               dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212                                               var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214                                               dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216                                               ufunc_kwargs = {\"ravel\": False}\n",
      "   217                                               func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218                                               return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.493085 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/diagnostics.py\n",
      "Function: ess at line 118\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   118                                           def ess(data, *, var_names=None, method=\"bulk\", relative=False, prob=None):\n",
      "   119                                               r\"\"\"Calculate estimate of the effective sample size.\n",
      "   120                                           \n",
      "   121                                               Parameters\n",
      "   122                                               ----------\n",
      "   123                                               data : obj\n",
      "   124                                                   Any object that can be converted to an az.InferenceData object.\n",
      "   125                                                   Refer to documentation of az.convert_to_dataset for details.\n",
      "   126                                                   For ndarray: shape = (chain, draw).\n",
      "   127                                                   For n-dimensional ndarray transform first to dataset with az.convert_to_dataset.\n",
      "   128                                               var_names : list\n",
      "   129                                                   Names of variables to include in the effective_sample_size_mean report\n",
      "   130                                               method : str\n",
      "   131                                                   Select ess method. Valid methods are\n",
      "   132                                                   - \"bulk\"\n",
      "   133                                                   - \"tail\"     # prob, optional\n",
      "   134                                                   - \"quantile\" # prob\n",
      "   135                                                   - \"mean\" (old ess)\n",
      "   136                                                   - \"sd\"\n",
      "   137                                                   - \"median\"\n",
      "   138                                                   - \"mad\" (mean absolute deviance)\n",
      "   139                                                   - \"z_scale\"\n",
      "   140                                                   - \"folded\"\n",
      "   141                                                   - \"identity\"\n",
      "   142                                               relative : bool\n",
      "   143                                                   Return relative ess\n",
      "   144                                                   `ress = ess / N`\n",
      "   145                                               prob : float, optional\n",
      "   146                                                   probability value for \"tail\" and \"quantile\" ess functions.\n",
      "   147                                           \n",
      "   148                                               Returns\n",
      "   149                                               -------\n",
      "   150                                               xarray.Dataset\n",
      "   151                                                   Return the effective sample size for mean, :math:`\\hat{N}_{eff}`\n",
      "   152                                           \n",
      "   153                                               Notes\n",
      "   154                                               -----\n",
      "   155                                               The basic ess diagnostic is computed by:\n",
      "   156                                           \n",
      "   157                                               .. math:: \\hat{N}_{eff} = \\frac{MN}{\\hat{\\tau}}\n",
      "   158                                               .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_t'\n",
      "   159                                           \n",
      "   160                                               where :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag t, and T\n",
      "   161                                               is the first odd positive integer for which the sum\n",
      "   162                                               :math:`\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+1}` is negative.\n",
      "   163                                           \n",
      "   164                                               The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n",
      "   165                                               criterion (Geyer, 1992; Geyer, 2011).\n",
      "   166                                           \n",
      "   167                                               References\n",
      "   168                                               ----------\n",
      "   169                                               Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n",
      "   170                                               https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html Section 15.4.2\n",
      "   171                                               Gelman et al. BDA (2014) Formula 11.8\n",
      "   172                                               \"\"\"\n",
      "   173                                               methods = {\n",
      "   174         1          3.0      3.0      0.0          \"bulk\": _ess_bulk,\n",
      "   175         1          2.0      2.0      0.0          \"tail\": _ess_tail,\n",
      "   176         1          2.0      2.0      0.0          \"quantile\": _ess_quantile,\n",
      "   177         1          2.0      2.0      0.0          \"mean\": _ess_mean,\n",
      "   178         1          2.0      2.0      0.0          \"sd\": _ess_sd,\n",
      "   179         1          2.0      2.0      0.0          \"median\": _ess_median,\n",
      "   180         1          1.0      1.0      0.0          \"mad\": _ess_mad,\n",
      "   181         1          1.0      1.0      0.0          \"z_scale\": _ess_z_scale,\n",
      "   182         1          1.0      1.0      0.0          \"folded\": _ess_folded,\n",
      "   183         1          2.0      2.0      0.0          \"identity\": _ess_identity,\n",
      "   184                                               }\n",
      "   185                                           \n",
      "   186         1          1.0      1.0      0.0      if method not in methods:\n",
      "   187                                                   raise TypeError(\n",
      "   188                                                       \"ESS method {} not found. Valid methods are:\\n{}\".format(method, \"\\n    \".join(methods))\n",
      "   189                                                   )\n",
      "   190         1          2.0      2.0      0.0      ess_func = methods[method]\n",
      "   191                                           \n",
      "   192         1          1.0      1.0      0.0      if (method == \"quantile\") and prob is None:\n",
      "   193                                                   raise TypeError(\"Quantile (prob) information needs to be defined.\")\n",
      "   194                                           \n",
      "   195         1          3.0      3.0      0.0      if isinstance(data, np.ndarray):\n",
      "   196                                                   data = np.atleast_2d(data)\n",
      "   197                                                   if len(data.shape) < 3:\n",
      "   198                                                       if prob is not None:\n",
      "   199                                                           return ess_func(  # pylint: disable=unexpected-keyword-arg\n",
      "   200                                                               data, prob=prob, relative=relative\n",
      "   201                                                           )\n",
      "   202                                                       else:\n",
      "   203                                                           return ess_func(data, relative=relative)\n",
      "   204                                                   else:\n",
      "   205                                                       msg = (\n",
      "   206                                                           \"Only uni-dimensional ndarray variables are supported.\"\n",
      "   207                                                           \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
      "   208                                                       )\n",
      "   209                                                       raise TypeError(msg)\n",
      "   210                                           \n",
      "   211         1       1991.0   1991.0      0.4      dataset = convert_to_dataset(data, group=\"posterior\")\n",
      "   212         1         12.0     12.0      0.0      var_names = _var_names(var_names, dataset)\n",
      "   213                                           \n",
      "   214         1          2.0      2.0      0.0      dataset = dataset if var_names is None else dataset[var_names]\n",
      "   215                                           \n",
      "   216         1          1.0      1.0      0.0      ufunc_kwargs = {\"ravel\": False}\n",
      "   217         1          2.0      2.0      0.0      func_kwargs = {\"relative\": relative} if prob is None else {\"prob\": prob, \"relative\": relative}\n",
      "   218         1     491052.0 491052.0     99.6      return _wrap_xarray_ufunc(ess_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.ess)\n",
    "wrapper(dict_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= np.conjugate(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocov(numpy_data,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.052308 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats_utils.py\n",
      "Function: autocov at line 16\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    16                                           def autocov(ary, axis=-1):\n",
      "    17                                               \"\"\"Compute autocovariance estimates for every lag for the input array.\n",
      "    18                                           \n",
      "    19                                               Parameters\n",
      "    20                                               ----------\n",
      "    21                                               ary : Numpy array\n",
      "    22                                                   An array containing MCMC samples\n",
      "    23                                           \n",
      "    24                                               Returns\n",
      "    25                                               -------\n",
      "    26                                               acov: Numpy array same size as the input array\n",
      "    27                                               \"\"\"\n",
      "    28         1          8.0      8.0      0.0      axis = axis if axis > 0 else len(ary.shape) + axis\n",
      "    29         1          4.0      4.0      0.0      n = ary.shape[axis]\n",
      "    30         1         34.0     34.0      0.1      m = next_fast_len(2 * n)\n",
      "    31                                           \n",
      "    32         1       4654.0   4654.0      8.9      ary = ary - ary.mean(axis, keepdims=True)\n",
      "    33                                           \n",
      "    34                                               # added to silence tuple warning for a submodule\n",
      "    35         1         35.0     35.0      0.1      with warnings.catch_warnings():\n",
      "    36         1         32.0     32.0      0.1          warnings.simplefilter(\"ignore\")\n",
      "    37                                           \n",
      "    38         1      14657.0  14657.0     28.0          ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
      "    39         1      11791.0  11791.0     22.5          ifft_ary *= np.conjugate(ifft_ary)\n",
      "    40                                           \n",
      "    41         1          5.0      5.0      0.0          shape = tuple(\n",
      "    42         1         32.0     32.0      0.1              slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
      "    43                                                   )\n",
      "    44         1      14758.0  14758.0     28.2          cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
      "    45         1       6295.0   6295.0     12.0          cov /= n\n",
      "    46                                           \n",
      "    47         1          3.0      3.0      0.0      return cov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(az.autocov)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck :: np.fft.rfft and np.conjugate'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Bottleneck :: np.fft.rfft and np.conjugate'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fft(x,m,axis):\n",
    "    return np.fft.rfft(x,n=m,axis=axis)\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def _fft_new(x,m,axis):\n",
    "    N =  np.fft.rfft(x,n=m,axis=axis)\n",
    "    return N\n",
    "\n",
    "def conjuc(data):\n",
    "    return np.conjugate(data)\n",
    "\n",
    "\n",
    "@numba.vectorize\n",
    "def nconjuc(data):\n",
    "    return np.conjugate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5 ms ± 46.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.55 ms ± 70.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _fft_new(numpy_data,2000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jitting fft is of no use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4 ms ± 61.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit conjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.conjugate([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 ms ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nconjuc(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocov_new(ary, axis=-1):\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= nconjuc(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.1 ms ± 407 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit autocov_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.2 ms ± 435 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (az.autocov(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(1000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    size = ary.size\n",
    "    rank = stats.rankdata(ary, method=\"average\")\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(ary.shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.46401 s\n",
      "File: <ipython-input-62-21c5aaeb33f5>\n",
      "Function: _z_scale at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def _z_scale(ary):\n",
      "     2         1          7.0      7.0      0.0      ary = np.asarray(ary)\n",
      "     3         1          3.0      3.0      0.0      size = ary.size\n",
      "     4         1     306246.0 306246.0     66.0      rank = stats.rankdata(ary, method=\"average\")\n",
      "     5         1     157742.0 157742.0     34.0      z = stats.norm.ppf((rank - 0.5) / size)\n",
      "     6         1         11.0     11.0      0.0      z = z.reshape(ary.shape)\n",
      "     7         1          1.0      1.0      0.0      return z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_z_scale)\n",
    "wrapper(numpy_data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def rankdata_new(arr):\n",
    "    sorter = np.argsort(arr, kind=\"quicksort\")\n",
    "    inv = np.empty(sorter.size, dtype=np.intp)\n",
    "    inv[sorter] = np.arange(sorter.size, dtype=np.intp)\n",
    "    arr = arr[sorter]\n",
    "    obs = np.r_[True, arr[1:] != arr[:-1]]\n",
    "    dense = summ(obs)[inv]\n",
    "    count = np.r_[np.nonzero(obs)[0], len(obs)]\n",
    "    return .5 * (count[dense] + count[dense - 1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ravel(numpy_data)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def summ(x):\n",
    "    return x.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.63 µs ± 187 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summ(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 µs ± 183 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cumsum(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(summ(x)==np.cumsum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 ms ± 18.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rankdata_new(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 ms ± 20.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stats.rankdata(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(rankdata_new(x), stats.rankdata(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    a_shape = ary.shape\n",
    "    size = ary.size\n",
    "    ary = np.ravel(ary)\n",
    "    rank = rankdata_new(ary)\n",
    "    z = stats.norm.ppf((rank - 0.5) / size)\n",
    "    z = z.reshape(a_shape)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.68 s ± 480 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9 s ± 98.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.alltrue(_z_scale_new(numpy_data)==_z_scale(numpy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.07 s ± 318 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.27 s ± 572 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ms ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 2.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").sample_stats.log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 ms ± 1.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.4 ms ± 2.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _z_scale_new(az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''''''''''''''''''''''''''''''''z_scale works well on large sets. It's a bit slow on schools though'''''''''''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/(len(data))-((a/(len(data)))**2)\n",
    "\n",
    "@numba.njit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(a)\n",
    "    for i in range(0,a):\n",
    "        var[i] = _var_1d(data[i,:])\n",
    "    return var\n",
    "\n",
    "\n",
    "\n",
    "def _rhat_new(ary):\n",
    "    ary = np.asarray(ary, dtype=float)\n",
    "    if _not_valid(ary, check_shape=False):\n",
    "        return np.nan\n",
    "    _, num_samples = ary.shape\n",
    "\n",
    "    # Calculate chain mean\n",
    "    chain_mean = np.mean(ary, axis=1)\n",
    "    # Calculate chain variance\n",
    "    chain_var = _var_2d(ary)\n",
    "    # Calculate between-chain variance\n",
    "    between_chain_variance = num_samples * _var_1d(chain_mean)\n",
    "    # Calculate within-chain variance\n",
    "    within_chain_variance = np.mean(chain_var)\n",
    "    # Estimate of marginal posterior variance\n",
    "    rhat_value = np.sqrt(\n",
    "        (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
    "    )\n",
    "    return rhat_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.91 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = az.load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 µs ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.4 µs ± 1.45 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rhat_rank_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    split_ary = _split_chains(ary)\n",
    "    rhat_bulk = _rhat_new(_z_scale(split_ary))\n",
    "\n",
    "    split_ary_folded = abs(split_ary - np.median(split_ary))\n",
    "    rhat_tail = _rhat_new(_z_scale(split_ary_folded))\n",
    "\n",
    "    rhat_rank = max(rhat_bulk, rhat_tail)\n",
    "    return rhat_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.37711934, 12.14856451, 11.91083021, 12.97533472])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(school,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.37711934, 12.14856451, 11.91083021, 12.97533472])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0115252895535172"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _rhat_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883 ms ± 5.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _rhat_rank_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 ms ± 6.22 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rk(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 ms ± 39.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit _rhat_rank_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38 ms ± 36.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit rk(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _z_fold(ary):\n",
    "    \"\"\"Fold and z-scale values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    ary = abs(ary - np.median(ary))\n",
    "    ary = _z_scale(ary)\n",
    "    return ary\n",
    "\n",
    "\n",
    "def _rhat_folded_new(ary):\n",
    "    \"\"\"Calculate split-Rhat for folded z-values.\"\"\"\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    ary = _z_fold(_split_chains(ary))\n",
    "    return _rhat_new(ary)\n",
    "\n",
    "\n",
    "def _rhat_z_scale_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_z_scale(_split_chains(ary)))\n",
    "\n",
    "\n",
    "def _rhat_split_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(_split_chains(ary))\n",
    "\n",
    "\n",
    "def _rhat_identity_new(ary):\n",
    "    ary = np.asarray(ary)\n",
    "    if _not_valid(ary, shape_kwargs=dict(min_draws=4, min_chains=2)):\n",
    "        return np.nan\n",
    "    return _rhat_new(ary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_new(data, *, var_names=None, method=\"rank\"):\n",
    "\n",
    "    methods = {\n",
    "        \"rank\": _rhat_rank_new,\n",
    "        \"split\": _rhat_split_new,\n",
    "        \"folded\": _rhat_folded_new,\n",
    "        \"z_scale\": _rhat_z_scale_new,\n",
    "        \"identity\": _rhat_identity_new,\n",
    "    }\n",
    "    if method not in methods:\n",
    "        raise TypeError(\n",
    "            \"R-hat method {} not found. Valid methods are:\\n{}\".format(\n",
    "                method, \"\\n    \".join(methods)\n",
    "            )\n",
    "        )\n",
    "    rhat_func = methods[method]\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.atleast_2d(data)\n",
    "        if len(data.shape) < 3:\n",
    "            return rhat_func(data)\n",
    "        else:\n",
    "            msg = (\n",
    "                \"Only uni-dimensional ndarray variables are supported.\"\n",
    "                \" Please transform first to dataset with `az.convert_to_dataset`.\"\n",
    "            )\n",
    "            raise TypeError(msg)\n",
    "\n",
    "    dataset = convert_to_dataset(data, group=\"posterior\")\n",
    "    var_names = _var_names(var_names, dataset)\n",
    "\n",
    "    dataset = dataset if var_names is None else dataset[var_names]\n",
    "\n",
    "    ufunc_kwargs = {\"ravel\": False}\n",
    "    func_kwargs = {}\n",
    "    return _wrap_xarray_ufunc(\n",
    "        rhat_func, dataset, ufunc_kwargs=ufunc_kwargs, func_kwargs=func_kwargs\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.random.randn(10000,1000)\n",
    "dict_data = {\"posterior\":numpy_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 s ± 1.14 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 s ± 5.09 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 ms ± 619 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms ± 419 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1 s ± 7.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.99 s ± 29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='folded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.88 s ± 7.83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.81 s ± 13.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method='z_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 ms ± 6.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rh(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.1 ms ± 3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rhat_new(numpy_data, method=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to work on fold and rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
