{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "from line_profiler import LineProfiler\n",
    "from arviz.data import load_arviz_data,from_dict,convert_to_inference_data\n",
    "from arviz.stats.stats import r2_score, hpd, _gpdfit, _gpinv,_ic_matrix, waic,psislw,loo,compare\n",
    "from arviz.stats.diagnostics import ess\n",
    "from arviz.stats.stats_utils import logsumexp as _logsumexp\n",
    "import scipy.stats as st\n",
    "import numba\n",
    "import math\n",
    "from numpy import sin, cos\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(10_000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'********************************************hpd********************************************************************'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''********************************************hpd********************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.28349 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271      1001       2171.0      2.2      0.1      if ary.ndim > 1:\n",
      "   272         1          2.0      2.0      0.0          hpd_array = np.array(\n",
      "   273         1         15.0     15.0      0.0              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275         1          2.0      2.0      0.0          return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277      1000     117899.0    117.9      5.2      ary = ary.copy()\n",
      "   278      1000       2188.0      2.2      0.1      n = len(ary)\n",
      "   279                                           \n",
      "   280      1000       1294.0      1.3      0.1      if circular:\n",
      "   281      1000     461793.0    461.8     20.2          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282      1000      15818.0     15.8      0.7          ary = ary - mean\n",
      "   283      1000     704318.0    704.3     30.8          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285      1000     901746.0    901.7     39.5      ary = np.sort(ary)\n",
      "   286      1000       9493.0      9.5      0.4      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287      1000       1593.0      1.6      0.1      n_intervals = n - interval_idx_inc\n",
      "   288      1000       8681.0      8.7      0.4      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290      1000       2101.0      2.1      0.1      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296      1000      12501.0     12.5      0.5      min_idx = np.argmin(interval_width)\n",
      "   297      1000       2420.0      2.4      0.1      hdi_min = ary[min_idx]\n",
      "   298      1000       2909.0      2.9      0.1      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300      1000       1352.0      1.4      0.1      if circular:\n",
      "   301      1000       1657.0      1.7      0.1          hdi_min = hdi_min + mean\n",
      "   302      1000       1301.0      1.3      0.1          hdi_max = hdi_max + mean\n",
      "   303      1000      15375.0     15.4      0.7          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304      1000       9389.0      9.4      0.4          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306      1000       7470.0      7.5      0.3      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_1, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.669335 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271         1          5.0      5.0      0.0      if ary.ndim > 1:\n",
      "   272                                                   hpd_array = np.array(\n",
      "   273                                                       [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275                                                   return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277         1     426740.0 426740.0     63.8      ary = ary.copy()\n",
      "   278         1         11.0     11.0      0.0      n = len(ary)\n",
      "   279                                           \n",
      "   280         1          4.0      4.0      0.0      if circular:\n",
      "   281         1      27237.0  27237.0      4.1          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282         1       3411.0   3411.0      0.5          ary = ary - mean\n",
      "   283         1      75286.0  75286.0     11.2          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285         1     136095.0 136095.0     20.3      ary = np.sort(ary)\n",
      "   286         1         28.0     28.0      0.0      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287         1          3.0      3.0      0.0      n_intervals = n - interval_idx_inc\n",
      "   288         1        211.0    211.0      0.0      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290         1          5.0      5.0      0.0      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296         1        234.0    234.0      0.0      min_idx = np.argmin(interval_width)\n",
      "   297         1          4.0      4.0      0.0      hdi_min = ary[min_idx]\n",
      "   298         1          5.0      5.0      0.0      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300         1          1.0      1.0      0.0      if circular:\n",
      "   301         1          2.0      2.0      0.0          hdi_min = hdi_min + mean\n",
      "   302         1          1.0      1.0      0.0          hdi_max = hdi_max + mean\n",
      "   303         1         30.0     30.0      0.0          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304         1         10.0     10.0      0.0          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306         1         12.0     12.0      0.0      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_2, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.098355 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271       501       1086.0      2.2      1.1      if ary.ndim > 1:\n",
      "   272         1          5.0      5.0      0.0          hpd_array = np.array(\n",
      "   273         1         15.0     15.0      0.0              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275         1          2.0      2.0      0.0          return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277       500       3218.0      6.4      3.3      ary = ary.copy()\n",
      "   278       500       1192.0      2.4      1.2      n = len(ary)\n",
      "   279                                           \n",
      "   280       500        853.0      1.7      0.9      if circular:\n",
      "   281       500      27301.0     54.6     27.8          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282       500       2887.0      5.8      2.9          ary = ary - mean\n",
      "   283       500       4058.0      8.1      4.1          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285       500      20322.0     40.6     20.7      ary = np.sort(ary)\n",
      "   286       500       3806.0      7.6      3.9      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287       500        890.0      1.8      0.9      n_intervals = n - interval_idx_inc\n",
      "   288       500       2696.0      5.4      2.7      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290       500       1168.0      2.3      1.2      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296       500       6365.0     12.7      6.5      min_idx = np.argmin(interval_width)\n",
      "   297       500       1494.0      3.0      1.5      hdi_min = ary[min_idx]\n",
      "   298       500       1489.0      3.0      1.5      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300       500        902.0      1.8      0.9      if circular:\n",
      "   301       500        993.0      2.0      1.0          hdi_min = hdi_min + mean\n",
      "   302       500        851.0      1.7      0.9          hdi_max = hdi_max + mean\n",
      "   303       500       7426.0     14.9      7.6          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304       500       5902.0     11.8      6.0          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306       500       3434.0      6.9      3.5      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(school, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBottlenecks:\\n    1)scipy.stats.circmean\\n    2)numpy arctan2\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bottlenecks:\n",
    "    1)scipy.stats.circmean\n",
    "    2)numpy arctan2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _circ_mean(ary, high, low):\n",
    "    ary = np.asarray(ary)\n",
    "    if ary.size==0:\n",
    "        return np.nan, np.nan\n",
    "    pi = np.pi\n",
    "    angles = (ary-low)*2*pi/(high-low)\n",
    "    S = sinusoidal(angles)\n",
    "    C = cosine(angles)\n",
    "    res = np.arctan2(S,C)\n",
    "    mask = res < 0\n",
    "    if mask.ndim > 0:\n",
    "        res[mask] += 2*pi\n",
    "    elif mask:\n",
    "        res += 2*pi\n",
    "    return res*(high - low)/2.0/pi + low\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def sinusoidal(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.sin(x[i])\n",
    "    return summ\n",
    "@numba.jit(nopython=True)\n",
    "def cosine(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.cos(x[i])\n",
    "    return summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 ms ± 3.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 ms ± 440 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.3 ms ± 312 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.84 ms ± 304 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377 ms ± 2.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.8 ms ± 255 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.3 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 ms ± 75 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_1, np.pi, -np.pi), st.circmean(data_1, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_2, np.pi, -np.pi), st.circmean(data_2, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(school, np.pi, -np.pi), st.circmean(school, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 s ± 5.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 ms ± 3.46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.9 ms ± 435 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2 ms ± 381 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 µs ± 3.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Performance of _circ_mean is a worse on larger datasets.\\n   On schools, the performance is much better.\\n   ¯\\\\_(ツ)_/¯\\n   Let's see the overall perfomance of hpd\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance of _circ_mean is a worse on larger datasets.\n",
    "   On schools, the performance is much better.\n",
    "   ¯\\_(ツ)_/¯\n",
    "   Let's see the overall perfomance of hpd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpd_new(ary, credible_interval=0.94, circular=False):\n",
    "    if ary.ndim > 1:\n",
    "        hpd_array = np.array(\n",
    "            [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
    "        )\n",
    "        return hpd_array\n",
    "    # Make a copy of trace\n",
    "    ary = ary.copy()\n",
    "    n = len(ary)\n",
    "\n",
    "    if circular:\n",
    "        mean = _circ_mean(ary, high=np.pi, low=-np.pi)\n",
    "        ary = ary - mean\n",
    "        ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
    "\n",
    "    ary = np.sort(ary)\n",
    "    interval_idx_inc = int(np.floor(credible_interval * n))\n",
    "    n_intervals = n - interval_idx_inc\n",
    "    interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
    "\n",
    "    if len(interval_width) == 0:\n",
    "        raise ValueError(\n",
    "            \"Too few elements for interval calculation. \"\n",
    "            \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
    "        )\n",
    "\n",
    "    min_idx = np.argmin(interval_width)\n",
    "    hdi_min = ary[min_idx]\n",
    "    hdi_max = ary[min_idx + interval_idx_inc]\n",
    "\n",
    "    if circular:\n",
    "        hdi_min = hdi_min + mean\n",
    "        hdi_max = hdi_max + mean\n",
    "        hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
    "        hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
    "\n",
    "    return np.array([hdi_min, hdi_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(school, 0.95,True), az.stats.hpd(school, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_1, 0.95,True), az.stats.hpd(data_1, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_2, 0.95,True), az.stats.hpd(data_2, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.7 ms ± 479 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.8 ms ± 606 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 s ± 9.55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99 s ± 4.82 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 ms ± 3.69 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As seen above, the performance of numba is better with school dataset.\\nThe performance with large datasets is a bit unstable.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''As seen above, the performance of numba is better with school dataset.\n",
    "The performance with large datasets is a bit unstable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'******************************************R2_SCORE***************************************************************'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''******************************************R2_SCORE***************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(1000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "data_3 = np.random.randn(1000,1000)\n",
    "data_4 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "n_school = load_arviz_data(\"non_centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.023943 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 565\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   565                                           def r2_score(y_true, y_pred):\n",
      "   566                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   567                                           \n",
      "   568                                               Parameters\n",
      "   569                                               ----------\n",
      "   570                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   571                                                   Ground truth (correct) target values.\n",
      "   572                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   573                                                   Estimated target values.\n",
      "   574                                           \n",
      "   575                                               Returns\n",
      "   576                                               -------\n",
      "   577                                               Pandas Series with the following indices:\n",
      "   578                                               r2: Bayesian R²\n",
      "   579                                               r2_std: standard deviation of the Bayesian R².\n",
      "   580                                               \"\"\"\n",
      "   581         1          7.0      7.0      0.0      if y_pred.ndim == 1:\n",
      "   582                                                   var_y_est = np.var(y_pred)\n",
      "   583                                                   var_e = np.var(y_true - y_pred)\n",
      "   584                                               else:\n",
      "   585         1       2433.0   2433.0     10.2          var_y_est = np.var(y_pred.mean(0))\n",
      "   586         1      19402.0  19402.0     81.0          var_e = np.var(y_true - y_pred, 0)\n",
      "   587                                           \n",
      "   588         1         40.0     40.0      0.2      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   589                                           \n",
      "   590         1       2061.0   2061.0      8.6      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_1, data_3)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.019324 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 565\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   565                                           def r2_score(y_true, y_pred):\n",
      "   566                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   567                                           \n",
      "   568                                               Parameters\n",
      "   569                                               ----------\n",
      "   570                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   571                                                   Ground truth (correct) target values.\n",
      "   572                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   573                                                   Estimated target values.\n",
      "   574                                           \n",
      "   575                                               Returns\n",
      "   576                                               -------\n",
      "   577                                               Pandas Series with the following indices:\n",
      "   578                                               r2: Bayesian R²\n",
      "   579                                               r2_std: standard deviation of the Bayesian R².\n",
      "   580                                               \"\"\"\n",
      "   581         1          5.0      5.0      0.0      if y_pred.ndim == 1:\n",
      "   582         1       6793.0   6793.0     35.2          var_y_est = np.var(y_pred)\n",
      "   583         1      11304.0  11304.0     58.5          var_e = np.var(y_true - y_pred)\n",
      "   584                                               else:\n",
      "   585                                                   var_y_est = np.var(y_pred.mean(0))\n",
      "   586                                                   var_e = np.var(y_true - y_pred, 0)\n",
      "   587                                           \n",
      "   588         1          6.0      6.0      0.0      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   589                                           \n",
      "   590         1       1216.0   1216.0      6.3      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_2, data_4)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.jit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(b)\n",
    "    for i in range(0,b):\n",
    "        var[i] = _var_1d(data[:,i])\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992210521845589"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_1d(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992210521845515"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2), np.var(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_4), np.var(data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.68 ms ± 59.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 ms ± 533 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2-data_4),np.var(data_2-data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.1 µs ± 6.67 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.4 µs ± 333 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(school,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14 ms ± 30.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.73 ms ± 95.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ms ± 71.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_3-data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.7 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_3-data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(school), np.var(school,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1), np.var(data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3), np.var(data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3-data_1), np.var(data_3-data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1-data_3), np.var(data_1-data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is doing wonders with variance. Lets inspect mean'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is doing wonders with variance. Lets inspect mean'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def mean_1d(data):\n",
    "    summ = 0\n",
    "    for i in data:\n",
    "        summ = summ+i\n",
    "    return summ/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014596802401655052"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014596802401655147"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.68 ms ± 69.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mean_1d(data_2-data_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.96 ms ± 93.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is not suitable here. Lets see the overall r2_score performance'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is not suitable here. Lets see the overall r2_score performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_new(y_true, y_pred):\n",
    "    if y_pred.ndim == 1:\n",
    "        var_y_est = _var_1d(y_pred)\n",
    "        var_e = _var_1d(y_true - y_pred)\n",
    "    else:\n",
    "        var_y_est = _var_1d(y_pred.mean(0))\n",
    "        var_e = _var_2d(y_true - y_pred)\n",
    "\n",
    "    r_squared = var_y_est / (var_y_est + var_e)\n",
    "\n",
    "    return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ms ± 33.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 43.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.08 ms ± 46.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5 ms ± 25.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593 µs ± 6.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649 µs ± 8.63 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = r2_score_new(school, n_school)\n",
    "df_2 = az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        False\n",
       "r2_std    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1==df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000567\n",
       "r2_std    0.000025\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000567\n",
       "r2_std    0.000025\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have not included the pandas test yet...will include them soon'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''I have not included the pandas test yet...will include them soon'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It can be clearly seen that numba greatly improves the performance of r2_score.\\nTBH, this was the function which I thought had the least scope of improvement\\nThank you mentors.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''It can be clearly seen that numba greatly improves the performance of r2_score.\n",
    "TBH, this was the function which I thought had the least scope of improvement\n",
    "Thank you mentors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpdfit'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpdfit\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.sort(np.random.randn(100000)))\n",
    "school = (np.sort((load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.426928 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 488\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   488                                           def _gpdfit(ary):\n",
      "   489                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   490                                           \n",
      "   491                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   492                                               distribution given the data.\n",
      "   493                                           \n",
      "   494                                               Parameters\n",
      "   495                                               ----------\n",
      "   496                                               ary : array\n",
      "   497                                                   sorted 1D data array\n",
      "   498                                           \n",
      "   499                                               Returns\n",
      "   500                                               -------\n",
      "   501                                               k : float\n",
      "   502                                                   estimated shape parameter\n",
      "   503                                               sigma : float\n",
      "   504                                                   estimated scale parameter\n",
      "   505                                               \"\"\"\n",
      "   506         1          3.0      3.0      0.0      prior_bs = 3\n",
      "   507         1          1.0      1.0      0.0      prior_k = 10\n",
      "   508         1          2.0      2.0      0.0      n = len(ary)\n",
      "   509         1          7.0      7.0      0.0      m_est = 30 + int(n ** 0.5)\n",
      "   510                                           \n",
      "   511         1         69.0     69.0      0.0      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   512         1         24.0     24.0      0.0      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   513         1         10.0     10.0      0.0      b_ary += 1 / ary[-1]\n",
      "   514                                           \n",
      "   515         1     392455.0 392455.0     91.9      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   516         1         72.0     72.0      0.0      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   517         1      33565.0  33565.0      7.9      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   518                                           \n",
      "   519                                               # remove negligible weights\n",
      "   520         1         59.0     59.0      0.0      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   521         1         30.0     30.0      0.0      if not np.all(real_idxs):\n",
      "   522         1          7.0      7.0      0.0          weights = weights[real_idxs]\n",
      "   523         1          4.0      4.0      0.0          b_ary = b_ary[real_idxs]\n",
      "   524                                               # normalise weights\n",
      "   525         1         18.0     18.0      0.0      weights /= weights.sum()\n",
      "   526                                           \n",
      "   527                                               # posterior mean for b\n",
      "   528         1         32.0     32.0      0.0      b_post = np.sum(b_ary * weights)\n",
      "   529                                               # estimate for k\n",
      "   530         1        563.0    563.0      0.1      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   531                                               # add prior for k_post\n",
      "   532         1          4.0      4.0      0.0      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   533         1          2.0      2.0      0.0      sigma = -k_post / b_post\n",
      "   534                                           \n",
      "   535         1          1.0      1.0      0.0      return k_post, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.002692 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 488\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   488                                           def _gpdfit(ary):\n",
      "   489                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   490                                           \n",
      "   491                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   492                                               distribution given the data.\n",
      "   493                                           \n",
      "   494                                               Parameters\n",
      "   495                                               ----------\n",
      "   496                                               ary : array\n",
      "   497                                                   sorted 1D data array\n",
      "   498                                           \n",
      "   499                                               Returns\n",
      "   500                                               -------\n",
      "   501                                               k : float\n",
      "   502                                                   estimated shape parameter\n",
      "   503                                               sigma : float\n",
      "   504                                                   estimated scale parameter\n",
      "   505                                               \"\"\"\n",
      "   506         1          4.0      4.0      0.1      prior_bs = 3\n",
      "   507         1          3.0      3.0      0.1      prior_k = 10\n",
      "   508         1          4.0      4.0      0.1      n = len(ary)\n",
      "   509         1          9.0      9.0      0.3      m_est = 30 + int(n ** 0.5)\n",
      "   510                                           \n",
      "   511         1        121.0    121.0      4.5      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   512         1         38.0     38.0      1.4      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   513         1         19.0     19.0      0.7      b_ary += 1 / ary[-1]\n",
      "   514                                           \n",
      "   515         1        915.0    915.0     34.0      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   516         1         55.0     55.0      2.0      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   517         1        701.0    701.0     26.0      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   518                                           \n",
      "   519                                               # remove negligible weights\n",
      "   520         1        299.0    299.0     11.1      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   521         1         47.0     47.0      1.7      if not np.all(real_idxs):\n",
      "   522         1          9.0      9.0      0.3          weights = weights[real_idxs]\n",
      "   523         1          5.0      5.0      0.2          b_ary = b_ary[real_idxs]\n",
      "   524                                               # normalise weights\n",
      "   525         1         34.0     34.0      1.3      weights /= weights.sum()\n",
      "   526                                           \n",
      "   527                                               # posterior mean for b\n",
      "   528         1         39.0     39.0      1.4      b_post = np.sum(b_ary * weights)\n",
      "   529                                               # estimate for k\n",
      "   530         1        108.0    108.0      4.0      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   531                                               # add prior for k_post\n",
      "   532         1          9.0      9.0      0.3      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   533         1        270.0    270.0     10.0      sigma = -k_post / b_post\n",
      "   534                                           \n",
      "   535         1          3.0      3.0      0.1      return k_post, sigma\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:515: RuntimeWarning: invalid value encountered in log1p\n",
      "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:520: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:533: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  sigma = -k_post / b_post\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(school)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck at 518'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bottleneck at 518\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def log_1p(data):\n",
    "    log_p = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        log_p[i] = math.log1p(data[i])\n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(data), np.log1p(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log1p\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(school), np.log1p(school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 ms ± 3.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 µs ± 11.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 µs ± 78.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log1p\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 µs ± 69.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\\nwhile in 10% of trials the performance comes down a notch w.r.t to numpy'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\n",
    "while in 10% of trials the performance comes down a notch w.r.t to numpy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overall performance of gpdfit'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''overall performance of gpdfit'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpdfit_new(ary):\n",
    "    prior_bs = 3\n",
    "    prior_k = 10\n",
    "    n = len(ary)\n",
    "    m_est = 30 + int(n ** 0.5)\n",
    "\n",
    "    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
    "    b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
    "    b_ary += 1 / ary[-1]\n",
    "    \n",
    "    K = -b_ary[:, None] * ary\n",
    "    k_ary = np.log1p(K).mean(axis=1) # pylint: disable=no-member Using my own log_1p here throws an exception\n",
    "    len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
    "    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
    "\n",
    "    # remove negligible weights\n",
    "    real_idxs = weights >= 10 * np.finfo(float).eps\n",
    "    if not np.all(real_idxs):\n",
    "        weights = weights[real_idxs]\n",
    "        b_ary = b_ary[real_idxs]\n",
    "    # normalise weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b_post = np.sum(b_ary * weights)\n",
    "    # estimate for k\n",
    "    k_post = log_1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
    "    # add prior for k_post\n",
    "    k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
    "    sigma = -k_post / b_post\n",
    "    return k_post, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 ms ± 4.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 ms ± 3.46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in log1p\n",
      "  if sys.path[0] == '':\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater_equal\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 µs ± 6.02 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:515: RuntimeWarning: invalid value encountered in log1p\n",
      "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:520: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:533: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  sigma = -k_post / b_post\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 µs ± 55.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similar performance on both the datasets. Up for discussion with mentors'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Similar performance on both the datasets. Up for discussion with mentors'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpvinv'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpvinv\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(100000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.10425 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 538\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   538                                           def _gpinv(probs, kappa, sigma):\n",
      "   539                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   540                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   541         1        307.0    307.0      0.3      x = np.full_like(probs, np.nan)\n",
      "   542         1          3.0      3.0      0.0      if sigma <= 0:\n",
      "   543                                                   return x\n",
      "   544         1        589.0    589.0      0.6      ok = (probs > 0) & (probs < 1)\n",
      "   545         1        339.0    339.0      0.3      if np.all(ok):\n",
      "   546                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   547                                                       x = -np.log1p(-probs)\n",
      "   548                                                   else:\n",
      "   549                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   550                                                   x *= sigma\n",
      "   551                                               else:\n",
      "   552         1         65.0     65.0      0.1          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   553                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   554                                                   else:\n",
      "   555         1     102601.0 102601.0     98.4              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   556         1        120.0    120.0      0.1          x *= sigma\n",
      "   557         1        127.0    127.0      0.1          x[probs == 0] = 0\n",
      "   558         1          2.0      2.0      0.0          if kappa >= 0:\n",
      "   559         1         96.0     96.0      0.1              x[probs == 1] = np.inf\n",
      "   560                                                   else:\n",
      "   561                                                       x[probs == 1] = -sigma / kappa\n",
      "   562         1          1.0      1.0      0.0      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(data, 5,2)\n",
    "lp.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000176 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 538\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   538                                           def _gpinv(probs, kappa, sigma):\n",
      "   539                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   540                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   541         1         31.0     31.0     17.6      x = np.full_like(probs, np.nan)\n",
      "   542         1          2.0      2.0      1.1      if sigma <= 0:\n",
      "   543                                                   return x\n",
      "   544         1         27.0     27.0     15.3      ok = (probs > 0) & (probs < 1)\n",
      "   545         1         33.0     33.0     18.8      if np.all(ok):\n",
      "   546                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   547                                                       x = -np.log1p(-probs)\n",
      "   548                                                   else:\n",
      "   549                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   550                                                   x *= sigma\n",
      "   551                                               else:\n",
      "   552         1         27.0     27.0     15.3          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   553                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   554                                                   else:\n",
      "   555         1         34.0     34.0     19.3              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   556         1          9.0      9.0      5.1          x *= sigma\n",
      "   557         1          7.0      7.0      4.0          x[probs == 0] = 0\n",
      "   558         1          1.0      1.0      0.6          if kappa >= 0:\n",
      "   559         1          4.0      4.0      2.3              x[probs == 1] = np.inf\n",
      "   560                                                   else:\n",
      "   561                                                       x[probs == 1] = -sigma / kappa\n",
      "   562         1          1.0      1.0      0.6      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(school, 5,2)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Log1p or expm is the bottleneck again'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Log1p or expm is the bottleneck again'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def expm(data):\n",
    "    expm = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        expm[i] = math.expm1(data[i])\n",
    "    return expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(expm(data), np.expm1(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 ms ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 µs ± 4.84 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3 µs ± 27.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7 µs ± 551 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numpy is a bit faster\\nLets see the overall performance'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numpy is a bit faster\n",
    "Lets see the overall performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpinv_new(probs, kappa, sigma):\n",
    "    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
    "    # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
    "    x = np.full_like(probs, np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (probs > 0) & (probs < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x = -log_1p(-probs)\n",
    "        else:\n",
    "            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x[ok] = -log_1p(-probs[ok])\n",
    "        else:\n",
    "            x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
    "        x *= sigma\n",
    "        x[probs == 0] = 0\n",
    "        if kappa >= 0:\n",
    "            x[probs == 1] = np.inf\n",
    "        else:\n",
    "            x[probs == 1] = -sigma / kappa\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _gpinv_new(data,-5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      nan,       nan,       nan, ...,       nan,       nan,\n",
       "       0.3783644])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      nan,       nan,       nan, ...,       nan,       nan,\n",
       "       0.3783644])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan,  0.])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 ms ± 83.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpinv_new(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.74 ms ± 33.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.2 µs ± 420 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpinv_new(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.7 µs ± 402 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numba _gpinv is similar in performance'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numba _gpinv is similar in performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataset_dict,ic='waic'):\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([waic(dataset, pointwise=True, scale=\"deviance\")])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=True)\n",
    "    return ics, ic_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics, ic_i = generator({\"1\":load_arviz_data(\"centered_eight\"),\"2\":load_arviz_data(\"non_centered_eight\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>waic_se</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_i</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.302151</td>\n",
       "      <td>2.727291</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.732429123207623, 6.813708378182436, 7.70616...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.429587</td>\n",
       "      <td>2.689941</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.76127241619229, 6.832600316056821, 7.725554...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        waic   waic_se    p_waic  warning  \\\n",
       "2  61.302151  2.727291  0.820067        0   \n",
       "1  61.429587  2.689941  0.919548        0   \n",
       "\n",
       "                                              waic_i waic_scale  \n",
       "2  [9.732429123207623, 6.813708378182436, 7.70616...   deviance  \n",
       "1  [9.76127241619229, 6.832600316056821, 7.725554...   deviance  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.002752 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _ic_matrix at line 232\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   232                                           def _ic_matrix(ics, ic_i):\n",
      "   233                                               \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
      "   234         1         26.0     26.0      0.9      cols, _ = ics.shape\n",
      "   235         1        466.0    466.0     16.9      rows = len(ics[ic_i].iloc[0])\n",
      "   236         1          9.0      9.0      0.3      ic_i_val = np.zeros((rows, cols))\n",
      "   237                                           \n",
      "   238         3        115.0     38.3      4.2      for idx, val in enumerate(ics.index):\n",
      "   239         2       2087.0   1043.5     75.8          ic = ics.loc[val][ic_i]\n",
      "   240                                           \n",
      "   241         2          7.0      3.5      0.3          if len(ic) != rows:\n",
      "   242                                                       raise ValueError(\"The number of observations should be the same across all models\")\n",
      "   243                                           \n",
      "   244         2         41.0     20.5      1.5          ic_i_val[:, idx] = ic\n",
      "   245                                           \n",
      "   246         1          1.0      1.0      0.0      return rows, cols, ic_i_val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_ic_matrix)\n",
    "wrapper(ics,ic_i)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def loop_lifter(ics,rows,cols):\n",
    "    ic_i_val = np.zeros((rows, cols))\n",
    "    for idx, val in enumerate(ics.index):\n",
    "        ic = ics.loc[val][ic_i]\n",
    "\n",
    "        if len(ic) != rows:\n",
    "            raise ValueError(\"The number of observations should be the same across all models\")\n",
    "\n",
    "        ic_i_val[:, idx] = ic\n",
    "    return ic_i_val\n",
    "\n",
    "def _ic_matrix_new(ics, ic_i):\n",
    "    \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
    "    cols, _ = ics.shape\n",
    "    rows = len(ics[ic_i].iloc[0])\n",
    "    return rows, cols, loop_lifter(ics,rows,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ic_matrix_new(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635 µs ± 5.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._ic_matrix(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\\n   Original _ic_matrix is better'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\n",
    "   Original _ic_matrix is better'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000,20)\n",
    "sample_stats = {\"log_likelihood\":data}\n",
    "data = from_dict(sample_stats=sample_stats)\n",
    "school = load_arviz_data(\"centered_eight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.662547 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: waic at line 865\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   865                                           def waic(data, pointwise=False, scale=\"deviance\"):\n",
      "   866                                               \"\"\"Calculate the widely available information criterion.\n",
      "   867                                           \n",
      "   868                                               Also calculates the WAIC's standard error and the effective number of\n",
      "   869                                               parameters of the samples in trace from model. Read more theory here - in\n",
      "   870                                               a paper by some of the leading authorities on model selection\n",
      "   871                                               dx.doi.org/10.1111/1467-9868.00353\n",
      "   872                                           \n",
      "   873                                               Parameters\n",
      "   874                                               ----------\n",
      "   875                                               data : obj\n",
      "   876                                                   Any object that can be converted to an az.InferenceData object\n",
      "   877                                                   Refer to documentation of az.convert_to_dataset for details\n",
      "   878                                               pointwise: bool\n",
      "   879                                                   if True the pointwise predictive accuracy will be returned.\n",
      "   880                                                   Default False\n",
      "   881                                               scale : str\n",
      "   882                                                   Output scale for loo. Available options are:\n",
      "   883                                           \n",
      "   884                                                   - `deviance` : (default) -2 * (log-score)\n",
      "   885                                                   - `log` : 1 * log-score\n",
      "   886                                                   - `negative_log` : -1 * (log-score)\n",
      "   887                                           \n",
      "   888                                               Returns\n",
      "   889                                               -------\n",
      "   890                                               DataFrame with the following columns:\n",
      "   891                                               waic: widely available information criterion\n",
      "   892                                               waic_se: standard error of waic\n",
      "   893                                               p_waic: effective number parameters\n",
      "   894                                               var_warn: 1 if posterior variance of the log predictive\n",
      "   895                                                    densities exceeds 0.4\n",
      "   896                                               waic_i: and array of the pointwise predictive accuracy, only if pointwise True\n",
      "   897                                               waic_scale: scale of the waic results\n",
      "   898                                               \"\"\"\n",
      "   899         1         41.0     41.0      0.0      inference_data = convert_to_inference_data(data)\n",
      "   900         2          8.0      4.0      0.0      for group in (\"sample_stats\",):\n",
      "   901         1          6.0      6.0      0.0          if not hasattr(inference_data, group):\n",
      "   902                                                       raise TypeError(\n",
      "   903                                                           \"Must be able to extract a {group} group from data!\".format(group=group)\n",
      "   904                                                       )\n",
      "   905         1         18.0     18.0      0.0      if \"log_likelihood\" not in inference_data.sample_stats:\n",
      "   906                                                   raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
      "   907         1        691.0    691.0      0.1      log_likelihood = inference_data.sample_stats.log_likelihood\n",
      "   908                                           \n",
      "   909         1          5.0      5.0      0.0      if scale.lower() == \"deviance\":\n",
      "   910         1          2.0      2.0      0.0          scale_value = -2\n",
      "   911                                               elif scale.lower() == \"log\":\n",
      "   912                                                   scale_value = 1\n",
      "   913                                               elif scale.lower() == \"negative_log\":\n",
      "   914                                                   scale_value = -1\n",
      "   915                                               else:\n",
      "   916                                                   raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
      "   917                                           \n",
      "   918         1       1399.0   1399.0      0.2      n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
      "   919         1         67.0     67.0      0.0      new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
      "   920         1         49.0     49.0      0.0      log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
      "   921                                           \n",
      "   922         1     385793.0 385793.0     58.2      lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
      "   923                                           \n",
      "   924         1     272702.0 272702.0     41.2      vars_lpd = np.var(log_likelihood, axis=0)\n",
      "   925         1          4.0      4.0      0.0      warn_mg = 0\n",
      "   926         1         76.0     76.0      0.0      if np.any(vars_lpd > 0.4):\n",
      "   927         1          4.0      4.0      0.0          warnings.warn(\n",
      "   928                                                       \"\"\"For one or more samples the posterior variance of the log predictive\n",
      "   929                                                   densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "   930                                                   http://arxiv.org/abs/1507.04544 for details\n",
      "   931         1        208.0    208.0      0.0          \"\"\"\n",
      "   932                                                   )\n",
      "   933         1          2.0      2.0      0.0          warn_mg = 1\n",
      "   934                                           \n",
      "   935         1         20.0     20.0      0.0      waic_i = scale_value * (lppd_i - vars_lpd)\n",
      "   936         1        106.0    106.0      0.0      waic_se = (len(waic_i) * np.var(waic_i)) ** 0.5\n",
      "   937         1         17.0     17.0      0.0      waic_sum = np.sum(waic_i)\n",
      "   938         1         10.0     10.0      0.0      p_waic = np.sum(vars_lpd)\n",
      "   939                                           \n",
      "   940         1          2.0      2.0      0.0      if pointwise:\n",
      "   941         1         18.0     18.0      0.0          if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
      "   942                                                       warnings.warn(\n",
      "   943                                                           \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
      "   944                                                       the Observed RV in your model to make sure it returns element-wise logp.\n",
      "   945                                                       \"\"\"\n",
      "   946                                                       )\n",
      "   947         1          3.0      3.0      0.0          return pd.Series(\n",
      "   948         1          2.0      2.0      0.0              data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
      "   949         1       1294.0   1294.0      0.2              index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
      "   950                                                   )\n",
      "   951                                               else:\n",
      "   952                                                   return pd.Series(\n",
      "   953                                                       data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
      "   954                                                       index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
      "   955                                                   )\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(waic)\n",
    "wrapper(data,True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_new(data, pointwise=False, scale=\"deviance\"):\n",
    "    inference_data = convert_to_inference_data(data)\n",
    "    for group in (\"sample_stats\",):\n",
    "        if not hasattr(inference_data, group):\n",
    "            raise TypeError(\n",
    "                \"Must be able to extract a {group} group from data!\".format(group=group)\n",
    "            )\n",
    "    if \"log_likelihood\" not in inference_data.sample_stats:\n",
    "        raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
    "    log_likelihood = inference_data.sample_stats.log_likelihood\n",
    "\n",
    "    if scale.lower() == \"deviance\":\n",
    "        scale_value = -2\n",
    "    elif scale.lower() == \"log\":\n",
    "        scale_value = 1\n",
    "    elif scale.lower() == \"negative_log\":\n",
    "        scale_value = -1\n",
    "    else:\n",
    "        raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
    "\n",
    "    n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
    "    new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
    "    log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
    "\n",
    "    lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
    "\n",
    "    vars_lpd = _var_2d(log_likelihood)\n",
    "    warn_mg = 0\n",
    "    if np.any(vars_lpd > 0.4):\n",
    "        warnings.warn(\n",
    "            \"\"\"For one or more samples the posterior variance of the log predictive\n",
    "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
    "        http://arxiv.org/abs/1507.04544 for details\n",
    "        \"\"\"\n",
    "        )\n",
    "        warn_mg = 1\n",
    "\n",
    "    waic_i = scale_value * (lppd_i - vars_lpd)\n",
    "    waic_se = (len(waic_i) * _var_1d(waic_i)) ** 0.5\n",
    "    waic_sum = np.sum(waic_i)\n",
    "    p_waic = np.sum(vars_lpd)\n",
    "\n",
    "    if pointwise:\n",
    "        if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
    "            warnings.warn(\n",
    "                \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
    "            the Observed RV in your model to make sure it returns element-wise logp.\n",
    "            \"\"\"\n",
    "            )\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
    "        )\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    19.9831\n",
       "waic_se                                                0.011613\n",
       "p_waic                                                  19.9913\n",
       "warning                                                       1\n",
       "waic_i        [1.0010586401655646, 1.000963285502681, 1.0021...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    19.9831\n",
       "waic_se                                                0.011613\n",
       "p_waic                                                  19.9913\n",
       "warning                                                       1\n",
       "waic_i        [1.0010586401655632, 1.0009632855026265, 1.002...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593 ms ± 7.58 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653 ms ± 11.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77 ms ± 52.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02 ms ± 36.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psislw'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"psislw\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def max_elem(data):\n",
    "    max = data[0]\n",
    "    for i in range(0,len(data)):\n",
    "        if data[i]>max:\n",
    "            max = data[i]\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(1000,1000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.706474 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: psislw at line 417\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   417                                           def psislw(log_weights, reff=1.0):\n",
      "   418                                               \"\"\"\n",
      "   419                                               Pareto smoothed importance sampling (PSIS).\n",
      "   420                                           \n",
      "   421                                               Parameters\n",
      "   422                                               ----------\n",
      "   423                                               log_weights : array\n",
      "   424                                                   Array of size (n_samples, n_observations)\n",
      "   425                                               reff : float\n",
      "   426                                                   relative MCMC efficiency, `ess / n`\n",
      "   427                                           \n",
      "   428                                               Returns\n",
      "   429                                               -------\n",
      "   430                                               lw_out : array\n",
      "   431                                                   Smoothed log weights\n",
      "   432                                               kss : array\n",
      "   433                                                   Pareto tail indices\n",
      "   434                                               \"\"\"\n",
      "   435         1          5.0      5.0      0.0      rows, cols = log_weights.shape\n",
      "   436                                           \n",
      "   437         1      11399.0  11399.0      1.6      log_weights_out = np.copy(log_weights, order=\"F\")\n",
      "   438         1         27.0     27.0      0.0      kss = np.empty(cols)\n",
      "   439                                           \n",
      "   440                                               # precalculate constants\n",
      "   441         1         45.0     45.0      0.0      cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
      "   442         1         51.0     51.0      0.0      cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
      "   443         1          3.0      3.0      0.0      k_min = 1.0 / 3\n",
      "   444                                           \n",
      "   445                                               # loop over sets of log weights\n",
      "   446      1001       2835.0      2.8      0.4      for i, x in enumerate(log_weights_out.T):\n",
      "   447                                                   # improve numerical accuracy\n",
      "   448      1000      22003.0     22.0      3.1          x -= np.max(x)\n",
      "   449                                                   # sort the array\n",
      "   450      1000      93122.0     93.1     13.2          x_sort_ind = np.argsort(x)\n",
      "   451                                                   # divide log weights into body and right tail\n",
      "   452      1000       5087.0      5.1      0.7          xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
      "   453                                           \n",
      "   454      1000       6530.0      6.5      0.9          expxcutoff = np.exp(xcutoff)\n",
      "   455      1000      12751.0     12.8      1.8          tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
      "   456      1000       2856.0      2.9      0.4          x_tail = x[tailinds]\n",
      "   457      1000       1955.0      2.0      0.3          tail_len = len(x_tail)\n",
      "   458      1000       1654.0      1.7      0.2          if tail_len <= 4:\n",
      "   459                                                       # not enough tail samples for gpdfit\n",
      "   460                                                       k = np.inf\n",
      "   461                                                   else:\n",
      "   462                                                       # order of tail samples\n",
      "   463      1000      16426.0     16.4      2.3              x_tail_si = np.argsort(x_tail)\n",
      "   464                                                       # fit generalized Pareto distribution to the right tail samples\n",
      "   465      1000      11751.0     11.8      1.7              x_tail = np.exp(x_tail) - expxcutoff\n",
      "   466      1000     375888.0    375.9     53.2              k, sigma = _gpdfit(x_tail[x_tail_si])\n",
      "   467                                           \n",
      "   468      1000       3087.0      3.1      0.4              if k >= k_min:\n",
      "   469                                                           # no smoothing if short tail or GPD fit failed\n",
      "   470                                                           # compute ordered statistic for the fit\n",
      "   471                                                           sti = np.arange(0.5, tail_len) / tail_len\n",
      "   472                                                           smoothed_tail = _gpinv(sti, k, sigma)\n",
      "   473                                                           smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
      "   474                                                               smoothed_tail + expxcutoff\n",
      "   475                                                           )\n",
      "   476                                                           # place the smoothed tail into the output array\n",
      "   477                                                           x[tailinds[x_tail_si]] = smoothed_tail\n",
      "   478                                                           # truncate smoothed values to the largest raw weight 0\n",
      "   479                                                           x[x > 0] = 0\n",
      "   480                                                   # renormalize weights\n",
      "   481      1000     136046.0    136.0     19.3          x -= _logsumexp(x)\n",
      "   482                                                   # store tail index k\n",
      "   483      1000       2952.0      3.0      0.4          kss[i] = k\n",
      "   484                                           \n",
      "   485         1          1.0      1.0      0.0      return log_weights_out, kss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(psislw)\n",
    "wrapper(data, 0.66)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lets use our own gpd funcs'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lets use our own gpd funcs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def psislw_new(log_weights, reff=1.0):\n",
    "    rows, cols = log_weights.shape\n",
    "\n",
    "    log_weights_out = np.copy(log_weights, order=\"F\")\n",
    "    kss = np.empty(cols)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
    "    k_min = 1.0 / 3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(log_weights_out.T):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
    "\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        x_tail = x[tailinds]\n",
    "        tail_len = len(x_tail)\n",
    "        if tail_len <= 4:\n",
    "            # not enough tail samples for gpdfit\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x_tail_si = np.argsort(x_tail)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            x_tail = np.exp(x_tail) - expxcutoff\n",
    "            k, sigma = _gpdfit_new(x_tail[x_tail_si])\n",
    "\n",
    "            if k >= k_min:\n",
    "                # no smoothing if short tail or GPD fit failed\n",
    "                # compute ordered statistic for the fit\n",
    "                sti = np.arange(0.5, tail_len) / tail_len\n",
    "                smoothed_tail = _gpinv_new(sti, k, sigma)\n",
    "                print(smoothed_tail.shape)\n",
    "                smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
    "                    smoothed_tail + expxcutoff\n",
    "                )\n",
    "                # place the smoothed tail into the output array\n",
    "                x[tailinds[x_tail_si]] = smoothed_tail\n",
    "                # truncate smoothed values to the largest raw weight 0\n",
    "                x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= _logsumexp(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "\n",
    "    return log_weights_out, kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-7.02437696, -6.79062618, -7.44527206, ..., -6.67175066,\n",
       "         -6.92373871, -6.7719152 ],\n",
       "        [-7.09172732, -6.94300782, -7.09153895, ..., -6.84311101,\n",
       "         -6.95800521, -6.68147164],\n",
       "        [-7.21193379, -7.26640903, -7.32056151, ..., -6.93042913,\n",
       "         -7.08143317, -6.78886468],\n",
       "        ...,\n",
       "        [-6.90129009, -6.64136345, -6.47781662, ..., -7.11086381,\n",
       "         -6.57429527, -7.2161785 ],\n",
       "        [-6.93267883, -7.37509929, -6.56168518, ..., -7.14337179,\n",
       "         -7.27876116, -7.28022593],\n",
       "        [-7.16067676, -7.31972747, -6.63735273, ..., -6.81122166,\n",
       "         -6.92576306, -7.10511   ]]),\n",
       " array([-0.70163877, -0.69663576, -0.95224513, -0.80766217, -0.73853571,\n",
       "        -0.86950093, -0.90456559, -0.76683665, -0.55684989, -0.90038144,\n",
       "        -0.75516426, -0.88055714, -0.81754279, -0.89271583, -0.79556245,\n",
       "        -0.78919625, -0.61463043, -0.80915335, -0.8131438 , -0.66826972,\n",
       "        -0.60271067, -0.79240902, -0.83913283, -0.76478472, -0.6240973 ,\n",
       "        -0.90878106, -0.72121955, -0.80539301, -0.8472668 , -0.74259674,\n",
       "        -0.78193057, -0.78536459, -0.76592473, -0.83644087, -0.72103018,\n",
       "        -0.90794454, -0.89559382, -0.69358892, -0.8029382 , -0.81626491,\n",
       "        -0.73504107, -0.81540371, -0.77121999, -0.90342577, -0.92533181,\n",
       "        -0.7847252 , -0.75525959, -0.7021749 , -0.94095698, -0.75123901,\n",
       "        -0.91506558, -0.80643264, -0.92670872, -0.79514525, -0.71340746,\n",
       "        -0.76568245, -0.86914074, -0.78254193, -0.81915563, -0.80486696,\n",
       "        -0.83853859, -0.77993586, -0.95712444, -0.76938844, -0.85788823,\n",
       "        -0.85334352, -0.92399145, -0.83702143, -0.80896241, -0.78346604,\n",
       "        -0.93098707, -0.7104762 , -1.03070086, -0.70039815, -0.69255948,\n",
       "        -0.74228415, -0.81909524, -0.74742229, -0.8589612 , -0.90419809,\n",
       "        -0.83218915, -0.5786506 , -0.72048696, -0.72527362, -0.83001037,\n",
       "        -0.75236478, -0.82870731, -0.67120147, -0.81368107, -0.73070858,\n",
       "        -0.75248562, -0.63870137, -0.73992061, -0.76257208, -0.68834001,\n",
       "        -0.94137188, -0.7597683 , -0.89184192, -0.76442177, -0.74309922,\n",
       "        -0.96822156, -0.92152046, -0.78043186, -0.86489238, -0.81281902,\n",
       "        -0.84252075, -0.80644654, -0.92723783, -0.74999717, -0.89017889,\n",
       "        -0.87780972, -0.73117976, -0.7821352 , -1.02675375, -0.67133013,\n",
       "        -0.63685351, -0.87676385, -0.83088663, -0.72342872, -0.85693377,\n",
       "        -0.73092864, -0.90865939, -0.77387143, -0.97144044, -1.02286784,\n",
       "        -0.930766  , -0.72786013, -0.73051684, -0.71553922, -0.82927937,\n",
       "        -0.86699284, -0.78861929, -0.72540642, -0.75041403, -0.76243522,\n",
       "        -0.70792245, -0.79385187, -0.7812046 , -0.67937917, -0.80870022,\n",
       "        -0.74426425, -0.65217238, -0.99714611, -0.79451062, -0.65055494,\n",
       "        -0.90516999, -1.03755862, -0.78337325, -0.96656189, -0.91455929,\n",
       "        -0.87392623, -0.78117782, -0.73312473, -0.8589605 , -0.82421545,\n",
       "        -0.71756371, -1.06246881, -0.72144981, -0.82415311, -0.75892551,\n",
       "        -0.85501303, -0.76763811, -0.71486048, -0.83789658, -0.94682375,\n",
       "        -0.84971922, -0.79256677, -0.99027183, -0.65768169, -0.72601702,\n",
       "        -0.68541726, -0.77519649, -0.87679991, -0.71290389, -0.72188338,\n",
       "        -0.62619441, -0.81843383, -0.82540222, -0.86941594, -0.8016421 ,\n",
       "        -0.8136894 , -0.85856798, -0.95044872, -0.87960214, -0.89441976,\n",
       "        -0.86386039, -0.87911874, -0.77736849, -0.65145775, -0.80201046,\n",
       "        -0.86115329, -0.83544274, -0.77777201, -0.78839536, -0.72959517,\n",
       "        -0.83938724, -0.76852181, -0.74193393, -0.80062332, -0.86556908,\n",
       "        -0.70243415, -0.76591189, -0.69760058, -0.86158749, -0.80602657,\n",
       "        -0.72865619, -0.8412883 , -0.86996117, -0.74933781, -0.80405757,\n",
       "        -0.73309946, -0.98853099, -0.91466603, -0.70963197, -0.81321444,\n",
       "        -0.81303168, -0.83109173, -0.63736469, -0.81198643, -0.8581604 ,\n",
       "        -0.85125589, -0.85993009, -0.75541469, -0.8208226 , -0.78400883,\n",
       "        -0.69353516, -0.59445727, -0.86685416, -0.83904877, -0.79333381,\n",
       "        -0.83508792, -0.71466011, -0.93616393, -0.85309119, -0.78881037,\n",
       "        -0.8257997 , -0.95793879, -0.95641077, -0.61873322, -0.71785672,\n",
       "        -0.80350161, -0.68265382, -0.76651128, -0.7971533 , -0.88555809,\n",
       "        -0.88217254, -0.84304035, -0.84460327, -0.81683772, -0.80724662,\n",
       "        -0.80842527, -0.85074954, -0.85840681, -0.82959592, -0.63038168,\n",
       "        -1.0171065 , -0.79665281, -0.98526306, -0.7505073 , -0.89308322,\n",
       "        -0.85391785, -0.66244875, -0.87212658, -0.744005  , -0.69848412,\n",
       "        -0.7162014 , -0.88317926, -0.86949161, -0.75109169, -0.70565721,\n",
       "        -0.77463928, -0.90061999, -0.87464047, -0.74067546, -0.81563026,\n",
       "        -0.88588611, -0.72114975, -0.93640636, -0.89432342, -0.84594413,\n",
       "        -0.65921821, -0.64684874, -0.85337052, -0.7841888 , -0.77853513,\n",
       "        -0.9445787 , -0.81762514, -0.95997694, -0.81804779, -0.88590521,\n",
       "        -1.01591766, -0.79266932, -0.87978667, -0.70025739, -0.80576573,\n",
       "        -0.86807981, -0.70857021, -0.77771089, -0.84351512, -0.75272758,\n",
       "        -0.83432693, -1.04404639, -0.83978893, -0.74280293, -0.78960646,\n",
       "        -0.74602946, -0.91420248, -0.84170315, -0.7236898 , -0.71088709,\n",
       "        -0.8372792 , -0.77114934, -0.82036272, -0.72219066, -0.85079663,\n",
       "        -0.73402016, -0.99037487, -0.74605533, -0.83835107, -0.72141792,\n",
       "        -0.84754716, -0.87926999, -0.73556296, -0.88315823, -0.78691748,\n",
       "        -0.84194441, -0.85136193, -0.73730694, -0.96045741, -0.64370592,\n",
       "        -0.64068116, -0.83256228, -0.64768093, -0.99605879, -0.65168785,\n",
       "        -0.88100742, -0.7948646 , -0.76278551, -0.79442377, -0.7598949 ,\n",
       "        -0.87031888, -0.73097844, -0.98976945, -1.03402437, -1.10410944,\n",
       "        -0.96486624, -0.7790645 , -0.86291075, -0.82316576, -0.7774336 ,\n",
       "        -0.83886603, -0.69515811, -0.99895915, -0.75777931, -0.83745681,\n",
       "        -0.8862785 , -0.68868912, -0.73475434, -0.9437453 , -0.70540807,\n",
       "        -0.74680779, -0.98575103, -0.73538703, -0.98168122, -0.70489726,\n",
       "        -0.84188144, -0.87451025, -0.68437184, -0.61402513, -0.84039374,\n",
       "        -0.91074636, -0.72323452, -0.81497589, -0.93980613, -0.76886663,\n",
       "        -0.71597539, -0.90611732, -0.90763982, -0.69229408, -0.67476852,\n",
       "        -0.90542432, -0.70506167, -0.88763007, -0.68239228, -0.79533264,\n",
       "        -0.52625203, -0.77279934, -0.70049516, -0.89416693, -0.86095151,\n",
       "        -0.74410468, -0.93992971, -0.83248822, -0.74763649, -0.65550735,\n",
       "        -0.80400372, -0.76637965, -0.74566926, -0.58631069, -0.68423709,\n",
       "        -0.72310247, -0.64467856, -0.72737817, -0.86579378, -0.86619436,\n",
       "        -0.79153032, -0.84092954, -0.7465371 , -0.8377215 , -0.74510705,\n",
       "        -0.89472476, -0.82613647, -0.8683455 , -0.85494632, -0.68467344,\n",
       "        -0.51648856, -0.71792797, -0.89701599, -0.6723463 , -0.67748274,\n",
       "        -0.843787  , -0.95445334, -0.70170946, -0.82605414, -0.76274595,\n",
       "        -0.82407574, -0.75557838, -1.10417691, -0.71771756, -0.70828072,\n",
       "        -0.84928835, -0.85058706, -0.79716074, -0.78648713, -0.98096489,\n",
       "        -0.94417982, -0.86008471, -0.70876444, -0.82157444, -0.974707  ,\n",
       "        -0.90699114, -0.79150624, -0.96042635, -0.78338393, -0.86141425,\n",
       "        -0.73294239, -1.00431259, -0.85655125, -0.68355959, -0.83556248,\n",
       "        -0.90274172, -0.81874712, -0.73880847, -0.74922495, -0.81280813,\n",
       "        -0.99078578, -0.67471222, -0.88319191, -0.79510106, -0.77408574,\n",
       "        -0.64601415, -0.97837368, -0.72534434, -0.8592518 , -0.71395373,\n",
       "        -0.93177101, -0.84526348, -0.7956412 , -0.89156015, -0.83861381,\n",
       "        -0.78327298, -0.68537205, -0.84453648, -0.80453333, -0.72590627,\n",
       "        -0.95458423, -0.71252162, -0.97952539, -0.81779837, -0.8191481 ,\n",
       "        -0.81566713, -0.81784299, -0.75578055, -0.74581249, -0.81528513,\n",
       "        -0.83338471, -0.66654986, -0.80292406, -0.72997151, -0.99192735,\n",
       "        -0.62983275, -0.96626946, -0.68742185, -0.71118488, -0.77763408,\n",
       "        -0.7221696 , -0.72562863, -0.70950659, -0.62672476, -0.91778552,\n",
       "        -0.62247094, -0.86137717, -0.69597477, -0.97038643, -0.80559403,\n",
       "        -0.80556902, -0.73647845, -0.85072772, -0.70133303, -0.75499339,\n",
       "        -0.84898123, -0.77741134, -0.60877328, -0.71944143, -0.63907452,\n",
       "        -0.7695025 , -0.86560755, -0.93531026, -0.89567318, -0.8852184 ,\n",
       "        -0.68840375, -0.8736269 , -0.89577985, -0.59108368, -0.72830008,\n",
       "        -0.64852897, -0.69982191, -0.8092221 , -0.74473888, -0.96766429,\n",
       "        -0.78207989, -0.72241436, -0.64193522, -0.84462686, -0.70737024,\n",
       "        -0.79747717, -0.84674351, -0.84700279, -0.87055985, -0.92426935,\n",
       "        -0.76935422, -0.74608367, -0.68354232, -1.01884999, -0.83288202,\n",
       "        -0.6808038 , -0.67566475, -0.9347261 , -0.87676019, -0.79250865,\n",
       "        -0.94053664, -0.86705361, -0.75237197, -0.77278197, -0.7737077 ,\n",
       "        -0.90816797, -0.83464623, -0.86343381, -0.89149031, -0.75242744,\n",
       "        -0.81829673, -0.77205457, -0.71567805, -0.81120383, -0.76964672,\n",
       "        -0.71354818, -0.81452344, -0.8687127 , -0.81225702, -0.72757423,\n",
       "        -0.74718498, -0.68634497, -0.83566101, -0.81407043, -0.83313328,\n",
       "        -0.70396246, -0.95863963, -0.71054428, -0.68171033, -0.70653356,\n",
       "        -0.74543589, -0.98816802, -0.86210241, -0.78504775, -0.95577132,\n",
       "        -0.88971557, -0.7700684 , -0.83078797, -0.79425133, -0.94954408,\n",
       "        -0.65683382, -0.7291314 , -0.83334476, -0.91508449, -0.82009294,\n",
       "        -0.7186022 , -0.80508279, -0.7675805 , -0.83218204, -0.8097874 ,\n",
       "        -0.73421745, -0.7799567 , -0.73671914, -0.82080482, -0.81007598,\n",
       "        -0.87594827, -0.87613801, -0.68703237, -0.76055205, -0.69979911,\n",
       "        -0.73389553, -0.76410117, -0.64032295, -0.74999888, -0.98417885,\n",
       "        -0.837928  , -0.92784241, -0.80994539, -0.69996576, -0.88468675,\n",
       "        -0.7076998 , -0.81642511, -0.67370854, -0.69410486, -0.69024929,\n",
       "        -0.81099921, -0.90694049, -0.83796406, -0.66847838, -0.77824304,\n",
       "        -0.83809764, -0.86398521, -0.91581306, -0.95312993, -0.82122925,\n",
       "        -0.73788552, -0.88286193, -0.86467396, -0.90224079, -0.84189807,\n",
       "        -0.92013693, -0.87229778, -0.83070121, -0.76591269, -0.65131555,\n",
       "        -0.69630667, -0.80973025, -0.8366624 , -0.78363456, -0.9570975 ,\n",
       "        -0.94424972, -0.92229105, -0.76053766, -0.8276832 , -0.62863539,\n",
       "        -0.98894879, -0.96861654, -0.8513234 , -0.93646709, -0.67806277,\n",
       "        -0.73112875, -0.75365447, -0.73708281, -0.86846989, -0.8928417 ,\n",
       "        -0.89969661, -0.86817254, -0.92881849, -0.82497142, -0.73376458,\n",
       "        -0.71373385, -0.91697062, -0.8009567 , -0.73797627, -0.86140329,\n",
       "        -0.85296099, -0.86631122, -0.84916517, -0.76420077, -0.68863575,\n",
       "        -0.76003741, -0.80630612, -0.91103076, -0.84861194, -0.59465944,\n",
       "        -0.82618688, -0.91723842, -0.66877285, -0.96127583, -0.79152246,\n",
       "        -0.80055047, -0.7768151 , -0.83428037, -0.72654826, -0.71104655,\n",
       "        -0.81204967, -0.8446113 , -0.76919523, -0.94706223, -0.94730472,\n",
       "        -0.74317207, -0.92485339, -0.92439744, -0.90477995, -0.66942056,\n",
       "        -0.77151391, -0.90129464, -0.89915737, -0.68732156, -0.70390181,\n",
       "        -0.73757106, -0.86458099, -0.67945604, -1.03852034, -0.786853  ,\n",
       "        -0.78683788, -0.83065296, -0.88105392, -0.902051  , -0.84410675,\n",
       "        -0.80590461, -0.85346957, -0.78320609, -0.6997456 , -0.76428284,\n",
       "        -0.75496762, -0.90417411, -0.772262  , -0.78122683, -0.86330996,\n",
       "        -0.77296115, -1.00422804, -0.84679129, -0.84074845, -0.80704027,\n",
       "        -0.76830095, -0.73813222, -0.74586909, -0.68459529, -0.70641809,\n",
       "        -0.78808817, -0.76913247, -0.68276906, -0.83586188, -0.77450253,\n",
       "        -0.89167367, -0.82486617, -0.95678304, -0.87759211, -0.69275964,\n",
       "        -0.79943062, -0.80528467, -0.90214938, -0.92290234, -0.85776849,\n",
       "        -0.72348585, -0.71939868, -0.81412042, -0.78402276, -0.60022308,\n",
       "        -0.75340485, -0.78292177, -0.80463004, -0.65239932, -0.98858781,\n",
       "        -0.72819169, -0.79550175, -0.63932795, -0.85209676, -0.75359146,\n",
       "        -0.83601114, -0.53098107, -0.79809613, -0.56941231, -0.9246254 ,\n",
       "        -0.68150811, -0.84441772, -0.69372705, -0.72669975, -0.79713884,\n",
       "        -0.85570447, -0.80087556, -0.84266922, -0.74168552, -0.74051064,\n",
       "        -0.84836757, -0.78497796, -0.83006827, -0.87834534, -0.91196181,\n",
       "        -0.73421915, -0.85054738, -0.66594517, -0.82147739, -0.66983889,\n",
       "        -0.84893306, -0.83727614, -0.65189312, -1.06805415, -0.74151432,\n",
       "        -0.71807271, -0.63740678, -0.81051939, -0.88043261, -0.75652274,\n",
       "        -0.67562178, -0.6642165 , -0.73266599, -0.82607427, -0.84643126,\n",
       "        -0.84730125, -0.87046107, -0.83169121, -0.64806183, -0.79489408,\n",
       "        -0.89979158, -0.81892856, -0.82737379, -0.89244628, -0.84007347,\n",
       "        -0.91906962, -0.79267851, -0.82854252, -0.80578683, -0.78951945,\n",
       "        -0.73013367, -0.87079106, -0.65070312, -0.72018724, -0.77544652,\n",
       "        -0.74267773, -0.68445451, -0.75147461, -0.85317032, -0.89691775,\n",
       "        -0.77685927, -0.85715244, -0.78581368, -0.66032363, -0.73647486,\n",
       "        -0.85869328, -0.74825007, -0.72096196, -0.79562658, -0.66512674,\n",
       "        -0.65699253, -0.83945079, -0.87198315, -0.79100637, -0.77523688,\n",
       "        -0.68712041, -0.92826843, -0.77832789, -0.65163581, -0.78767705,\n",
       "        -0.74786676, -0.8163942 , -0.73292693, -0.82687329, -0.76376095,\n",
       "        -0.74732191, -0.81937424, -0.87392346, -0.77281403, -0.63084305,\n",
       "        -0.86054136, -0.85342517, -0.75883019, -1.02013938, -0.86405962,\n",
       "        -0.65515681, -0.77221848, -0.77810224, -0.74557227, -0.78283972,\n",
       "        -1.05913697, -0.88854796, -0.49481468, -0.87712274, -0.89656083,\n",
       "        -0.84611437, -0.74978126, -0.74345829, -0.88839196, -0.76850463,\n",
       "        -0.80738264, -0.87106152, -0.88642616, -0.80460733, -1.07305246,\n",
       "        -0.80317844, -0.79023799, -0.90128824, -0.70060002, -0.7106046 ,\n",
       "        -0.82699232, -0.81351015, -0.90293709, -0.88611793, -0.88049809,\n",
       "        -0.7896307 , -0.83982169, -0.98181318, -0.71991795, -0.79175832,\n",
       "        -0.74331096, -0.82713987, -0.75228618, -0.7839971 , -0.68417443,\n",
       "        -0.95836484, -0.75793179, -0.82172174, -0.72428904, -0.98905762,\n",
       "        -0.82140212, -0.99733939, -0.78914047, -0.7117392 , -0.93744218,\n",
       "        -0.72111335, -0.82716982, -0.74705907, -0.88444045, -0.90797729,\n",
       "        -0.75709918, -0.98499529, -0.91102048, -0.80719643, -0.98795241,\n",
       "        -0.84257779, -0.66737844, -0.86492628, -0.77388949, -0.72467944,\n",
       "        -1.00935848, -0.8098362 , -0.77589328, -0.76380597, -0.87458593,\n",
       "        -0.81843566, -0.78161358, -0.95831564, -0.96923667, -0.79864606,\n",
       "        -0.64871675, -0.7118155 , -0.72540809, -0.72107239, -0.86641254,\n",
       "        -0.64986476, -0.81447895, -0.89374894, -0.68933849, -0.89081061,\n",
       "        -0.76550321, -0.6868435 , -0.82009943, -0.6452278 , -0.75855686,\n",
       "        -0.81596228, -0.84354211, -0.84406933, -0.92745956, -0.78005293,\n",
       "        -0.75208271, -0.78334507, -0.74876297, -0.85537457, -0.75421267,\n",
       "        -0.75607883, -0.84457926, -0.96864549, -0.93726137, -0.77693536,\n",
       "        -0.82234956, -0.93182559, -0.92219537, -0.80352424, -0.90085825,\n",
       "        -0.92060462, -0.73145011, -0.8499021 , -0.81725194, -0.78583198,\n",
       "        -0.81030578, -0.86213354, -0.69467288, -0.78694316, -0.61759823,\n",
       "        -0.78158201, -0.70732444, -0.86231042, -0.70244929, -0.82352166,\n",
       "        -0.76063661, -0.90252765, -0.83667179, -0.87017149, -1.01780312]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psislw_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(k,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 ms ± 18.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psislw_new(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 ms ± 24.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.psislw(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.3 ms ± 1.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psislw_new(school,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.4 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.psislw(school,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Not much improvement'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"LOO\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000,20)\n",
    "mu = np.random.randn(1000,1000)\n",
    "theta = np.random.randn(1000,1000)\n",
    "sd = np.random.randn(1000,1000)\n",
    "posterior = {\"mu\":mu,\"theta\":theta,\"sd\":sd}\n",
    "sample_stats = {\"log_likelihood\":data}\n",
    "data = from_dict(sample_stats=sample_stats,posterior=posterior)\n",
    "school = load_arviz_data(\"centered_eight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> sample_stats"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 6.19942 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: loo at line 309\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   309                                           def loo(data, pointwise=False, reff=None, scale=\"deviance\"):\n",
      "   310                                               \"\"\"Pareto-smoothed importance sampling leave-one-out cross-validation.\n",
      "   311                                           \n",
      "   312                                               Calculates leave-one-out (LOO) cross-validation for out of sample predictive model fit,\n",
      "   313                                               following Vehtari et al. (2017). Cross-validation is computed using Pareto-smoothed\n",
      "   314                                               importance sampling (PSIS).\n",
      "   315                                           \n",
      "   316                                               Parameters\n",
      "   317                                               ----------\n",
      "   318                                               data : result of MCMC run\n",
      "   319                                               pointwise: bool, optional\n",
      "   320                                                   if True the pointwise predictive accuracy will be returned. Defaults to False\n",
      "   321                                               reff : float, optional\n",
      "   322                                                   Relative MCMC efficiency, `ess / n` i.e. number of effective samples divided by\n",
      "   323                                                   the number of actual samples. Computed from trace by default.\n",
      "   324                                               scale : str\n",
      "   325                                                   Output scale for loo. Available options are:\n",
      "   326                                           \n",
      "   327                                                   - `deviance` : (default) -2 * (log-score)\n",
      "   328                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "   329                                                   - `negative_log` : -1 * (log-score)\n",
      "   330                                           \n",
      "   331                                               Returns\n",
      "   332                                               -------\n",
      "   333                                               pandas.Series with the following columns:\n",
      "   334                                               loo: approximated Leave-one-out cross-validation\n",
      "   335                                               loo_se: standard error of loo\n",
      "   336                                               p_loo: effective number of parameters\n",
      "   337                                               shape_warn: 1 if the estimated shape parameter of\n",
      "   338                                                   Pareto distribution is greater than 0.7 for one or more samples\n",
      "   339                                               loo_i: array of pointwise predictive accuracy, only if pointwise True\n",
      "   340                                               pareto_k: array of Pareto shape values, only if pointwise True\n",
      "   341                                               loo_scale: scale of the loo results\n",
      "   342                                               \"\"\"\n",
      "   343         1         14.0     14.0      0.0      inference_data = convert_to_inference_data(data)\n",
      "   344         3          8.0      2.7      0.0      for group in (\"posterior\", \"sample_stats\"):\n",
      "   345         2          6.0      3.0      0.0          if not hasattr(inference_data, group):\n",
      "   346                                                       raise TypeError(\n",
      "   347                                                           \"Must be able to extract a {group} group from data!\".format(group=group)\n",
      "   348                                                       )\n",
      "   349         1          8.0      8.0      0.0      if \"log_likelihood\" not in inference_data.sample_stats:\n",
      "   350                                                   raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
      "   351         1          2.0      2.0      0.0      posterior = inference_data.posterior\n",
      "   352         1        474.0    474.0      0.0      log_likelihood = inference_data.sample_stats.log_likelihood\n",
      "   353         1        515.0    515.0      0.0      n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
      "   354         1         28.0     28.0      0.0      new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
      "   355         1         19.0     19.0      0.0      log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
      "   356                                           \n",
      "   357         1          4.0      4.0      0.0      if scale.lower() == \"deviance\":\n",
      "   358         1          2.0      2.0      0.0          scale_value = -2\n",
      "   359                                               elif scale.lower() == \"log\":\n",
      "   360                                                   scale_value = 1\n",
      "   361                                               elif scale.lower() == \"negative_log\":\n",
      "   362                                                   scale_value = -1\n",
      "   363                                               else:\n",
      "   364                                                   raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
      "   365                                           \n",
      "   366         1          2.0      2.0      0.0      if reff is None:\n",
      "   367                                                   n_chains = len(posterior.chain)\n",
      "   368                                                   if n_chains == 1:\n",
      "   369                                                       reff = 1.0\n",
      "   370                                                   else:\n",
      "   371                                                       ess_p = ess(posterior, method=\"mean\")\n",
      "   372                                                       # this mean is over all data variables\n",
      "   373                                                       reff = (\n",
      "   374                                                           np.hstack([ess_p[v].values.flatten() for v in ess_p.data_vars]).mean() / n_samples\n",
      "   375                                                       )\n",
      "   376                                           \n",
      "   377         1    5391129.0 5391129.0     87.0      log_weights, pareto_shape = psislw(-log_likelihood, reff)\n",
      "   378         1      86314.0  86314.0      1.4      log_weights += log_likelihood\n",
      "   379                                           \n",
      "   380         1          6.0      6.0      0.0      warn_mg = 0\n",
      "   381         1         99.0     99.0      0.0      if np.any(pareto_shape > 0.7):\n",
      "   382                                                   warnings.warn(\n",
      "   383                                                       \"\"\"Estimated shape parameter of Pareto distribution is greater than 0.7 for\n",
      "   384                                                   one or more samples. You should consider using a more robust model, this is because\n",
      "   385                                                   importance sampling is less likely to work well if the marginal posterior and LOO posterior\n",
      "   386                                                   are very different. This is more likely to happen with a non-robust model and highly\n",
      "   387                                                   influential observations.\"\"\"\n",
      "   388                                                   )\n",
      "   389                                                   warn_mg = 1\n",
      "   390                                           \n",
      "   391         1     292076.0 292076.0      4.7      loo_lppd_i = scale_value * _logsumexp(log_weights, axis=0)\n",
      "   392         1         31.0     31.0      0.0      loo_lppd = loo_lppd_i.sum()\n",
      "   393         1        118.0    118.0      0.0      loo_lppd_se = (len(loo_lppd_i) * np.var(loo_lppd_i)) ** 0.5\n",
      "   394                                           \n",
      "   395         1     427432.0 427432.0      6.9      lppd = np.sum(_logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0]))\n",
      "   396         1          8.0      8.0      0.0      p_loo = lppd - loo_lppd / scale_value\n",
      "   397                                           \n",
      "   398         1          3.0      3.0      0.0      if pointwise:\n",
      "   399         1         30.0     30.0      0.0          if np.equal(loo_lppd, loo_lppd_i).all():  # pylint: disable=no-member\n",
      "   400                                                       warnings.warn(\n",
      "   401                                                           \"\"\"The point-wise LOO is the same with the sum LOO, please double check\n",
      "   402                                                                     the Observed RV in your model to make sure it returns element-wise logp.\n",
      "   403                                                                     \"\"\"\n",
      "   404                                                       )\n",
      "   405         1          3.0      3.0      0.0          return pd.Series(\n",
      "   406         1          3.0      3.0      0.0              data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, loo_lppd_i, pareto_shape, scale],\n",
      "   407         1       1091.0   1091.0      0.0              index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_i\", \"pareto_k\", \"loo_scale\"],\n",
      "   408                                                   )\n",
      "   409                                           \n",
      "   410                                               else:\n",
      "   411                                                   return pd.Series(\n",
      "   412                                                       data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, scale],\n",
      "   413                                                       index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_scale\"],\n",
      "   414                                                   )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(loo)\n",
    "wrapper(data,True,0.8)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_new(data, pointwise=False, reff=None, scale=\"deviance\"):\n",
    "    inference_data = convert_to_inference_data(data)\n",
    "    for group in (\"posterior\", \"sample_stats\"):\n",
    "        if not hasattr(inference_data, group):\n",
    "            raise TypeError(\n",
    "                \"Must be able to extract a {group} group from data!\".format(group=group)\n",
    "            )\n",
    "    if \"log_likelihood\" not in inference_data.sample_stats:\n",
    "        raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
    "    posterior = inference_data.posterior\n",
    "    log_likelihood = inference_data.sample_stats.log_likelihood\n",
    "    n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
    "    new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
    "    log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
    "\n",
    "    if scale.lower() == \"deviance\":\n",
    "        scale_value = -2\n",
    "    elif scale.lower() == \"log\":\n",
    "        scale_value = 1\n",
    "    elif scale.lower() == \"negative_log\":\n",
    "        scale_value = -1\n",
    "    else:\n",
    "        raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
    "\n",
    "    if reff is None:\n",
    "        n_chains = len(posterior.chain)\n",
    "        if n_chains == 1:\n",
    "            reff = 1.0\n",
    "        else:\n",
    "            ess_p = ess(posterior, method=\"mean\")\n",
    "            # this mean is over all data variables\n",
    "            reff = (\n",
    "                np.hstack([ess_p[v].values.flatten() for v in ess_p.data_vars]).mean() / n_samples\n",
    "            )\n",
    "\n",
    "    log_weights, pareto_shape = psislw_new(-log_likelihood, reff)\n",
    "    log_weights += log_likelihood\n",
    "\n",
    "    warn_mg = 0\n",
    "    if np.any(pareto_shape > 0.7):\n",
    "        warnings.warn(\n",
    "            \"\"\"Estimated shape parameter of Pareto distribution is greater than 0.7 for\n",
    "        one or more samples. You should consider using a more robust model, this is because\n",
    "        importance sampling is less likely to work well if the marginal posterior and LOO posterior\n",
    "        are very different. This is more likely to happen with a non-robust model and highly\n",
    "        influential observations.\"\"\"\n",
    "        )\n",
    "        warn_mg = 1\n",
    "\n",
    "    loo_lppd_i = scale_value * _logsumexp(log_weights, axis=0)\n",
    "    loo_lppd = loo_lppd_i.sum()\n",
    "    loo_lppd_se = (len(loo_lppd_i) * _var_1d(loo_lppd_i)) ** 0.5\n",
    "\n",
    "    lppd = np.sum(_logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0]))\n",
    "    p_loo = lppd - loo_lppd / scale_value\n",
    "\n",
    "    if pointwise:\n",
    "        if np.equal(loo_lppd, loo_lppd_i).all():  # pylint: disable=no-member\n",
    "            warnings.warn(\n",
    "                \"\"\"The point-wise LOO is the same with the sum LOO, please double check\n",
    "                          the Observed RV in your model to make sure it returns element-wise logp.\n",
    "                          \"\"\"\n",
    "            )\n",
    "        return pd.Series(\n",
    "            data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, loo_lppd_i, pareto_shape, scale],\n",
    "            index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_i\", \"pareto_k\", \"loo_scale\"],\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, scale],\n",
    "            index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_scale\"],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.32 s ± 191 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loo_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.86 s ± 9.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.loo(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "30.4 ms ± 1.96 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loo_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.9 ms ± 944 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.loo(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loo                                                    61.4893\n",
       "loo_se                                                 2.70297\n",
       "p_loo                                                 0.949409\n",
       "warning                                                      0\n",
       "loo_i        [9.77935763951731, 6.840747572952978, 7.726586...\n",
       "pareto_k     [0.4439131521000332, 0.3284878432072214, 0.529...\n",
       "loo_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loo                                                    61.4893\n",
       "loo_se                                                 2.70297\n",
       "p_loo                                                 0.949409\n",
       "warning                                                      0\n",
       "loo_i        [9.77935763951731, 6.840747572952978, 7.726586...\n",
       "pareto_k     [0.4439131521000332, 0.3284878432072214, 0.529...\n",
       "loo_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " az.stats.loo(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def summation(data):\n",
    "    x = 0\n",
    "    for i in range(0,len(data)):\n",
    "        x = x+data[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.randn(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(summation(c), np.sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summation(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.93 ms ± 60.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.random.randn(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def logarithm(data):\n",
    "    log_arr = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        log_arr[i] = math.log(data[i])\n",
    "    return log_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(logarithm(data), np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 ms ± 35.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit logarithm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 µs ± 2.28 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not much improvement in performance. Sometimes a bit slower as well'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Not much improvement in performance. Sometimes a bit slower as well'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Compare'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Compare\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = load_arviz_data(\"centered_eight\")\n",
    "nschool = load_arviz_data(\"non_centered_eight\")\n",
    "compare_dict = {\"1\":school,\"2\":nschool}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>d_waic</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.3022</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.72729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.4296</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.68994</td>\n",
       "      <td>0.106882</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      waic    p_waic    d_waic weight       se       dse warning waic_scale\n",
       "2  61.3022  0.820067         0    0.5  2.72729         0       0   deviance\n",
       "1  61.4296  0.919548  0.127437    0.5  2.68994  0.106882       0   deviance"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(compare_dict, method='stacking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.042857 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: compare at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def compare(\n",
      "    26                                               dataset_dict,\n",
      "    27                                               ic=\"waic\",\n",
      "    28                                               method=\"BB-pseudo-BMA\",\n",
      "    29                                               b_samples=1000,\n",
      "    30                                               alpha=1,\n",
      "    31                                               seed=None,\n",
      "    32                                               scale=\"deviance\",\n",
      "    33                                           ):\n",
      "    34                                               r\"\"\"Compare models based on WAIC or LOO cross validation.\n",
      "    35                                           \n",
      "    36                                               WAIC is Widely applicable information criterion, and LOO is leave-one-out\n",
      "    37                                               (LOO) cross-validation. Read more theory here - in a paper by some of the\n",
      "    38                                               leading authorities on model selection - dx.doi.org/10.1111/1467-9868.00353\n",
      "    39                                           \n",
      "    40                                               Parameters\n",
      "    41                                               ----------\n",
      "    42                                               dataset_dict : dict[str] -> InferenceData\n",
      "    43                                                   A dictionary of model names and InferenceData objects\n",
      "    44                                               ic : str\n",
      "    45                                                   Information Criterion (WAIC or LOO) used to compare models. Default WAIC.\n",
      "    46                                               method : str\n",
      "    47                                                   Method used to estimate the weights for each model. Available options are:\n",
      "    48                                           \n",
      "    49                                                   - 'stacking' : stacking of predictive distributions.\n",
      "    50                                                   - 'BB-pseudo-BMA' : (default) pseudo-Bayesian Model averaging using Akaike-type\n",
      "    51                                                      weighting. The weights are stabilized using the Bayesian bootstrap\n",
      "    52                                                   - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n",
      "    53                                                      weighting, without Bootstrap stabilization (not recommended)\n",
      "    54                                           \n",
      "    55                                                   For more information read https://arxiv.org/abs/1704.02030\n",
      "    56                                               b_samples: int\n",
      "    57                                                   Number of samples taken by the Bayesian bootstrap estimation.\n",
      "    58                                                   Only useful when method = 'BB-pseudo-BMA'.\n",
      "    59                                               alpha : float\n",
      "    60                                                   The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n",
      "    61                                                   useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n",
      "    62                                                   on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n",
      "    63                                               seed : int or np.random.RandomState instance\n",
      "    64                                                   If int or RandomState, use it for seeding Bayesian bootstrap. Only\n",
      "    65                                                   useful when method = 'BB-pseudo-BMA'. Default None the global\n",
      "    66                                                   np.random state is used.\n",
      "    67                                               scale : str\n",
      "    68                                                   Output scale for IC. Available options are:\n",
      "    69                                           \n",
      "    70                                                   - `deviance` : (default) -2 * (log-score)\n",
      "    71                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "    72                                                   - `negative_log` : -1 * (log-score)\n",
      "    73                                           \n",
      "    74                                               Returns\n",
      "    75                                               -------\n",
      "    76                                               A DataFrame, ordered from lowest to highest IC. The index reflects the order in which the\n",
      "    77                                               models are passed to this function. The columns are:\n",
      "    78                                               IC : Information Criteria (WAIC or LOO).\n",
      "    79                                                   Smaller IC indicates higher out-of-sample predictive fit (\"better\" model). Default WAIC.\n",
      "    80                                                   If `scale == log` higher IC indicates higher out-of-sample predictive fit (\"better\" model).\n",
      "    81                                               pIC : Estimated effective number of parameters.\n",
      "    82                                               dIC : Relative difference between each IC (WAIC or LOO)\n",
      "    83                                               and the lowest IC (WAIC or LOO).\n",
      "    84                                                   It's always 0 for the top-ranked model.\n",
      "    85                                               weight: Relative weight for each model.\n",
      "    86                                                   This can be loosely interpreted as the probability of each model (among the compared model)\n",
      "    87                                                   given the data. By default the uncertainty in the weights estimation is considered using\n",
      "    88                                                   Bayesian bootstrap.\n",
      "    89                                               SE : Standard error of the IC estimate.\n",
      "    90                                                   If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n",
      "    91                                               dSE : Standard error of the difference in IC between each model and\n",
      "    92                                               the top-ranked model.\n",
      "    93                                                   It's always 0 for the top-ranked model.\n",
      "    94                                               warning : A value of 1 indicates that the computation of the IC may not be reliable. This could\n",
      "    95                                                   be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details.\n",
      "    96                                               scale : Scale used for the IC.\n",
      "    97                                               \"\"\"\n",
      "    98         1          9.0      9.0      0.0      names = list(dataset_dict.keys())\n",
      "    99         1          5.0      5.0      0.0      scale = scale.lower()\n",
      "   100         1          4.0      4.0      0.0      if scale == \"log\":\n",
      "   101                                                   scale_value = 1\n",
      "   102                                                   ascending = False\n",
      "   103                                               else:\n",
      "   104         1          4.0      4.0      0.0          if scale == \"negative_log\":\n",
      "   105                                                       scale_value = -1\n",
      "   106                                                   else:\n",
      "   107         1          5.0      5.0      0.0              scale_value = -2\n",
      "   108         1          4.0      4.0      0.0          ascending = True\n",
      "   109                                           \n",
      "   110         1          5.0      5.0      0.0      if ic == \"waic\":\n",
      "   111         1          5.0      5.0      0.0          ic_func = waic\n",
      "   112         1          5.0      5.0      0.0          df_comp = pd.DataFrame(\n",
      "   113         1          4.0      4.0      0.0              index=names,\n",
      "   114         1      10362.0  10362.0     24.2              columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
      "   115                                                   )\n",
      "   116         1         13.0     13.0      0.0          scale_col = \"waic_scale\"\n",
      "   117                                           \n",
      "   118                                               elif ic == \"loo\":\n",
      "   119                                                   ic_func = loo\n",
      "   120                                                   df_comp = pd.DataFrame(\n",
      "   121                                                       index=names,\n",
      "   122                                                       columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
      "   123                                                   )\n",
      "   124                                                   scale_col = \"loo_scale\"\n",
      "   125                                           \n",
      "   126                                               else:\n",
      "   127                                                   raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
      "   128                                           \n",
      "   129         1         15.0     15.0      0.0      if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
      "   130                                                   raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
      "   131                                           \n",
      "   132         1         16.0     16.0      0.0      ic_se = \"{}_se\".format(ic)\n",
      "   133         1         14.0     14.0      0.0      p_ic = \"p_{}\".format(ic)\n",
      "   134         1         13.0     13.0      0.0      ic_i = \"{}_i\".format(ic)\n",
      "   135                                           \n",
      "   136         1       1723.0   1723.0      4.0      ics = pd.DataFrame()\n",
      "   137         1         19.0     19.0      0.0      names = []\n",
      "   138         3         30.0     10.0      0.1      for name, dataset in dataset_dict.items():\n",
      "   139         2         22.0     11.0      0.1          names.append(name)\n",
      "   140         2      18796.0   9398.0     43.9          ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
      "   141         1        623.0    623.0      1.5      ics.index = names\n",
      "   142         1       1323.0   1323.0      3.1      ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
      "   143                                           \n",
      "   144         1          7.0      7.0      0.0      if method.lower() == \"stacking\":\n",
      "   145         1       1928.0   1928.0      4.5          rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   146         1         29.0     29.0      0.1          exp_ic_i = np.exp(ic_i_val / scale_value)\n",
      "   147         1          5.0      5.0      0.0          last_col = cols - 1\n",
      "   148                                           \n",
      "   149         1          4.0      4.0      0.0          def w_fuller(weights):\n",
      "   150                                                       return np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
      "   151                                           \n",
      "   152         1          5.0      5.0      0.0          def log_score(weights):\n",
      "   153                                                       w_full = w_fuller(weights)\n",
      "   154                                                       score = 0.0\n",
      "   155                                                       for i in range(rows):\n",
      "   156                                                           score += np.log(np.dot(exp_ic_i[i], w_full))\n",
      "   157                                                       return -score\n",
      "   158                                           \n",
      "   159         1          4.0      4.0      0.0          def gradient(weights):\n",
      "   160                                                       w_full = w_fuller(weights)\n",
      "   161                                                       grad = np.zeros(last_col)\n",
      "   162                                                       for k in range(last_col - 1):\n",
      "   163                                                           for i in range(rows):\n",
      "   164                                                               grad[k] += (exp_ic_i[i, k] - exp_ic_i[i, last_col]) / np.dot(\n",
      "   165                                                                   exp_ic_i[i], w_full\n",
      "   166                                                               )\n",
      "   167                                                       return -grad\n",
      "   168                                           \n",
      "   169         1         30.0     30.0      0.1          theta = np.full(last_col, 1.0 / cols)\n",
      "   170         1          9.0      9.0      0.0          bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
      "   171                                                   constraints = [\n",
      "   172         1          5.0      5.0      0.0              {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
      "   173         1          5.0      5.0      0.0              {\"type\": \"ineq\", \"fun\": np.sum},\n",
      "   174                                                   ]\n",
      "   175                                           \n",
      "   176         1          4.0      4.0      0.0          weights = minimize(\n",
      "   177         1       1224.0   1224.0      2.9              fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
      "   178                                                   )\n",
      "   179                                           \n",
      "   180         1         35.0     35.0      0.1          weights = w_fuller(weights[\"x\"])\n",
      "   181         1        181.0    181.0      0.4          ses = ics[ic_se]\n",
      "   182                                           \n",
      "   183                                               elif method.lower() == \"bb-pseudo-bma\":\n",
      "   184                                                   rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   185                                                   ic_i_val = ic_i_val * rows\n",
      "   186                                           \n",
      "   187                                                   b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
      "   188                                                   weights = np.zeros((b_samples, cols))\n",
      "   189                                                   z_bs = np.zeros_like(weights)\n",
      "   190                                                   for i in range(b_samples):\n",
      "   191                                                       z_b = np.dot(b_weighting[i], ic_i_val)\n",
      "   192                                                       u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
      "   193                                                       z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
      "   194                                                       weights[i] = u_weights / np.sum(u_weights)\n",
      "   195                                           \n",
      "   196                                                   weights = weights.mean(axis=0)\n",
      "   197                                                   ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
      "   198                                           \n",
      "   199                                               elif method.lower() == \"pseudo-bma\":\n",
      "   200                                                   min_ic = ics.iloc[0][ic]\n",
      "   201                                                   z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
      "   202                                                   weights = z_rv / np.sum(z_rv)\n",
      "   203                                                   ses = ics[ic_se]\n",
      "   204                                           \n",
      "   205         1         24.0     24.0      0.1      if np.any(weights):\n",
      "   206         1        101.0    101.0      0.2          min_ic_i_val = ics[ic_i].iloc[0]\n",
      "   207         3         89.0     29.7      0.2          for idx, val in enumerate(ics.index):\n",
      "   208         2       2070.0   1035.0      4.8              res = ics.loc[val]\n",
      "   209         2         19.0      9.5      0.0              if scale_value < 0:\n",
      "   210         2        394.0    197.0      0.9                  diff = res[ic_i] - min_ic_i_val\n",
      "   211                                                       else:\n",
      "   212                                                           diff = min_ic_i_val - res[ic_i]\n",
      "   213         2        117.0     58.5      0.3              d_ic = np.sum(diff)\n",
      "   214         2        209.0    104.5      0.5              d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
      "   215         2        338.0    169.0      0.8              std_err = ses.loc[val]\n",
      "   216         2         10.0      5.0      0.0              weight = weights[idx]\n",
      "   217                                                       df_comp.at[val] = (\n",
      "   218         2        102.0     51.0      0.2                  res[ic],\n",
      "   219         2         79.0     39.5      0.2                  res[p_ic],\n",
      "   220         2          9.0      4.5      0.0                  d_ic,\n",
      "   221         2          8.0      4.0      0.0                  weight,\n",
      "   222         2          8.0      4.0      0.0                  std_err,\n",
      "   223         2          8.0      4.0      0.0                  d_std_err,\n",
      "   224         2         75.0     37.5      0.2                  res[\"warning\"],\n",
      "   225         2       1763.0    881.5      4.1                  res[scale_col],\n",
      "   226                                                       )\n",
      "   227                                           \n",
      "   228         1        940.0    940.0      2.2      return df_comp.sort_values(by=ic, ascending=ascending)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(compare)\n",
    "wrapper(compare_dict,'waic','stacking')\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.103767 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: compare at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def compare(\n",
      "    26                                               dataset_dict,\n",
      "    27                                               ic=\"waic\",\n",
      "    28                                               method=\"BB-pseudo-BMA\",\n",
      "    29                                               b_samples=1000,\n",
      "    30                                               alpha=1,\n",
      "    31                                               seed=None,\n",
      "    32                                               scale=\"deviance\",\n",
      "    33                                           ):\n",
      "    34                                               r\"\"\"Compare models based on WAIC or LOO cross validation.\n",
      "    35                                           \n",
      "    36                                               WAIC is Widely applicable information criterion, and LOO is leave-one-out\n",
      "    37                                               (LOO) cross-validation. Read more theory here - in a paper by some of the\n",
      "    38                                               leading authorities on model selection - dx.doi.org/10.1111/1467-9868.00353\n",
      "    39                                           \n",
      "    40                                               Parameters\n",
      "    41                                               ----------\n",
      "    42                                               dataset_dict : dict[str] -> InferenceData\n",
      "    43                                                   A dictionary of model names and InferenceData objects\n",
      "    44                                               ic : str\n",
      "    45                                                   Information Criterion (WAIC or LOO) used to compare models. Default WAIC.\n",
      "    46                                               method : str\n",
      "    47                                                   Method used to estimate the weights for each model. Available options are:\n",
      "    48                                           \n",
      "    49                                                   - 'stacking' : stacking of predictive distributions.\n",
      "    50                                                   - 'BB-pseudo-BMA' : (default) pseudo-Bayesian Model averaging using Akaike-type\n",
      "    51                                                      weighting. The weights are stabilized using the Bayesian bootstrap\n",
      "    52                                                   - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n",
      "    53                                                      weighting, without Bootstrap stabilization (not recommended)\n",
      "    54                                           \n",
      "    55                                                   For more information read https://arxiv.org/abs/1704.02030\n",
      "    56                                               b_samples: int\n",
      "    57                                                   Number of samples taken by the Bayesian bootstrap estimation.\n",
      "    58                                                   Only useful when method = 'BB-pseudo-BMA'.\n",
      "    59                                               alpha : float\n",
      "    60                                                   The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n",
      "    61                                                   useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n",
      "    62                                                   on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n",
      "    63                                               seed : int or np.random.RandomState instance\n",
      "    64                                                   If int or RandomState, use it for seeding Bayesian bootstrap. Only\n",
      "    65                                                   useful when method = 'BB-pseudo-BMA'. Default None the global\n",
      "    66                                                   np.random state is used.\n",
      "    67                                               scale : str\n",
      "    68                                                   Output scale for IC. Available options are:\n",
      "    69                                           \n",
      "    70                                                   - `deviance` : (default) -2 * (log-score)\n",
      "    71                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "    72                                                   - `negative_log` : -1 * (log-score)\n",
      "    73                                           \n",
      "    74                                               Returns\n",
      "    75                                               -------\n",
      "    76                                               A DataFrame, ordered from lowest to highest IC. The index reflects the order in which the\n",
      "    77                                               models are passed to this function. The columns are:\n",
      "    78                                               IC : Information Criteria (WAIC or LOO).\n",
      "    79                                                   Smaller IC indicates higher out-of-sample predictive fit (\"better\" model). Default WAIC.\n",
      "    80                                                   If `scale == log` higher IC indicates higher out-of-sample predictive fit (\"better\" model).\n",
      "    81                                               pIC : Estimated effective number of parameters.\n",
      "    82                                               dIC : Relative difference between each IC (WAIC or LOO)\n",
      "    83                                               and the lowest IC (WAIC or LOO).\n",
      "    84                                                   It's always 0 for the top-ranked model.\n",
      "    85                                               weight: Relative weight for each model.\n",
      "    86                                                   This can be loosely interpreted as the probability of each model (among the compared model)\n",
      "    87                                                   given the data. By default the uncertainty in the weights estimation is considered using\n",
      "    88                                                   Bayesian bootstrap.\n",
      "    89                                               SE : Standard error of the IC estimate.\n",
      "    90                                                   If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n",
      "    91                                               dSE : Standard error of the difference in IC between each model and\n",
      "    92                                               the top-ranked model.\n",
      "    93                                                   It's always 0 for the top-ranked model.\n",
      "    94                                               warning : A value of 1 indicates that the computation of the IC may not be reliable. This could\n",
      "    95                                                   be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details.\n",
      "    96                                               scale : Scale used for the IC.\n",
      "    97                                               \"\"\"\n",
      "    98         1         11.0     11.0      0.0      names = list(dataset_dict.keys())\n",
      "    99         1          7.0      7.0      0.0      scale = scale.lower()\n",
      "   100         1          5.0      5.0      0.0      if scale == \"log\":\n",
      "   101                                                   scale_value = 1\n",
      "   102                                                   ascending = False\n",
      "   103                                               else:\n",
      "   104         1          4.0      4.0      0.0          if scale == \"negative_log\":\n",
      "   105                                                       scale_value = -1\n",
      "   106                                                   else:\n",
      "   107         1          4.0      4.0      0.0              scale_value = -2\n",
      "   108         1          5.0      5.0      0.0          ascending = True\n",
      "   109                                           \n",
      "   110         1          5.0      5.0      0.0      if ic == \"waic\":\n",
      "   111         1          4.0      4.0      0.0          ic_func = waic\n",
      "   112         1          6.0      6.0      0.0          df_comp = pd.DataFrame(\n",
      "   113         1          4.0      4.0      0.0              index=names,\n",
      "   114         1      11017.0  11017.0     10.6              columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
      "   115                                                   )\n",
      "   116         1          6.0      6.0      0.0          scale_col = \"waic_scale\"\n",
      "   117                                           \n",
      "   118                                               elif ic == \"loo\":\n",
      "   119                                                   ic_func = loo\n",
      "   120                                                   df_comp = pd.DataFrame(\n",
      "   121                                                       index=names,\n",
      "   122                                                       columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
      "   123                                                   )\n",
      "   124                                                   scale_col = \"loo_scale\"\n",
      "   125                                           \n",
      "   126                                               else:\n",
      "   127                                                   raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
      "   128                                           \n",
      "   129         1          6.0      6.0      0.0      if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
      "   130                                                   raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
      "   131                                           \n",
      "   132         1          6.0      6.0      0.0      ic_se = \"{}_se\".format(ic)\n",
      "   133         1          5.0      5.0      0.0      p_ic = \"p_{}\".format(ic)\n",
      "   134         1          4.0      4.0      0.0      ic_i = \"{}_i\".format(ic)\n",
      "   135                                           \n",
      "   136         1        776.0    776.0      0.7      ics = pd.DataFrame()\n",
      "   137         1          6.0      6.0      0.0      names = []\n",
      "   138         3         20.0      6.7      0.0      for name, dataset in dataset_dict.items():\n",
      "   139         2         10.0      5.0      0.0          names.append(name)\n",
      "   140         2      17648.0   8824.0     17.0          ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
      "   141         1        670.0    670.0      0.6      ics.index = names\n",
      "   142         1       1405.0   1405.0      1.4      ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
      "   143                                           \n",
      "   144         1          7.0      7.0      0.0      if method.lower() == \"stacking\":\n",
      "   145                                                   rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   146                                                   exp_ic_i = np.exp(ic_i_val / scale_value)\n",
      "   147                                                   last_col = cols - 1\n",
      "   148                                           \n",
      "   149                                                   def w_fuller(weights):\n",
      "   150                                                       return np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
      "   151                                           \n",
      "   152                                                   def log_score(weights):\n",
      "   153                                                       w_full = w_fuller(weights)\n",
      "   154                                                       score = 0.0\n",
      "   155                                                       for i in range(rows):\n",
      "   156                                                           score += np.log(np.dot(exp_ic_i[i], w_full))\n",
      "   157                                                       return -score\n",
      "   158                                           \n",
      "   159                                                   def gradient(weights):\n",
      "   160                                                       w_full = w_fuller(weights)\n",
      "   161                                                       grad = np.zeros(last_col)\n",
      "   162                                                       for k in range(last_col - 1):\n",
      "   163                                                           for i in range(rows):\n",
      "   164                                                               grad[k] += (exp_ic_i[i, k] - exp_ic_i[i, last_col]) / np.dot(\n",
      "   165                                                                   exp_ic_i[i], w_full\n",
      "   166                                                               )\n",
      "   167                                                       return -grad\n",
      "   168                                           \n",
      "   169                                                   theta = np.full(last_col, 1.0 / cols)\n",
      "   170                                                   bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
      "   171                                                   constraints = [\n",
      "   172                                                       {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
      "   173                                                       {\"type\": \"ineq\", \"fun\": np.sum},\n",
      "   174                                                   ]\n",
      "   175                                           \n",
      "   176                                                   weights = minimize(\n",
      "   177                                                       fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
      "   178                                                   )\n",
      "   179                                           \n",
      "   180                                                   weights = w_fuller(weights[\"x\"])\n",
      "   181                                                   ses = ics[ic_se]\n",
      "   182                                           \n",
      "   183         1          5.0      5.0      0.0      elif method.lower() == \"bb-pseudo-bma\":\n",
      "   184         1       1812.0   1812.0      1.7          rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   185         1         17.0     17.0      0.0          ic_i_val = ic_i_val * rows\n",
      "   186                                           \n",
      "   187         1        705.0    705.0      0.7          b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
      "   188         1         10.0     10.0      0.0          weights = np.zeros((b_samples, cols))\n",
      "   189         1         33.0     33.0      0.0          z_bs = np.zeros_like(weights)\n",
      "   190      1001       4599.0      4.6      4.4          for i in range(b_samples):\n",
      "   191      1000       8279.0      8.3      8.0              z_b = np.dot(b_weighting[i], ic_i_val)\n",
      "   192      1000      22569.0     22.6     21.7              u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
      "   193      1000       6391.0      6.4      6.2              z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
      "   194      1000      20749.0     20.7     20.0              weights[i] = u_weights / np.sum(u_weights)\n",
      "   195                                           \n",
      "   196         1        192.0    192.0      0.2          weights = weights.mean(axis=0)\n",
      "   197         1       1032.0   1032.0      1.0          ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
      "   198                                           \n",
      "   199                                               elif method.lower() == \"pseudo-bma\":\n",
      "   200                                                   min_ic = ics.iloc[0][ic]\n",
      "   201                                                   z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
      "   202                                                   weights = z_rv / np.sum(z_rv)\n",
      "   203                                                   ses = ics[ic_se]\n",
      "   204                                           \n",
      "   205         1         30.0     30.0      0.0      if np.any(weights):\n",
      "   206         1        126.0    126.0      0.1          min_ic_i_val = ics[ic_i].iloc[0]\n",
      "   207         3         85.0     28.3      0.1          for idx, val in enumerate(ics.index):\n",
      "   208         2       1387.0    693.5      1.3              res = ics.loc[val]\n",
      "   209         2         10.0      5.0      0.0              if scale_value < 0:\n",
      "   210         2        168.0     84.0      0.2                  diff = res[ic_i] - min_ic_i_val\n",
      "   211                                                       else:\n",
      "   212                                                           diff = min_ic_i_val - res[ic_i]\n",
      "   213         2         54.0     27.0      0.1              d_ic = np.sum(diff)\n",
      "   214         2        193.0     96.5      0.2              d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
      "   215         2        365.0    182.5      0.4              std_err = ses.loc[val]\n",
      "   216         2         12.0      6.0      0.0              weight = weights[idx]\n",
      "   217                                                       df_comp.at[val] = (\n",
      "   218         2        101.0     50.5      0.1                  res[ic],\n",
      "   219         2         80.0     40.0      0.1                  res[p_ic],\n",
      "   220         2          9.0      4.5      0.0                  d_ic,\n",
      "   221         2          9.0      4.5      0.0                  weight,\n",
      "   222         2          8.0      4.0      0.0                  std_err,\n",
      "   223         2          8.0      4.0      0.0                  d_std_err,\n",
      "   224         2         80.0     40.0      0.1                  res[\"warning\"],\n",
      "   225         2       2026.0   1013.0      2.0                  res[scale_col],\n",
      "   226                                                       )\n",
      "   227                                           \n",
      "   228         1        982.0    982.0      0.9      return df_comp.sort_values(by=ic, ascending=ascending)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(compare)\n",
    "wrapper(compare_dict,'waic','bb-pseudo-bma')\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_new(\n",
    "    dataset_dict,\n",
    "    ic=\"waic\",\n",
    "    method=\"BB-pseudo-BMA\",\n",
    "    b_samples=1000,\n",
    "    alpha=1,\n",
    "    seed=None,\n",
    "    scale=\"deviance\",\n",
    "):\n",
    "    names = list(dataset_dict.keys())\n",
    "    scale = scale.lower()\n",
    "    if scale == \"log\":\n",
    "        scale_value = 1\n",
    "        ascending = False\n",
    "    else:\n",
    "        if scale == \"negative_log\":\n",
    "            scale_value = -1\n",
    "        else:\n",
    "            scale_value = -2\n",
    "        ascending = True\n",
    "\n",
    "    if ic == \"waic\":\n",
    "        ic_func = waic_new\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
    "        )\n",
    "        scale_col = \"waic_scale\"\n",
    "\n",
    "    elif ic == \"loo\":\n",
    "        ic_func = loo\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
    "        )\n",
    "        scale_col = \"loo_scale\"\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
    "\n",
    "    if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
    "        raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
    "\n",
    "    ic_se = \"{}_se\".format(ic)\n",
    "    p_ic = \"p_{}\".format(ic)\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
    "\n",
    "    if method.lower() == \"stacking\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        exp_ic_i = np.exp(ic_i_val / scale_value)\n",
    "        last_col = cols - 1\n",
    "\n",
    "        def w_fuller(weights):\n",
    "            return np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
    "\n",
    "        def log_score(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            score = 0.0\n",
    "            for i in range(rows):\n",
    "                score += np.log(np.dot(exp_ic_i[i], w_full))\n",
    "            return -score\n",
    "\n",
    "        def gradient(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            grad = np.zeros(last_col)\n",
    "            for k in range(last_col - 1):\n",
    "                for i in range(rows):\n",
    "                    grad[k] += (exp_ic_i[i, k] - exp_ic_i[i, last_col]) / np.dot(\n",
    "                        exp_ic_i[i], w_full\n",
    "                    )\n",
    "            return -grad\n",
    "\n",
    "        theta = np.full(last_col, 1.0 / cols)\n",
    "        bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
    "        constraints = [\n",
    "            {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
    "            {\"type\": \"ineq\", \"fun\": np.sum},\n",
    "        ]\n",
    "\n",
    "        weights = minimize(\n",
    "            fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
    "        )\n",
    "\n",
    "        weights = w_fuller(weights[\"x\"])\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    elif method.lower() == \"bb-pseudo-bma\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        ic_i_val = ic_i_val * rows\n",
    "\n",
    "        b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
    "        weights = np.zeros((b_samples, cols))\n",
    "        z_bs = np.zeros_like(weights)\n",
    "        for i in range(b_samples):\n",
    "            z_b = np.dot(b_weighting[i], ic_i_val)\n",
    "            u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
    "            z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
    "            weights[i] = u_weights / np.sum(u_weights)\n",
    "\n",
    "        weights = weights.mean(axis=0)\n",
    "        ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
    "\n",
    "    elif method.lower() == \"pseudo-bma\":\n",
    "        min_ic = ics.iloc[0][ic]\n",
    "        z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
    "        weights = z_rv / np.sum(z_rv)\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    if np.any(weights):\n",
    "        min_ic_i_val = ics[ic_i].iloc[0]\n",
    "        for idx, val in enumerate(ics.index):\n",
    "            res = ics.loc[val]\n",
    "            if scale_value < 0:\n",
    "                diff = res[ic_i] - min_ic_i_val\n",
    "            else:\n",
    "                diff = min_ic_i_val - res[ic_i]\n",
    "            d_ic = np.sum(diff)\n",
    "            d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
    "            std_err = ses.loc[val]\n",
    "            weight = weights[idx]\n",
    "            df_comp.at[val] = (\n",
    "                res[ic],\n",
    "                res[p_ic],\n",
    "                d_ic,\n",
    "                weight,\n",
    "                std_err,\n",
    "                d_std_err,\n",
    "                res[\"warning\"],\n",
    "                res[scale_col],\n",
    "            )\n",
    "\n",
    "    return df_comp.sort_values(by=ic, ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.2 ms ± 70.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compare_new(compare_dict,'waic','stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8 ms ± 89.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.compare(compare_dict,'waic','stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Work in progress. Will finish compare and summary by tomorrow. '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
