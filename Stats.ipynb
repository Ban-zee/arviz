{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'atomicwrites'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c01d03606e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pytest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_assert_rewrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcmdline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhookimpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/_pytest/assertion/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrewrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/_pytest/assertion/rewrite.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0matomicwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_pytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaferepr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaferepr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'atomicwrites'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "from line_profiler import LineProfiler\n",
    "from arviz.data import load_arviz_data,from_dict,convert_to_inference_data\n",
    "from arviz.stats.stats import r2_score, hpd, _gpdfit, _gpinv,_ic_matrix, waic,psislw,loo,compare\n",
    "from arviz.stats.diagnostics import ess\n",
    "from arviz.stats.stats_utils import logsumexp as _logsumexp\n",
    "import scipy.stats as st\n",
    "import numba\n",
    "import math\n",
    "from numpy import sin, cos\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "from pytest import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(10_000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'********************************************hpd********************************************************************'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''********************************************hpd********************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.60059 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271      1001       2539.0      2.5      0.1      if ary.ndim > 1:\n",
      "   272         1          4.0      4.0      0.0          hpd_array = np.array(\n",
      "   273         1         17.0     17.0      0.0              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275         1          3.0      3.0      0.0          return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277      1000     159385.0    159.4      6.1      ary = ary.copy()\n",
      "   278      1000       2666.0      2.7      0.1      n = len(ary)\n",
      "   279                                           \n",
      "   280      1000       1452.0      1.5      0.1      if circular:\n",
      "   281      1000     653994.0    654.0     25.1          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282      1000      17666.0     17.7      0.7          ary = ary - mean\n",
      "   283      1000     743125.0    743.1     28.6          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285      1000     925685.0    925.7     35.6      ary = np.sort(ary)\n",
      "   286      1000      13275.0     13.3      0.5      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287      1000       1752.0      1.8      0.1      n_intervals = n - interval_idx_inc\n",
      "   288      1000      11196.0     11.2      0.4      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290      1000       2501.0      2.5      0.1      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296      1000      14851.0     14.9      0.6      min_idx = np.argmin(interval_width)\n",
      "   297      1000       2869.0      2.9      0.1      hdi_min = ary[min_idx]\n",
      "   298      1000       3569.0      3.6      0.1      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300      1000       1468.0      1.5      0.1      if circular:\n",
      "   301      1000       1861.0      1.9      0.1          hdi_min = hdi_min + mean\n",
      "   302      1000       1414.0      1.4      0.1          hdi_max = hdi_max + mean\n",
      "   303      1000      19023.0     19.0      0.7          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304      1000      10661.0     10.7      0.4          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306      1000       9614.0      9.6      0.4      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_1, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.594237 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271         1          7.0      7.0      0.0      if ary.ndim > 1:\n",
      "   272                                                   hpd_array = np.array(\n",
      "   273                                                       [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275                                                   return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277         1     294627.0 294627.0     49.6      ary = ary.copy()\n",
      "   278         1         10.0     10.0      0.0      n = len(ary)\n",
      "   279                                           \n",
      "   280         1          4.0      4.0      0.0      if circular:\n",
      "   281         1      75145.0  75145.0     12.6          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282         1       3487.0   3487.0      0.6          ary = ary - mean\n",
      "   283         1      77090.0  77090.0     13.0          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285         1     143355.0 143355.0     24.1      ary = np.sort(ary)\n",
      "   286         1         40.0     40.0      0.0      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287         1          4.0      4.0      0.0      n_intervals = n - interval_idx_inc\n",
      "   288         1        202.0    202.0      0.0      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290         1          4.0      4.0      0.0      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296         1        201.0    201.0      0.0      min_idx = np.argmin(interval_width)\n",
      "   297         1          5.0      5.0      0.0      hdi_min = ary[min_idx]\n",
      "   298         1          4.0      4.0      0.0      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300         1          1.0      1.0      0.0      if circular:\n",
      "   301         1          2.0      2.0      0.0          hdi_min = hdi_min + mean\n",
      "   302         1          2.0      2.0      0.0          hdi_max = hdi_max + mean\n",
      "   303         1         27.0     27.0      0.0          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304         1          9.0      9.0      0.0          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306         1         11.0     11.0      0.0      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_2, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.086086 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 249\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   249                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   250                                               \"\"\"\n",
      "   251                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   252                                           \n",
      "   253                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   254                                               for unimodal distributions.\n",
      "   255                                           \n",
      "   256                                               Parameters\n",
      "   257                                               ----------\n",
      "   258                                               x : Numpy array\n",
      "   259                                                   An array containing posterior samples\n",
      "   260                                               credible_interval : float, optional\n",
      "   261                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   262                                               circular : bool, optional\n",
      "   263                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   264                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   265                                           \n",
      "   266                                               Returns\n",
      "   267                                               -------\n",
      "   268                                               np.ndarray\n",
      "   269                                                   lower and upper value of the interval.\n",
      "   270                                               \"\"\"\n",
      "   271       501        921.0      1.8      1.1      if ary.ndim > 1:\n",
      "   272         1          6.0      6.0      0.0          hpd_array = np.array(\n",
      "   273         1        792.0    792.0      0.9              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   274                                                   )\n",
      "   275         1          4.0      4.0      0.0          return hpd_array\n",
      "   276                                               # Make a copy of trace\n",
      "   277       500       3840.0      7.7      4.5      ary = ary.copy()\n",
      "   278       500       1025.0      2.0      1.2      n = len(ary)\n",
      "   279                                           \n",
      "   280       500        777.0      1.6      0.9      if circular:\n",
      "   281       500      25180.0     50.4     29.2          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   282       500       2708.0      5.4      3.1          ary = ary - mean\n",
      "   283       500       3371.0      6.7      3.9          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   284                                           \n",
      "   285       500      14929.0     29.9     17.3      ary = np.sort(ary)\n",
      "   286       500       3449.0      6.9      4.0      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   287       500        693.0      1.4      0.8      n_intervals = n - interval_idx_inc\n",
      "   288       500       2477.0      5.0      2.9      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   289                                           \n",
      "   290       500        850.0      1.7      1.0      if len(interval_width) == 0:\n",
      "   291                                                   raise ValueError(\n",
      "   292                                                       \"Too few elements for interval calculation. \"\n",
      "   293                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   294                                                   )\n",
      "   295                                           \n",
      "   296       500       5902.0     11.8      6.9      min_idx = np.argmin(interval_width)\n",
      "   297       500       1265.0      2.5      1.5      hdi_min = ary[min_idx]\n",
      "   298       500       1297.0      2.6      1.5      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   299                                           \n",
      "   300       500        631.0      1.3      0.7      if circular:\n",
      "   301       500        803.0      1.6      0.9          hdi_min = hdi_min + mean\n",
      "   302       500        685.0      1.4      0.8          hdi_max = hdi_max + mean\n",
      "   303       500       6320.0     12.6      7.3          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   304       500       5100.0     10.2      5.9          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   305                                           \n",
      "   306       500       3061.0      6.1      3.6      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(school, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBottlenecks:\\n    1)scipy.stats.circmean\\n    2)numpy arctan2\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bottlenecks:\n",
    "    1)scipy.stats.circmean\n",
    "    2)numpy arctan2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _circ_mean(ary, high, low):\n",
    "    ary = np.asarray(ary)\n",
    "    if ary.size==0:\n",
    "        return np.nan, np.nan\n",
    "    pi = np.pi\n",
    "    angles = (ary-low)*2*pi/(high-low)\n",
    "    S = sinusoidal(angles)\n",
    "    C = cosine(angles)\n",
    "    res = np.arctan2(S,C)\n",
    "    mask = res < 0\n",
    "    if mask.ndim > 0:\n",
    "        res[mask] += 2*pi\n",
    "    elif mask:\n",
    "        res += 2*pi\n",
    "    return res*(high - low)/2.0/pi + low\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def sinusoidal(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.sin(x[i])\n",
    "    return summ\n",
    "@numba.jit(nopython=True)\n",
    "def cosine(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.cos(x[i])\n",
    "    return summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 ms ± 2.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 ms ± 3.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cumsum(np.sin(data_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.4 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.81 ms ± 253 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 ms ± 17.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2 ms ± 6.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.6 ms ± 379 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.84 ms ± 159 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_1, np.pi, -np.pi), st.circmean(data_1, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_2, np.pi, -np.pi), st.circmean(data_2, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(school, np.pi, -np.pi), st.circmean(school, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 s ± 60.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 ms ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.3 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.3 ms ± 718 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 µs ± 1.94 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 µs ± 22.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Performance of _circ_mean is a worse on larger datasets.\\n   On schools, the performance is much better.\\n   ¯\\\\_(ツ)_/¯\\n   Let's see the overall perfomance of hpd\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance of _circ_mean is a worse on larger datasets.\n",
    "   On schools, the performance is much better.\n",
    "   ¯\\_(ツ)_/¯\n",
    "   Let's see the overall perfomance of hpd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpd_new(ary, credible_interval=0.94, circular=False):\n",
    "    if ary.ndim > 1:\n",
    "        hpd_array = np.array(\n",
    "            [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
    "        )\n",
    "        return hpd_array\n",
    "    # Make a copy of trace\n",
    "    ary = ary.copy()\n",
    "    n = len(ary)\n",
    "\n",
    "    if circular:\n",
    "        mean = _circ_mean(ary, high=np.pi, low=-np.pi)\n",
    "        ary = ary - mean\n",
    "        ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
    "\n",
    "    ary = np.sort(ary)\n",
    "    interval_idx_inc = int(np.floor(credible_interval * n))\n",
    "    n_intervals = n - interval_idx_inc\n",
    "    interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
    "\n",
    "    if len(interval_width) == 0:\n",
    "        raise ValueError(\n",
    "            \"Too few elements for interval calculation. \"\n",
    "            \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
    "        )\n",
    "\n",
    "    min_idx = np.argmin(interval_width)\n",
    "    hdi_min = ary[min_idx]\n",
    "    hdi_max = ary[min_idx + interval_idx_inc]\n",
    "\n",
    "    if circular:\n",
    "        hdi_min = hdi_min + mean\n",
    "        hdi_max = hdi_max + mean\n",
    "        hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
    "        hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
    "\n",
    "    return np.array([hdi_min, hdi_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(school, 0.95,True), az.stats.hpd(school, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_1, 0.95,True), az.stats.hpd(data_1, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_2, 0.95,True), az.stats.hpd(data_2, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.7 ms ± 2.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.8 ms ± 2.94 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 s ± 127 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25 s ± 145 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 ms ± 25.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 ms ± 8.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As seen above, the performance of numba is better with school dataset.\\nThe performance with large datasets is a bit unstable.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''As seen above, the performance of numba is better with school dataset.\n",
    "The performance with large datasets is a bit unstable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'******************************************R2_SCORE***************************************************************'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''******************************************R2_SCORE***************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(1000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "data_3 = np.random.randn(1000,1000)\n",
    "data_4 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "n_school = load_arviz_data(\"non_centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.130522 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 565\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   565                                           def r2_score(y_true, y_pred):\n",
      "   566                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   567                                           \n",
      "   568                                               Parameters\n",
      "   569                                               ----------\n",
      "   570                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   571                                                   Ground truth (correct) target values.\n",
      "   572                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   573                                                   Estimated target values.\n",
      "   574                                           \n",
      "   575                                               Returns\n",
      "   576                                               -------\n",
      "   577                                               Pandas Series with the following indices:\n",
      "   578                                               r2: Bayesian R²\n",
      "   579                                               r2_std: standard deviation of the Bayesian R².\n",
      "   580                                               \"\"\"\n",
      "   581         1          5.0      5.0      0.0      if y_pred.ndim == 1:\n",
      "   582                                                   var_y_est = np.var(y_pred)\n",
      "   583                                                   var_e = np.var(y_true - y_pred)\n",
      "   584                                               else:\n",
      "   585         1       2788.0   2788.0      2.1          var_y_est = np.var(y_pred.mean(0))\n",
      "   586         1      50642.0  50642.0     38.8          var_e = np.var(y_true - y_pred, 0)\n",
      "   587                                           \n",
      "   588         1         73.0     73.0      0.1      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   589                                           \n",
      "   590         1      77014.0  77014.0     59.0      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_1, data_3)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.032282 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 565\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   565                                           def r2_score(y_true, y_pred):\n",
      "   566                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   567                                           \n",
      "   568                                               Parameters\n",
      "   569                                               ----------\n",
      "   570                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   571                                                   Ground truth (correct) target values.\n",
      "   572                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   573                                                   Estimated target values.\n",
      "   574                                           \n",
      "   575                                               Returns\n",
      "   576                                               -------\n",
      "   577                                               Pandas Series with the following indices:\n",
      "   578                                               r2: Bayesian R²\n",
      "   579                                               r2_std: standard deviation of the Bayesian R².\n",
      "   580                                               \"\"\"\n",
      "   581         1          4.0      4.0      0.0      if y_pred.ndim == 1:\n",
      "   582         1       8267.0   8267.0     25.6          var_y_est = np.var(y_pred)\n",
      "   583         1      21732.0  21732.0     67.3          var_e = np.var(y_true - y_pred)\n",
      "   584                                               else:\n",
      "   585                                                   var_y_est = np.var(y_pred.mean(0))\n",
      "   586                                                   var_e = np.var(y_true - y_pred, 0)\n",
      "   587                                           \n",
      "   588         1         12.0     12.0      0.0      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   589                                           \n",
      "   590         1       2267.0   2267.0      7.0      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_2, data_4)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.jit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(b)\n",
    "    for i in range(0,b):\n",
    "        var[i] = _var_1d(data[:,i])\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0039432724541626"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_1d(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0039432724541426"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2), np.var(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_4), np.var(data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.51 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 1.97 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2-data_4),np.var(data_2-data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.4 µs ± 5.21 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 µs ± 1.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(school,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.57 ms ± 661 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.47 ms ± 417 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_3-data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2 ms ± 2.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_3-data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(school), np.var(school,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1), np.var(data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3), np.var(data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3-data_1), np.var(data_3-data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1-data_3), np.var(data_1-data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is doing wonders with variance. Lets inspect mean'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is doing wonders with variance. Lets inspect mean'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def mean_1d(data):\n",
    "    summ = 0\n",
    "    for i in data:\n",
    "        summ = summ+i\n",
    "    return summ/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017768132660067894"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0017768132660068384"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.05 ms ± 361 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mean_1d(data_2-data_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.94 ms ± 326 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is not suitable here. Lets see the overall r2_score performance'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is not suitable here. Lets see the overall r2_score performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_new(y_true, y_pred):\n",
    "    if y_pred.ndim == 1:\n",
    "        var_y_est = _var_1d(y_pred)\n",
    "        var_e = _var_1d(y_true - y_pred)\n",
    "    else:\n",
    "        var_y_est = _var_1d(y_pred.mean(0))\n",
    "        var_e = _var_2d(y_true - y_pred)\n",
    "\n",
    "    r_squared = var_y_est / (var_y_est + var_e)\n",
    "\n",
    "    return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.39 ms ± 327 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.21 ms ± 886 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.4 ms ± 2.29 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597 µs ± 6.01 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712 µs ± 56.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = r2_score_new(school, n_school)\n",
    "df_2 = az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        False\n",
       "r2_std    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1==df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000501\n",
       "r2_std    0.000023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000501\n",
       "r2_std    0.000023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have not included the pandas test yet...will include them soon'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''I have not included the pandas test yet...will include them soon'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It can be clearly seen that numba greatly improves the performance of r2_score.\\nTBH, this was the function which I thought had the least scope of improvement\\nThank you mentors.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''It can be clearly seen that numba greatly improves the performance of r2_score.\n",
    "TBH, this was the function which I thought had the least scope of improvement\n",
    "Thank you mentors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpdfit'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpdfit\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.sort(np.random.randn(100000)))\n",
    "school = np.abs((np.sort((load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)[1,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 1.26034 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 488\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   488                                           def _gpdfit(ary):\n",
      "   489                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   490                                           \n",
      "   491                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   492                                               distribution given the data.\n",
      "   493                                           \n",
      "   494                                               Parameters\n",
      "   495                                               ----------\n",
      "   496                                               ary : array\n",
      "   497                                                   sorted 1D data array\n",
      "   498                                           \n",
      "   499                                               Returns\n",
      "   500                                               -------\n",
      "   501                                               k : float\n",
      "   502                                                   estimated shape parameter\n",
      "   503                                               sigma : float\n",
      "   504                                                   estimated scale parameter\n",
      "   505                                               \"\"\"\n",
      "   506         1          5.0      5.0      0.0      prior_bs = 3\n",
      "   507         1          4.0      4.0      0.0      prior_k = 10\n",
      "   508         1          5.0      5.0      0.0      n = len(ary)\n",
      "   509         1         12.0     12.0      0.0      m_est = 30 + int(n ** 0.5)\n",
      "   510                                           \n",
      "   511         1        120.0    120.0      0.0      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   512         1         46.0     46.0      0.0      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   513         1         22.0     22.0      0.0      b_ary += 1 / ary[-1]\n",
      "   514                                           \n",
      "   515         1     783837.0 783837.0     62.2      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   516         1        102.0    102.0      0.0      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   517         1     475388.0 475388.0     37.7      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   518                                           \n",
      "   519                                               # remove negligible weights\n",
      "   520         1         74.0     74.0      0.0      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   521         1         54.0     54.0      0.0      if not np.all(real_idxs):\n",
      "   522         1         10.0     10.0      0.0          weights = weights[real_idxs]\n",
      "   523         1          4.0      4.0      0.0          b_ary = b_ary[real_idxs]\n",
      "   524                                               # normalise weights\n",
      "   525         1         27.0     27.0      0.0      weights /= weights.sum()\n",
      "   526                                           \n",
      "   527                                               # posterior mean for b\n",
      "   528         1         34.0     34.0      0.0      b_post = np.sum(b_ary * weights)\n",
      "   529                                               # estimate for k\n",
      "   530         1        589.0    589.0      0.0      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   531                                               # add prior for k_post\n",
      "   532         1          4.0      4.0      0.0      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   533         1          1.0      1.0      0.0      sigma = -k_post / b_post\n",
      "   534                                           \n",
      "   535         1          1.0      1.0      0.0      return k_post, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.003453 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 488\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   488                                           def _gpdfit(ary):\n",
      "   489                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   490                                           \n",
      "   491                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   492                                               distribution given the data.\n",
      "   493                                           \n",
      "   494                                               Parameters\n",
      "   495                                               ----------\n",
      "   496                                               ary : array\n",
      "   497                                                   sorted 1D data array\n",
      "   498                                           \n",
      "   499                                               Returns\n",
      "   500                                               -------\n",
      "   501                                               k : float\n",
      "   502                                                   estimated shape parameter\n",
      "   503                                               sigma : float\n",
      "   504                                                   estimated scale parameter\n",
      "   505                                               \"\"\"\n",
      "   506         1          4.0      4.0      0.1      prior_bs = 3\n",
      "   507         1          3.0      3.0      0.1      prior_k = 10\n",
      "   508         1          5.0      5.0      0.1      n = len(ary)\n",
      "   509         1         15.0     15.0      0.4      m_est = 30 + int(n ** 0.5)\n",
      "   510                                           \n",
      "   511         1        122.0    122.0      3.5      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   512         1         37.0     37.0      1.1      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   513         1         16.0     16.0      0.5      b_ary += 1 / ary[-1]\n",
      "   514                                           \n",
      "   515         1       2249.0   2249.0     65.1      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   516         1         36.0     36.0      1.0      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   517         1        690.0    690.0     20.0      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   518                                           \n",
      "   519                                               # remove negligible weights\n",
      "   520         1         40.0     40.0      1.2      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   521         1         38.0     38.0      1.1      if not np.all(real_idxs):\n",
      "   522         1         11.0     11.0      0.3          weights = weights[real_idxs]\n",
      "   523         1          5.0      5.0      0.1          b_ary = b_ary[real_idxs]\n",
      "   524                                               # normalise weights\n",
      "   525         1         28.0     28.0      0.8      weights /= weights.sum()\n",
      "   526                                           \n",
      "   527                                               # posterior mean for b\n",
      "   528         1         33.0     33.0      1.0      b_post = np.sum(b_ary * weights)\n",
      "   529                                               # estimate for k\n",
      "   530         1        109.0    109.0      3.2      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   531                                               # add prior for k_post\n",
      "   532         1          7.0      7.0      0.2      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   533         1          3.0      3.0      0.1      sigma = -k_post / b_post\n",
      "   534                                           \n",
      "   535         1          2.0      2.0      0.1      return k_post, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(school)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck at 518'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bottleneck at 518\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def log_1p(data):\n",
    "    log_p = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        if data[i]>-1:\n",
    "            log_p[i] = math.log1p(data[i])\n",
    "        else:\n",
    "            log_p[i] = np.nan\n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(data),np.log1p(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(school), np.log1p(school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 ms ± 87.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 µs ± 6.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4 µs ± 125 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 µs ± 285 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\\nwhile in 10% of trials the performance comes down a notch w.r.t to numpy. Performance on large datasets is....slow \\nto put it mildly'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\n",
    "while in 10% of trials the performance comes down a notch w.r.t to numpy. Performance on large datasets is....slow \n",
    "to put it mildly'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overall performance of gpdfit'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''overall performance of gpdfit'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpdfit_new(ary):\n",
    "    prior_bs = 3\n",
    "    prior_k = 10\n",
    "    n = len(ary)\n",
    "    m_est = 30 + int(n ** 0.5)\n",
    "\n",
    "    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
    "    b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
    "    b_ary += 1 / ary[-1]\n",
    "    \n",
    "    K = -b_ary[:, None] * ary\n",
    "    k_ary = np.log1p(K).mean(axis=1) # pylint: disable=no-member Using my own log_1p here throws an exception\n",
    "    len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
    "    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
    "\n",
    "    # remove negligible weights\n",
    "    real_idxs = weights >= 10 * np.finfo(float).eps\n",
    "    if not np.all(real_idxs):\n",
    "        weights = weights[real_idxs]\n",
    "        b_ary = b_ary[real_idxs]\n",
    "    # normalise weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b_post = np.sum(b_ary * weights)\n",
    "    # estimate for k\n",
    "    k_post = log_1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
    "    # add prior for k_post\n",
    "    k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
    "    sigma = -k_post / b_post\n",
    "    return k_post, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 ms ± 3.95 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 ms ± 3.55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 µs ± 7.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536 µs ± 54.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similar performance on both the datasets. Up for discussion with mentors'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Similar performance on both the datasets. Up for discussion with mentors'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpvinv'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpvinv\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(100000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.058517 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 538\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   538                                           def _gpinv(probs, kappa, sigma):\n",
      "   539                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   540                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   541         1        310.0    310.0      0.5      x = np.full_like(probs, np.nan)\n",
      "   542         1          2.0      2.0      0.0      if sigma <= 0:\n",
      "   543                                                   return x\n",
      "   544         1        969.0    969.0      1.7      ok = (probs > 0) & (probs < 1)\n",
      "   545         1         74.0     74.0      0.1      if np.all(ok):\n",
      "   546                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   547                                                       x = -np.log1p(-probs)\n",
      "   548                                                   else:\n",
      "   549                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   550                                                   x *= sigma\n",
      "   551                                               else:\n",
      "   552         1         63.0     63.0      0.1          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   553                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   554                                                   else:\n",
      "   555         1      56621.0  56621.0     96.8              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   556         1        160.0    160.0      0.3          x *= sigma\n",
      "   557         1        179.0    179.0      0.3          x[probs == 0] = 0\n",
      "   558         1          3.0      3.0      0.0          if kappa >= 0:\n",
      "   559         1        134.0    134.0      0.2              x[probs == 1] = np.inf\n",
      "   560                                                   else:\n",
      "   561                                                       x[probs == 1] = -sigma / kappa\n",
      "   562         1          2.0      2.0      0.0      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(data, 5,2)\n",
    "lp.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000297 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 538\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   538                                           def _gpinv(probs, kappa, sigma):\n",
      "   539                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   540                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   541         1         43.0     43.0     14.5      x = np.full_like(probs, np.nan)\n",
      "   542         1          5.0      5.0      1.7      if sigma <= 0:\n",
      "   543                                                   return x\n",
      "   544         1         38.0     38.0     12.8      ok = (probs > 0) & (probs < 1)\n",
      "   545         1         47.0     47.0     15.8      if np.all(ok):\n",
      "   546                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   547                                                       x = -np.log1p(-probs)\n",
      "   548                                                   else:\n",
      "   549                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   550                                                   x *= sigma\n",
      "   551                                               else:\n",
      "   552         1         47.0     47.0     15.8          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   553                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   554                                                   else:\n",
      "   555         1         70.0     70.0     23.6              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   556         1         16.0     16.0      5.4          x *= sigma\n",
      "   557         1         14.0     14.0      4.7          x[probs == 0] = 0\n",
      "   558         1          3.0      3.0      1.0          if kappa >= 0:\n",
      "   559         1         12.0     12.0      4.0              x[probs == 1] = np.inf\n",
      "   560                                                   else:\n",
      "   561                                                       x[probs == 1] = -sigma / kappa\n",
      "   562         1          2.0      2.0      0.7      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(school, 5,2)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Log1p or expm is the bottleneck again'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Log1p or expm is the bottleneck again'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def expm(data):\n",
    "    expm = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        expm[i] = math.expm1(data[i])\n",
    "    return expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(expm(data), np.expm1(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.79 ms ± 69.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 µs ± 4.27 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7 µs ± 322 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 µs ± 79.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numpy is a bit faster\\nLets see the overall performance'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numpy is a bit faster\n",
    "Lets see the overall performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpinv_new(probs, kappa, sigma):\n",
    "    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
    "    # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
    "    x = np.full_like(probs, np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (probs > 0) & (probs < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x = -log_1p(-probs)\n",
    "        else:\n",
    "            x = expm(-kappa * np.log1p(-probs)) / kappa\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x[ok] = -log_1p(-probs[ok])\n",
    "        else:\n",
    "            x[ok] = expm(-kappa * np.log1p(-probs[ok])) / kappa\n",
    "        x *= sigma\n",
    "        x[probs == 0] = 0\n",
    "        if kappa >= 0:\n",
    "            x[probs == 1] = np.inf\n",
    "        else:\n",
    "            x[probs == 1] = -sigma / kappa\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _gpinv_new(data,-5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan, ..., 0.4       ,        nan,\n",
       "       0.15525064])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan, ..., 0.4       ,        nan,\n",
       "       0.15525064])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ...,  0., nan,  0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.67 ms ± 170 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpinv_new(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.85 ms ± 78.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _gpinv_new(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.4 µs ± 1.75 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numba _gpinv is similar in performance'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numba _gpinv is similar in performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataset_dict,ic='waic'):\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([waic(dataset, pointwise=True, scale=\"deviance\")])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=True)\n",
    "    return ics, ic_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics, ic_i = generator({\"1\":load_arviz_data(\"centered_eight\"),\"2\":load_arviz_data(\"non_centered_eight\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>waic_se</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_i</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.302151</td>\n",
       "      <td>2.727291</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.732429123207623, 6.813708378182436, 7.70616...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.429587</td>\n",
       "      <td>2.689941</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.76127241619229, 6.832600316056821, 7.725554...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        waic   waic_se    p_waic  warning  \\\n",
       "2  61.302151  2.727291  0.820067        0   \n",
       "1  61.429587  2.689941  0.919548        0   \n",
       "\n",
       "                                              waic_i waic_scale  \n",
       "2  [9.732429123207623, 6.813708378182436, 7.70616...   deviance  \n",
       "1  [9.76127241619229, 6.832600316056821, 7.725554...   deviance  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.004775 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _ic_matrix at line 232\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   232                                           def _ic_matrix(ics, ic_i):\n",
      "   233                                               \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
      "   234         1         30.0     30.0      0.6      cols, _ = ics.shape\n",
      "   235         1        763.0    763.0     16.0      rows = len(ics[ic_i].iloc[0])\n",
      "   236         1         13.0     13.0      0.3      ic_i_val = np.zeros((rows, cols))\n",
      "   237                                           \n",
      "   238         3        203.0     67.7      4.3      for idx, val in enumerate(ics.index):\n",
      "   239         2       3708.0   1854.0     77.7          ic = ics.loc[val][ic_i]\n",
      "   240                                           \n",
      "   241         2         11.0      5.5      0.2          if len(ic) != rows:\n",
      "   242                                                       raise ValueError(\"The number of observations should be the same across all models\")\n",
      "   243                                           \n",
      "   244         2         46.0     23.0      1.0          ic_i_val[:, idx] = ic\n",
      "   245                                           \n",
      "   246         1          1.0      1.0      0.0      return rows, cols, ic_i_val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_ic_matrix)\n",
    "wrapper(ics,ic_i)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def loop_lifter(ics,rows,cols):\n",
    "    ic_i_val = np.zeros((rows, cols))\n",
    "    for idx, val in enumerate(ics.index):\n",
    "        ic = ics.loc[val][ic_i]\n",
    "\n",
    "        if len(ic) != rows:\n",
    "            raise ValueError(\"The number of observations should be the same across all models\")\n",
    "\n",
    "        ic_i_val[:, idx] = ic\n",
    "    return ic_i_val\n",
    "\n",
    "def _ic_matrix_new(ics, ic_i):\n",
    "    \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
    "    cols, _ = ics.shape\n",
    "    rows = len(ics[ic_i].iloc[0])\n",
    "    return rows, cols, loop_lifter(ics,rows,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814 µs ± 116 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ic_matrix_new(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717 µs ± 72.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._ic_matrix(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\\n   Original _ic_matrix is better'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\n",
    "   Original _ic_matrix is better'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000,20)\n",
    "sample_stats = {\"log_likelihood\":data}\n",
    "data = from_dict(sample_stats=sample_stats)\n",
    "school = load_arviz_data(\"centered_eight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.678223 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: waic at line 865\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   865                                           def waic(data, pointwise=False, scale=\"deviance\"):\n",
      "   866                                               \"\"\"Calculate the widely available information criterion.\n",
      "   867                                           \n",
      "   868                                               Also calculates the WAIC's standard error and the effective number of\n",
      "   869                                               parameters of the samples in trace from model. Read more theory here - in\n",
      "   870                                               a paper by some of the leading authorities on model selection\n",
      "   871                                               dx.doi.org/10.1111/1467-9868.00353\n",
      "   872                                           \n",
      "   873                                               Parameters\n",
      "   874                                               ----------\n",
      "   875                                               data : obj\n",
      "   876                                                   Any object that can be converted to an az.InferenceData object\n",
      "   877                                                   Refer to documentation of az.convert_to_dataset for details\n",
      "   878                                               pointwise: bool\n",
      "   879                                                   if True the pointwise predictive accuracy will be returned.\n",
      "   880                                                   Default False\n",
      "   881                                               scale : str\n",
      "   882                                                   Output scale for loo. Available options are:\n",
      "   883                                           \n",
      "   884                                                   - `deviance` : (default) -2 * (log-score)\n",
      "   885                                                   - `log` : 1 * log-score\n",
      "   886                                                   - `negative_log` : -1 * (log-score)\n",
      "   887                                           \n",
      "   888                                               Returns\n",
      "   889                                               -------\n",
      "   890                                               DataFrame with the following columns:\n",
      "   891                                               waic: widely available information criterion\n",
      "   892                                               waic_se: standard error of waic\n",
      "   893                                               p_waic: effective number parameters\n",
      "   894                                               var_warn: 1 if posterior variance of the log predictive\n",
      "   895                                                    densities exceeds 0.4\n",
      "   896                                               waic_i: and array of the pointwise predictive accuracy, only if pointwise True\n",
      "   897                                               waic_scale: scale of the waic results\n",
      "   898                                               \"\"\"\n",
      "   899         1         41.0     41.0      0.0      inference_data = convert_to_inference_data(data)\n",
      "   900         2         11.0      5.5      0.0      for group in (\"sample_stats\",):\n",
      "   901         1          8.0      8.0      0.0          if not hasattr(inference_data, group):\n",
      "   902                                                       raise TypeError(\n",
      "   903                                                           \"Must be able to extract a {group} group from data!\".format(group=group)\n",
      "   904                                                       )\n",
      "   905         1         24.0     24.0      0.0      if \"log_likelihood\" not in inference_data.sample_stats:\n",
      "   906                                                   raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
      "   907         1       1225.0   1225.0      0.2      log_likelihood = inference_data.sample_stats.log_likelihood\n",
      "   908                                           \n",
      "   909         1          8.0      8.0      0.0      if scale.lower() == \"deviance\":\n",
      "   910         1          5.0      5.0      0.0          scale_value = -2\n",
      "   911                                               elif scale.lower() == \"log\":\n",
      "   912                                                   scale_value = 1\n",
      "   913                                               elif scale.lower() == \"negative_log\":\n",
      "   914                                                   scale_value = -1\n",
      "   915                                               else:\n",
      "   916                                                   raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
      "   917                                           \n",
      "   918         1       1658.0   1658.0      0.2      n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
      "   919         1         84.0     84.0      0.0      new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
      "   920         1         51.0     51.0      0.0      log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
      "   921                                           \n",
      "   922         1     402405.0 402405.0     59.3      lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
      "   923                                           \n",
      "   924         1     270846.0 270846.0     39.9      vars_lpd = np.var(log_likelihood, axis=0)\n",
      "   925         1          4.0      4.0      0.0      warn_mg = 0\n",
      "   926         1         81.0     81.0      0.0      if np.any(vars_lpd > 0.4):\n",
      "   927         1          4.0      4.0      0.0          warnings.warn(\n",
      "   928                                                       \"\"\"For one or more samples the posterior variance of the log predictive\n",
      "   929                                                   densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "   930                                                   http://arxiv.org/abs/1507.04544 for details\n",
      "   931         1        244.0    244.0      0.0          \"\"\"\n",
      "   932                                                   )\n",
      "   933         1          2.0      2.0      0.0          warn_mg = 1\n",
      "   934                                           \n",
      "   935         1         19.0     19.0      0.0      waic_i = scale_value * (lppd_i - vars_lpd)\n",
      "   936         1        108.0    108.0      0.0      waic_se = (len(waic_i) * np.var(waic_i)) ** 0.5\n",
      "   937         1         17.0     17.0      0.0      waic_sum = np.sum(waic_i)\n",
      "   938         1         11.0     11.0      0.0      p_waic = np.sum(vars_lpd)\n",
      "   939                                           \n",
      "   940         1          2.0      2.0      0.0      if pointwise:\n",
      "   941         1         20.0     20.0      0.0          if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
      "   942                                                       warnings.warn(\n",
      "   943                                                           \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
      "   944                                                       the Observed RV in your model to make sure it returns element-wise logp.\n",
      "   945                                                       \"\"\"\n",
      "   946                                                       )\n",
      "   947         1          3.0      3.0      0.0          return pd.Series(\n",
      "   948         1          2.0      2.0      0.0              data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
      "   949         1       1340.0   1340.0      0.2              index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
      "   950                                                   )\n",
      "   951                                               else:\n",
      "   952                                                   return pd.Series(\n",
      "   953                                                       data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
      "   954                                                       index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
      "   955                                                   )\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(waic)\n",
    "wrapper(data,True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_new(data, pointwise=False, scale=\"deviance\"):\n",
    "    inference_data = convert_to_inference_data(data)\n",
    "    for group in (\"sample_stats\",):\n",
    "        if not hasattr(inference_data, group):\n",
    "            raise TypeError(\n",
    "                \"Must be able to extract a {group} group from data!\".format(group=group)\n",
    "            )\n",
    "    if \"log_likelihood\" not in inference_data.sample_stats:\n",
    "        raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
    "    log_likelihood = inference_data.sample_stats.log_likelihood\n",
    "\n",
    "    if scale.lower() == \"deviance\":\n",
    "        scale_value = -2\n",
    "    elif scale.lower() == \"log\":\n",
    "        scale_value = 1\n",
    "    elif scale.lower() == \"negative_log\":\n",
    "        scale_value = -1\n",
    "    else:\n",
    "        raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
    "\n",
    "    n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
    "    new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
    "    log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
    "\n",
    "    lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
    "\n",
    "    vars_lpd = _var_2d(log_likelihood)\n",
    "    warn_mg = 0\n",
    "    if np.any(vars_lpd > 0.4):\n",
    "        warnings.warn(\n",
    "            \"\"\"For one or more samples the posterior variance of the log predictive\n",
    "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
    "        http://arxiv.org/abs/1507.04544 for details\n",
    "        \"\"\"\n",
    "        )\n",
    "        warn_mg = 1\n",
    "\n",
    "    waic_i = scale_value * (lppd_i - vars_lpd)\n",
    "    waic_se = (len(waic_i) * _var_1d(waic_i)) ** 0.5\n",
    "    waic_sum = np.sum(waic_i)\n",
    "    p_waic = np.sum(vars_lpd)\n",
    "\n",
    "    if pointwise:\n",
    "        if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
    "            warnings.warn(\n",
    "                \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
    "            the Observed RV in your model to make sure it returns element-wise logp.\n",
    "            \"\"\"\n",
    "            )\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
    "        )\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    20.0055\n",
       "waic_se                                               0.0109872\n",
       "p_waic                                                   20.014\n",
       "warning                                                       1\n",
       "waic_i        [1.0001109256408993, 0.9997360431832794, 0.998...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    20.0055\n",
       "waic_se                                               0.0109872\n",
       "p_waic                                                   20.014\n",
       "warning                                                       1\n",
       "waic_i        [1.0001109256408607, 0.9997360431832267, 0.998...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:931: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 ms ± 90.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41 ms ± 208 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57 ms ± 374 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psislw'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"psislw\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def max_elem(data):\n",
    "    max = data[0]\n",
    "    for i in range(0,len(data)):\n",
    "        if data[i]>max:\n",
    "            max = data[i]\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(1000,1000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.789172 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: psislw at line 417\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   417                                           def psislw(log_weights, reff=1.0):\n",
      "   418                                               \"\"\"\n",
      "   419                                               Pareto smoothed importance sampling (PSIS).\n",
      "   420                                           \n",
      "   421                                               Parameters\n",
      "   422                                               ----------\n",
      "   423                                               log_weights : array\n",
      "   424                                                   Array of size (n_samples, n_observations)\n",
      "   425                                               reff : float\n",
      "   426                                                   relative MCMC efficiency, `ess / n`\n",
      "   427                                           \n",
      "   428                                               Returns\n",
      "   429                                               -------\n",
      "   430                                               lw_out : array\n",
      "   431                                                   Smoothed log weights\n",
      "   432                                               kss : array\n",
      "   433                                                   Pareto tail indices\n",
      "   434                                               \"\"\"\n",
      "   435         1          4.0      4.0      0.0      rows, cols = log_weights.shape\n",
      "   436                                           \n",
      "   437         1      13747.0  13747.0      1.7      log_weights_out = np.copy(log_weights, order=\"F\")\n",
      "   438         1         60.0     60.0      0.0      kss = np.empty(cols)\n",
      "   439                                           \n",
      "   440                                               # precalculate constants\n",
      "   441         1         55.0     55.0      0.0      cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
      "   442         1         46.0     46.0      0.0      cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
      "   443         1          2.0      2.0      0.0      k_min = 1.0 / 3\n",
      "   444                                           \n",
      "   445                                               # loop over sets of log weights\n",
      "   446      1001       3355.0      3.4      0.4      for i, x in enumerate(log_weights_out.T):\n",
      "   447                                                   # improve numerical accuracy\n",
      "   448      1000      24919.0     24.9      3.2          x -= np.max(x)\n",
      "   449                                                   # sort the array\n",
      "   450      1000      95035.0     95.0     12.0          x_sort_ind = np.argsort(x)\n",
      "   451                                                   # divide log weights into body and right tail\n",
      "   452      1000       5911.0      5.9      0.7          xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
      "   453                                           \n",
      "   454      1000       7997.0      8.0      1.0          expxcutoff = np.exp(xcutoff)\n",
      "   455      1000      14318.0     14.3      1.8          tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
      "   456      1000       3282.0      3.3      0.4          x_tail = x[tailinds]\n",
      "   457      1000       2199.0      2.2      0.3          tail_len = len(x_tail)\n",
      "   458      1000       1726.0      1.7      0.2          if tail_len <= 4:\n",
      "   459                                                       # not enough tail samples for gpdfit\n",
      "   460                                                       k = np.inf\n",
      "   461                                                   else:\n",
      "   462                                                       # order of tail samples\n",
      "   463      1000      17644.0     17.6      2.2              x_tail_si = np.argsort(x_tail)\n",
      "   464                                                       # fit generalized Pareto distribution to the right tail samples\n",
      "   465      1000      13018.0     13.0      1.6              x_tail = np.exp(x_tail) - expxcutoff\n",
      "   466      1000     427005.0    427.0     54.1              k, sigma = _gpdfit(x_tail[x_tail_si])\n",
      "   467                                           \n",
      "   468      1000       3197.0      3.2      0.4              if k >= k_min:\n",
      "   469                                                           # no smoothing if short tail or GPD fit failed\n",
      "   470                                                           # compute ordered statistic for the fit\n",
      "   471                                                           sti = np.arange(0.5, tail_len) / tail_len\n",
      "   472                                                           smoothed_tail = _gpinv(sti, k, sigma)\n",
      "   473                                                           smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
      "   474                                                               smoothed_tail + expxcutoff\n",
      "   475                                                           )\n",
      "   476                                                           # place the smoothed tail into the output array\n",
      "   477                                                           x[tailinds[x_tail_si]] = smoothed_tail\n",
      "   478                                                           # truncate smoothed values to the largest raw weight 0\n",
      "   479                                                           x[x > 0] = 0\n",
      "   480                                                   # renormalize weights\n",
      "   481      1000     152134.0    152.1     19.3          x -= _logsumexp(x)\n",
      "   482                                                   # store tail index k\n",
      "   483      1000       3516.0      3.5      0.4          kss[i] = k\n",
      "   484                                           \n",
      "   485         1          2.0      2.0      0.0      return log_weights_out, kss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(psislw)\n",
    "wrapper(data, 0.66)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lets use our own gpd funcs'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lets use our own gpd funcs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def psislw_new(log_weights, reff=1.0):\n",
    "    rows, cols = log_weights.shape\n",
    "\n",
    "    log_weights_out = np.copy(log_weights, order=\"F\")\n",
    "    kss = np.empty(cols)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
    "    k_min = 1.0 / 3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(log_weights_out.T):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
    "\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        x_tail = x[tailinds]\n",
    "        tail_len = len(x_tail)\n",
    "        if tail_len <= 4:\n",
    "            # not enough tail samples for gpdfit\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x_tail_si = np.argsort(x_tail)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            x_tail = np.exp(x_tail) - expxcutoff\n",
    "            k, sigma = _gpdfit_new(x_tail[x_tail_si])\n",
    "\n",
    "            if k >= k_min:\n",
    "                # no smoothing if short tail or GPD fit failed\n",
    "                # compute ordered statistic for the fit\n",
    "                sti = np.arange(0.5, tail_len) / tail_len\n",
    "                smoothed_tail = _gpinv_new(sti, k, sigma)\n",
    "                print(smoothed_tail.shape)\n",
    "                smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
    "                    smoothed_tail + expxcutoff\n",
    "                )\n",
    "                # place the smoothed tail into the output array\n",
    "                x[tailinds[x_tail_si]] = smoothed_tail\n",
    "                # truncate smoothed values to the largest raw weight 0\n",
    "                x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= _logsumexp(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "\n",
    "    return log_weights_out, kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-7.24458336, -6.45542208, -7.22064496, ..., -7.20762862,\n",
       "         -6.79213841, -6.58060632],\n",
       "        [-7.41325314, -7.07948432, -7.20591076, ..., -7.16446562,\n",
       "         -7.068009  , -7.29749703],\n",
       "        [-6.63120208, -6.92522927, -7.35638611, ..., -7.05805899,\n",
       "         -7.17555111, -7.27267608],\n",
       "        ...,\n",
       "        [-6.62006196, -6.90663181, -7.00081026, ..., -6.63930009,\n",
       "         -7.04663538, -6.73331676],\n",
       "        [-7.07982562, -6.46380939, -7.26656309, ..., -6.85599341,\n",
       "         -7.42139695, -6.85001487],\n",
       "        [-7.21116532, -6.73833036, -7.37009515, ..., -6.90124615,\n",
       "         -6.75996199, -6.83192004]]),\n",
       " array([-0.73279016, -0.90755503, -0.86620979, -0.64396395, -0.72351762,\n",
       "        -0.75122911, -0.92520345, -0.64258316, -0.58947724, -0.94671867,\n",
       "        -0.74093495, -0.76844757, -0.91327719, -0.89761062, -0.99053363,\n",
       "        -0.69668055, -1.065511  , -0.74380369, -0.80705144, -0.77455233,\n",
       "        -0.88437539, -0.71495365, -0.90521139, -0.9332444 , -0.8227635 ,\n",
       "        -0.8286873 , -0.69962269, -0.73591442, -0.74007041, -0.92159558,\n",
       "        -0.64758202, -0.69437007, -0.70260635, -0.816586  , -0.76236391,\n",
       "        -0.6777628 , -0.83752028, -0.94665217, -0.6379528 , -0.79977434,\n",
       "        -0.62894584, -0.82228794, -0.68353451, -0.75942356, -0.66678098,\n",
       "        -0.74741557, -0.63720265, -0.87364006, -0.93060009, -0.76432118,\n",
       "        -0.71874276, -0.996314  , -0.98827582, -0.74871143, -0.50315744,\n",
       "        -0.8605159 , -0.75813678, -0.81597924, -0.59987808, -0.77829891,\n",
       "        -0.68383951, -0.88002862, -0.77987049, -0.8278559 , -0.8668703 ,\n",
       "        -0.76536803, -0.91419851, -0.75547339, -0.92360931, -0.65061123,\n",
       "        -0.84894198, -0.76499194, -0.8928366 , -0.81522083, -0.78934758,\n",
       "        -0.80375858, -0.86156295, -0.73096474, -0.65602759, -0.80535082,\n",
       "        -0.88502835, -0.74966787, -0.90821715, -0.86089206, -0.78208764,\n",
       "        -0.86832862, -0.75647137, -0.86491188, -0.57952488, -0.80678372,\n",
       "        -0.73083287, -0.7708781 , -0.865548  , -0.74556602, -0.74060754,\n",
       "        -0.71275241, -0.57638408, -0.62389203, -0.80434784, -0.8820502 ,\n",
       "        -0.74204172, -0.82410438, -0.59201081, -0.78001602, -0.89541068,\n",
       "        -0.93727112, -0.94657776, -0.81096733, -0.92242103, -0.83957629,\n",
       "        -0.8218569 , -0.90433671, -0.78821587, -0.93461964, -0.82804508,\n",
       "        -0.80336606, -0.75791026, -0.70163839, -0.86740549, -0.71666834,\n",
       "        -0.70608506, -0.81954733, -0.69857015, -0.74563533, -0.72091696,\n",
       "        -0.86261454, -0.7253225 , -0.71031561, -0.88144105, -0.85944607,\n",
       "        -0.89133809, -0.87092745, -0.78917116, -0.75809732, -0.65762332,\n",
       "        -0.73500969, -0.79919045, -0.81107666, -0.71836438, -0.9468065 ,\n",
       "        -0.74345501, -0.77577158, -0.62880328, -0.83626111, -0.86713568,\n",
       "        -0.66077034, -0.77197007, -0.78646781, -0.79244597, -0.69580837,\n",
       "        -0.80865412, -0.82867778, -0.84666731, -0.7168939 , -0.84520329,\n",
       "        -0.91102647, -0.88170202, -0.79337396, -0.80982319, -0.61063985,\n",
       "        -0.75749148, -0.89508597, -0.87180297, -0.94766657, -0.75486641,\n",
       "        -0.65723754, -0.79952956, -0.89538073, -0.80870876, -0.74661649,\n",
       "        -0.6609537 , -0.77360617, -0.87979815, -0.79924967, -0.94757078,\n",
       "        -0.87591208, -0.73561367, -0.8010021 , -0.77921853, -0.95619844,\n",
       "        -0.92441565, -0.68427958, -0.72444601, -0.7677875 , -0.87237215,\n",
       "        -0.83397572, -0.68340373, -0.78740774, -0.85988599, -0.84889763,\n",
       "        -0.76390114, -0.75501081, -1.00147366, -0.90032605, -0.73111478,\n",
       "        -0.80584634, -0.60159196, -0.60221902, -0.83527621, -0.97739044,\n",
       "        -0.67750327, -0.79454093, -0.8820678 , -0.72214081, -0.79927095,\n",
       "        -0.97137476, -0.81516871, -0.81166118, -0.7086869 , -0.74802254,\n",
       "        -0.87340273, -0.98253336, -0.63273927, -0.66452113, -0.83074269,\n",
       "        -0.84687238, -0.85370731, -0.70248837, -0.80961892, -0.9767169 ,\n",
       "        -0.77604278, -0.82084546, -0.88532751, -0.76536597, -0.97618822,\n",
       "        -0.76668182, -0.71986506, -0.85947653, -0.94898699, -0.77281456,\n",
       "        -0.92515975, -0.86494389, -0.73638048, -0.95245092, -0.76456979,\n",
       "        -0.78544679, -0.63961073, -0.83891456, -0.87273212, -0.87960549,\n",
       "        -0.86142315, -0.69378881, -0.92316236, -1.00264451, -0.85080998,\n",
       "        -0.64419411, -0.80608113, -0.70841928, -0.71339687, -0.81847325,\n",
       "        -0.8174232 , -0.74924523, -0.6989021 , -0.7767715 , -0.80147271,\n",
       "        -0.77869405, -0.66766648, -0.77330253, -0.91577816, -0.80139674,\n",
       "        -0.87181514, -0.84432159, -0.70877957, -0.87101084, -0.94202661,\n",
       "        -0.70500333, -0.76605525, -0.74983068, -0.77232392, -0.91659907,\n",
       "        -0.6696155 , -0.91799007, -0.86599232, -0.89666892, -0.81708249,\n",
       "        -0.95274914, -0.84065738, -0.69827455, -0.78539353, -0.99515575,\n",
       "        -0.74639893, -0.85577869, -0.80060563, -0.89523092, -0.93785918,\n",
       "        -0.86968533, -0.80335001, -0.74736294, -0.75155888, -0.62633827,\n",
       "        -0.73203448, -0.74425776, -0.90537827, -0.77697386, -0.93007429,\n",
       "        -0.7178435 , -1.02375896, -0.85667352, -0.89609511, -0.85078646,\n",
       "        -0.74215498, -0.74654512, -0.7261571 , -0.69837426, -0.94178086,\n",
       "        -0.62279494, -0.8238761 , -0.82796417, -0.8218542 , -0.8480726 ,\n",
       "        -0.88669921, -0.82543927, -0.93094655, -0.67921947, -0.93719266,\n",
       "        -0.78133221, -0.76279495, -0.92486324, -0.78569199, -0.80888539,\n",
       "        -0.73937629, -1.05154748, -1.01306863, -0.76919677, -0.95828765,\n",
       "        -0.83545224, -0.7652795 , -0.56613116, -0.89101099, -0.87142935,\n",
       "        -0.99576653, -0.91183132, -0.79768065, -0.68669447, -0.78322282,\n",
       "        -0.61802797, -0.76404226, -0.75722991, -0.65759799, -0.88887755,\n",
       "        -0.71955337, -0.78342679, -0.94609911, -0.85610277, -0.80146621,\n",
       "        -0.77463432, -0.7597608 , -0.87970172, -0.65829727, -0.75578019,\n",
       "        -0.93961194, -0.94270853, -0.83699128, -1.06939702, -0.80349351,\n",
       "        -0.81824621, -0.6637471 , -0.82846127, -0.91381457, -0.68293513,\n",
       "        -0.76225421, -0.78743221, -0.78242138, -0.83776952, -0.85635005,\n",
       "        -0.94880374, -0.98740376, -0.77660291, -1.0034304 , -0.75033585,\n",
       "        -0.77593111, -1.00998957, -0.74403751, -0.83708763, -0.98141371,\n",
       "        -0.74895015, -0.80246416, -0.80126956, -0.87437293, -0.81728925,\n",
       "        -0.7511617 , -0.90570893, -0.84717356, -0.7391839 , -0.67058535,\n",
       "        -0.87652773, -0.86364821, -0.91304609, -0.75864672, -0.84787241,\n",
       "        -0.7970968 , -0.69478477, -0.86057714, -0.6864495 , -0.94098176,\n",
       "        -0.83996333, -0.82235885, -0.7828865 , -0.7459834 , -0.70313488,\n",
       "        -0.66467231, -0.75693087, -0.79069968, -0.85712603, -0.89382855,\n",
       "        -0.75078276, -0.68448227, -0.64911608, -0.89686924, -0.77626596,\n",
       "        -0.90397478, -0.68920383, -0.84432344, -0.85874501, -0.75233055,\n",
       "        -0.80787829, -0.68431752, -0.94391497, -0.69206269, -0.73394446,\n",
       "        -0.80151821, -0.69715474, -0.72461837, -0.87518619, -0.54590469,\n",
       "        -0.81512973, -0.74264088, -0.81908678, -0.81989134, -0.74799335,\n",
       "        -0.79185463, -0.87803994, -0.82607509, -0.81761359, -0.75436285,\n",
       "        -0.89032305, -0.71768844, -0.80435042, -0.85863448, -0.7885906 ,\n",
       "        -0.68879935, -0.91007031, -0.88066749, -0.84643661, -0.86322487,\n",
       "        -1.02486341, -0.6366802 , -0.93479788, -0.69827667, -0.59225953,\n",
       "        -0.82096437, -0.92231364, -0.82747102, -0.80559969, -0.64784437,\n",
       "        -0.70953625, -0.74554265, -0.71227336, -0.65103808, -0.75140612,\n",
       "        -0.86725773, -0.79948367, -0.76373219, -0.64764674, -0.78196298,\n",
       "        -0.75133991, -0.87713454, -0.75942637, -0.76446938, -0.79123264,\n",
       "        -0.83153017, -0.68208916, -0.79235431, -0.97173846, -0.78350521,\n",
       "        -0.82201112, -1.11075451, -0.81207129, -0.67319495, -0.75386265,\n",
       "        -0.71128262, -0.83399434, -0.76140928, -0.69685931, -0.85497473,\n",
       "        -0.7078502 , -0.82064575, -0.90461009, -0.83523443, -0.71141763,\n",
       "        -0.73764095, -0.874868  , -0.81067124, -0.69921263, -0.66225073,\n",
       "        -0.67635744, -0.89855291, -0.71211396, -0.793996  , -0.71524752,\n",
       "        -0.64510315, -1.0045205 , -0.74110611, -0.73969437, -0.85655549,\n",
       "        -0.70405806, -0.82321431, -0.84434635, -0.78583502, -0.82604507,\n",
       "        -0.96254604, -0.78450633, -0.6579522 , -0.80265896, -0.79343994,\n",
       "        -0.70834658, -0.62995822, -0.78929879, -0.79903151, -0.66909904,\n",
       "        -0.88459967, -0.93359868, -0.62397108, -0.83345696, -0.75963377,\n",
       "        -0.71168932, -0.86825209, -0.70314279, -0.72594675, -0.66294413,\n",
       "        -0.81772327, -0.76916128, -0.77530308, -0.83885779, -0.85198676,\n",
       "        -0.84321409, -0.7372847 , -0.75384077, -0.67707144, -0.86198082,\n",
       "        -0.79757386, -0.81885863, -0.80213383, -0.68868399, -0.81914936,\n",
       "        -0.84639508, -0.70238115, -0.78723917, -0.98579982, -0.72230806,\n",
       "        -0.89514596, -0.78935718, -0.78154265, -0.74394916, -0.86125748,\n",
       "        -0.92217395, -0.88953407, -0.81313813, -0.76016176, -0.78621232,\n",
       "        -0.85851899, -0.75464527, -0.78346034, -0.78627079, -0.86344058,\n",
       "        -0.74440829, -0.78413901, -0.80249252, -0.84054802, -0.57984686,\n",
       "        -0.8303377 , -0.65579805, -1.01727378, -0.88174628, -0.85393759,\n",
       "        -0.9276974 , -0.62463788, -0.62692419, -0.89703095, -1.01667692,\n",
       "        -0.64912154, -0.69286   , -0.60155631, -0.80024099, -0.75758942,\n",
       "        -0.807009  , -0.65618824, -0.74454883, -0.78037832, -0.95953448,\n",
       "        -0.98967118, -0.89297107, -1.04655126, -0.9123422 , -0.84762533,\n",
       "        -0.90639475, -0.80031452, -0.95463078, -0.89383707, -0.7497999 ,\n",
       "        -0.87651647, -0.88118922, -0.87870416, -0.69917249, -0.65344322,\n",
       "        -0.81031724, -0.7848709 , -0.78384015, -0.91823431, -0.77787681,\n",
       "        -0.70916104, -0.92482959, -0.81019959, -0.94990998, -0.70588477,\n",
       "        -0.76736256, -0.86584138, -0.77931627, -0.68404993, -1.04754665,\n",
       "        -0.61846975, -0.93002735, -0.7219964 , -0.84237439, -0.82488871,\n",
       "        -0.75369657, -0.5765255 , -0.68952327, -0.82296221, -0.71011544,\n",
       "        -0.88092888, -0.73465204, -0.74756926, -0.79127304, -0.75730086,\n",
       "        -0.78548629, -0.89480364, -0.82286015, -0.92282991, -0.66332196,\n",
       "        -0.73046051, -0.76885125, -0.69800631, -0.83000281, -0.97688516,\n",
       "        -0.86260957, -0.73902232, -0.73731771, -0.81588159, -0.74278357,\n",
       "        -0.76621774, -0.91644755, -0.89350156, -0.90521916, -0.89584711,\n",
       "        -0.70982374, -0.69854674, -0.84898738, -0.73419799, -0.82535152,\n",
       "        -0.74910179, -0.89510248, -0.85028606, -0.86265248, -0.81463087,\n",
       "        -0.83677933, -0.88688001, -0.83584668, -0.75531092, -0.82860233,\n",
       "        -0.98797546, -0.76181767, -0.92943269, -0.86075633, -0.76745512,\n",
       "        -0.74815684, -0.78325746, -0.9112257 , -0.78840874, -0.70511555,\n",
       "        -0.58583523, -0.80215613, -0.7149179 , -0.82435347, -0.67949282,\n",
       "        -0.75116394, -0.84914305, -0.65621786, -0.91037486, -0.96988521,\n",
       "        -0.77493602, -0.78085081, -0.75601654, -0.57727488, -0.88260298,\n",
       "        -0.77312038, -1.02905022, -0.78474373, -0.89341706, -0.78392124,\n",
       "        -0.82433154, -0.90315247, -0.93705122, -0.78550597, -0.67039713,\n",
       "        -0.78752847, -0.87537467, -0.82516223, -0.75822013, -0.83787441,\n",
       "        -0.77686203, -0.9206735 , -0.68156746, -0.96557626, -0.83397757,\n",
       "        -1.03586639, -0.816678  , -0.85294339, -0.78866454, -0.63525917,\n",
       "        -0.86797499, -0.80765547, -0.76252541, -0.67909963, -0.73531906,\n",
       "        -0.85650341, -0.99244542, -0.97032136, -0.79831341, -0.73935571,\n",
       "        -0.81036104, -0.79719682, -0.89018782, -0.90872759, -0.79601808,\n",
       "        -0.85014127, -0.71355642, -0.81979598, -0.85555064, -0.85708846,\n",
       "        -0.82914123, -0.63363643, -0.73979176, -0.84612592, -0.75400726,\n",
       "        -0.78903987, -0.82231969, -0.81149603, -0.85824419, -0.91043434,\n",
       "        -0.6996435 , -0.63510253, -0.81590667, -0.79634033, -0.61607427,\n",
       "        -0.57254794, -1.04330963, -0.78001186, -0.84585875, -0.9006792 ,\n",
       "        -0.6530272 , -0.73387735, -0.88703436, -0.79586713, -0.73621568,\n",
       "        -0.71401303, -0.99699938, -0.71965843, -0.65969441, -0.72690254,\n",
       "        -0.7050431 , -0.71740599, -0.91857172, -0.89188314, -0.92717648,\n",
       "        -0.71514861, -0.92779859, -0.77799429, -0.81525208, -0.76184649,\n",
       "        -0.658414  , -0.81432934, -0.79180888, -0.70341586, -0.79655455,\n",
       "        -0.8382121 , -0.72321866, -0.70649933, -0.60090013, -0.91201951,\n",
       "        -0.74823025, -0.82346074, -0.81545021, -0.78227333, -0.77666454,\n",
       "        -0.77623331, -0.71178   , -0.84822267, -0.88096174, -0.85681495,\n",
       "        -0.81669471, -0.65528057, -0.91144341, -0.60766338, -0.66136229,\n",
       "        -0.73164018, -0.95825101, -0.83156971, -0.82258362, -0.79907046,\n",
       "        -0.82032714, -0.73309541, -0.77943569, -0.75730502, -0.85375591,\n",
       "        -0.75265355, -0.84662484, -0.71561681, -0.79753987, -0.73852105,\n",
       "        -0.86180986, -0.76730937, -0.84323546, -0.76027836, -0.75367315,\n",
       "        -1.01530459, -0.79291521, -0.82097178, -0.80438618, -0.81178052,\n",
       "        -0.92598663, -1.00886251, -0.86432773, -0.77258399, -0.85142237,\n",
       "        -0.74554547, -0.78317166, -0.64566823, -1.050319  , -0.72384412,\n",
       "        -0.79810036, -0.76968348, -0.83854767, -0.71516288, -0.93296594,\n",
       "        -0.91027881, -0.66004976, -0.75700384, -0.66467488, -0.7424447 ,\n",
       "        -0.93456142, -0.91524778, -0.74059252, -0.87990139, -0.93428694,\n",
       "        -1.02270666, -0.81090614, -0.8142817 , -0.86356983, -0.88642806,\n",
       "        -0.7354017 , -0.69843492, -0.7641707 , -0.88914236, -0.87735043,\n",
       "        -0.92409377, -0.78181401, -0.96511782, -0.98843553, -0.92327469,\n",
       "        -0.6334749 , -0.66164798, -0.80075896, -0.91740316, -0.78635825,\n",
       "        -0.91824974, -0.81266558, -0.89114731, -0.9215094 , -0.73255447,\n",
       "        -0.92569301, -0.79065911, -1.04802231, -0.84471562, -0.89763064,\n",
       "        -0.85953499, -0.79090727, -0.87806628, -0.76868012, -0.73174949,\n",
       "        -0.86831864, -0.98557075, -0.81915462, -0.85686557, -0.73769431,\n",
       "        -0.76708023, -0.70265643, -0.70937024, -0.73669993, -0.81074106,\n",
       "        -0.6985388 , -0.84039272, -0.77167605, -0.66024558, -0.76410935,\n",
       "        -0.63390477, -0.91779429, -0.76375752, -0.99235094, -0.80454615,\n",
       "        -0.73430142, -0.75172144, -0.74764959, -0.57414466, -0.89378909,\n",
       "        -0.7654961 , -0.81418148, -0.82096077, -0.82529528, -0.95307418,\n",
       "        -0.73054555, -0.73609366, -0.83977969, -0.88159148, -0.76997305,\n",
       "        -0.7579544 , -0.7732522 , -0.84188423, -0.78203842, -0.78375734,\n",
       "        -0.63096263, -0.90712793, -0.91258556, -0.69351211, -0.67711836,\n",
       "        -0.8386846 , -0.71882898, -0.67111076, -0.75606142, -0.91361723,\n",
       "        -0.83106681, -0.88700371, -0.77849958, -0.69731179, -0.7908703 ,\n",
       "        -0.86272824, -0.62439318, -0.68537087, -0.75543221, -0.83937915,\n",
       "        -0.82648871, -0.767037  , -0.69224636, -0.68747483, -0.90080402,\n",
       "        -0.9891225 , -0.85362212, -0.7891099 , -0.63833134, -0.6945231 ,\n",
       "        -0.95026843, -0.79036634, -0.90405294, -0.76258306, -0.95409469,\n",
       "        -0.82307239, -0.86323999, -0.86349158, -0.59774297, -0.7247012 ,\n",
       "        -0.75696545, -0.79294287, -0.88355541, -0.68022982, -0.61272079,\n",
       "        -0.67951918, -0.77939365, -0.95368106, -0.76827785, -1.0088389 ,\n",
       "        -0.64283213, -0.71910025, -0.5719179 , -0.83087581, -0.87511433,\n",
       "        -0.79553107, -0.86638996, -0.85262044, -0.7491595 , -0.80354731,\n",
       "        -0.79853629, -0.70336923, -0.75904125, -0.83333829, -0.80709454,\n",
       "        -0.63375488, -0.81473655, -0.90893314, -0.75731976, -0.83701428]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psislw_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 ms ± 20.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psislw_new(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 ms ± 13.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.psislw(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.2 ms ± 667 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psislw_new(school,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.8 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.psislw(school,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Not much improvement'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"LOO\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000,20)\n",
    "mu = np.random.randn(1000,1000)\n",
    "theta = np.random.randn(1000,1000)\n",
    "sd = np.random.randn(1000,1000)\n",
    "posterior = {\"mu\":mu,\"theta\":theta,\"sd\":sd}\n",
    "sample_stats = {\"log_likelihood\":data}\n",
    "data = from_dict(sample_stats=sample_stats,posterior=posterior)\n",
    "school = load_arviz_data(\"centered_eight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference data with groups:\n",
       "\t> posterior\n",
       "\t> sample_stats"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 5.90912 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: loo at line 309\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   309                                           def loo(data, pointwise=False, reff=None, scale=\"deviance\"):\n",
      "   310                                               \"\"\"Pareto-smoothed importance sampling leave-one-out cross-validation.\n",
      "   311                                           \n",
      "   312                                               Calculates leave-one-out (LOO) cross-validation for out of sample predictive model fit,\n",
      "   313                                               following Vehtari et al. (2017). Cross-validation is computed using Pareto-smoothed\n",
      "   314                                               importance sampling (PSIS).\n",
      "   315                                           \n",
      "   316                                               Parameters\n",
      "   317                                               ----------\n",
      "   318                                               data : result of MCMC run\n",
      "   319                                               pointwise: bool, optional\n",
      "   320                                                   if True the pointwise predictive accuracy will be returned. Defaults to False\n",
      "   321                                               reff : float, optional\n",
      "   322                                                   Relative MCMC efficiency, `ess / n` i.e. number of effective samples divided by\n",
      "   323                                                   the number of actual samples. Computed from trace by default.\n",
      "   324                                               scale : str\n",
      "   325                                                   Output scale for loo. Available options are:\n",
      "   326                                           \n",
      "   327                                                   - `deviance` : (default) -2 * (log-score)\n",
      "   328                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "   329                                                   - `negative_log` : -1 * (log-score)\n",
      "   330                                           \n",
      "   331                                               Returns\n",
      "   332                                               -------\n",
      "   333                                               pandas.Series with the following columns:\n",
      "   334                                               loo: approximated Leave-one-out cross-validation\n",
      "   335                                               loo_se: standard error of loo\n",
      "   336                                               p_loo: effective number of parameters\n",
      "   337                                               shape_warn: 1 if the estimated shape parameter of\n",
      "   338                                                   Pareto distribution is greater than 0.7 for one or more samples\n",
      "   339                                               loo_i: array of pointwise predictive accuracy, only if pointwise True\n",
      "   340                                               pareto_k: array of Pareto shape values, only if pointwise True\n",
      "   341                                               loo_scale: scale of the loo results\n",
      "   342                                               \"\"\"\n",
      "   343         1         24.0     24.0      0.0      inference_data = convert_to_inference_data(data)\n",
      "   344         3         12.0      4.0      0.0      for group in (\"posterior\", \"sample_stats\"):\n",
      "   345         2         11.0      5.5      0.0          if not hasattr(inference_data, group):\n",
      "   346                                                       raise TypeError(\n",
      "   347                                                           \"Must be able to extract a {group} group from data!\".format(group=group)\n",
      "   348                                                       )\n",
      "   349         1         16.0     16.0      0.0      if \"log_likelihood\" not in inference_data.sample_stats:\n",
      "   350                                                   raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
      "   351         1          3.0      3.0      0.0      posterior = inference_data.posterior\n",
      "   352         1       1162.0   1162.0      0.0      log_likelihood = inference_data.sample_stats.log_likelihood\n",
      "   353         1       1513.0   1513.0      0.0      n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
      "   354         1         84.0     84.0      0.0      new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
      "   355         1         44.0     44.0      0.0      log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
      "   356                                           \n",
      "   357         1          8.0      8.0      0.0      if scale.lower() == \"deviance\":\n",
      "   358         1          5.0      5.0      0.0          scale_value = -2\n",
      "   359                                               elif scale.lower() == \"log\":\n",
      "   360                                                   scale_value = 1\n",
      "   361                                               elif scale.lower() == \"negative_log\":\n",
      "   362                                                   scale_value = -1\n",
      "   363                                               else:\n",
      "   364                                                   raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
      "   365                                           \n",
      "   366         1         54.0     54.0      0.0      if reff is None:\n",
      "   367                                                   n_chains = len(posterior.chain)\n",
      "   368                                                   if n_chains == 1:\n",
      "   369                                                       reff = 1.0\n",
      "   370                                                   else:\n",
      "   371                                                       ess_p = ess(posterior, method=\"mean\")\n",
      "   372                                                       # this mean is over all data variables\n",
      "   373                                                       reff = (\n",
      "   374                                                           np.hstack([ess_p[v].values.flatten() for v in ess_p.data_vars]).mean() / n_samples\n",
      "   375                                                       )\n",
      "   376                                           \n",
      "   377         1    5147319.0 5147319.0     87.1      log_weights, pareto_shape = psislw(-log_likelihood, reff)\n",
      "   378         1      93078.0  93078.0      1.6      log_weights += log_likelihood\n",
      "   379                                           \n",
      "   380         1          3.0      3.0      0.0      warn_mg = 0\n",
      "   381         1         65.0     65.0      0.0      if np.any(pareto_shape > 0.7):\n",
      "   382                                                   warnings.warn(\n",
      "   383                                                       \"\"\"Estimated shape parameter of Pareto distribution is greater than 0.7 for\n",
      "   384                                                   one or more samples. You should consider using a more robust model, this is because\n",
      "   385                                                   importance sampling is less likely to work well if the marginal posterior and LOO posterior\n",
      "   386                                                   are very different. This is more likely to happen with a non-robust model and highly\n",
      "   387                                                   influential observations.\"\"\"\n",
      "   388                                                   )\n",
      "   389                                                   warn_mg = 1\n",
      "   390                                           \n",
      "   391         1     285593.0 285593.0      4.8      loo_lppd_i = scale_value * _logsumexp(log_weights, axis=0)\n",
      "   392         1         31.0     31.0      0.0      loo_lppd = loo_lppd_i.sum()\n",
      "   393         1        120.0    120.0      0.0      loo_lppd_se = (len(loo_lppd_i) * np.var(loo_lppd_i)) ** 0.5\n",
      "   394                                           \n",
      "   395         1     378843.0 378843.0      6.4      lppd = np.sum(_logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0]))\n",
      "   396         1          8.0      8.0      0.0      p_loo = lppd - loo_lppd / scale_value\n",
      "   397                                           \n",
      "   398         1          3.0      3.0      0.0      if pointwise:\n",
      "   399         1         31.0     31.0      0.0          if np.equal(loo_lppd, loo_lppd_i).all():  # pylint: disable=no-member\n",
      "   400                                                       warnings.warn(\n",
      "   401                                                           \"\"\"The point-wise LOO is the same with the sum LOO, please double check\n",
      "   402                                                                     the Observed RV in your model to make sure it returns element-wise logp.\n",
      "   403                                                                     \"\"\"\n",
      "   404                                                       )\n",
      "   405         1          4.0      4.0      0.0          return pd.Series(\n",
      "   406         1          3.0      3.0      0.0              data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, loo_lppd_i, pareto_shape, scale],\n",
      "   407         1       1082.0   1082.0      0.0              index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_i\", \"pareto_k\", \"loo_scale\"],\n",
      "   408                                                   )\n",
      "   409                                           \n",
      "   410                                               else:\n",
      "   411                                                   return pd.Series(\n",
      "   412                                                       data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, scale],\n",
      "   413                                                       index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_scale\"],\n",
      "   414                                                   )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(loo)\n",
    "wrapper(data,True,0.8)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_new(data, pointwise=False, reff=None, scale=\"deviance\"):\n",
    "    inference_data = convert_to_inference_data(data)\n",
    "    for group in (\"posterior\", \"sample_stats\"):\n",
    "        if not hasattr(inference_data, group):\n",
    "            raise TypeError(\n",
    "                \"Must be able to extract a {group} group from data!\".format(group=group)\n",
    "            )\n",
    "    if \"log_likelihood\" not in inference_data.sample_stats:\n",
    "        raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
    "    posterior = inference_data.posterior\n",
    "    log_likelihood = inference_data.sample_stats.log_likelihood\n",
    "    n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
    "    new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
    "    log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
    "\n",
    "    if scale.lower() == \"deviance\":\n",
    "        scale_value = -2\n",
    "    elif scale.lower() == \"log\":\n",
    "        scale_value = 1\n",
    "    elif scale.lower() == \"negative_log\":\n",
    "        scale_value = -1\n",
    "    else:\n",
    "        raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
    "\n",
    "    if reff is None:\n",
    "        n_chains = len(posterior.chain)\n",
    "        if n_chains == 1:\n",
    "            reff = 1.0\n",
    "        else:\n",
    "            ess_p = ess(posterior, method=\"mean\")\n",
    "            # this mean is over all data variables\n",
    "            reff = (\n",
    "                np.hstack([ess_p[v].values.flatten() for v in ess_p.data_vars]).mean() / n_samples\n",
    "            )\n",
    "\n",
    "    log_weights, pareto_shape = psislw_new(-log_likelihood, reff)\n",
    "    log_weights += log_likelihood\n",
    "\n",
    "    warn_mg = 0\n",
    "    if np.any(pareto_shape > 0.7):\n",
    "        warnings.warn(\n",
    "            \"\"\"Estimated shape parameter of Pareto distribution is greater than 0.7 for\n",
    "        one or more samples. You should consider using a more robust model, this is because\n",
    "        importance sampling is less likely to work well if the marginal posterior and LOO posterior\n",
    "        are very different. This is more likely to happen with a non-robust model and highly\n",
    "        influential observations.\"\"\"\n",
    "        )\n",
    "        warn_mg = 1\n",
    "\n",
    "    loo_lppd_i = scale_value * _logsumexp(log_weights, axis=0)\n",
    "    loo_lppd = loo_lppd_i.sum()\n",
    "    loo_lppd_se = (len(loo_lppd_i) * _var_1d(loo_lppd_i)) ** 0.5\n",
    "\n",
    "    lppd = np.sum(_logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0]))\n",
    "    p_loo = lppd - loo_lppd / scale_value\n",
    "\n",
    "    if pointwise:\n",
    "        if np.equal(loo_lppd, loo_lppd_i).all():  # pylint: disable=no-member\n",
    "            warnings.warn(\n",
    "                \"\"\"The point-wise LOO is the same with the sum LOO, please double check\n",
    "                          the Observed RV in your model to make sure it returns element-wise logp.\n",
    "                          \"\"\"\n",
    "            )\n",
    "        return pd.Series(\n",
    "            data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, loo_lppd_i, pareto_shape, scale],\n",
    "            index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_i\", \"pareto_k\", \"loo_scale\"],\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            data=[loo_lppd, loo_lppd_se, p_loo, warn_mg, scale],\n",
    "            index=[\"loo\", \"loo_se\", \"p_loo\", \"warning\", \"loo_scale\"],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.08 s ± 96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loo_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/home/banzee/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25 s ± 383 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.loo(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n",
      "26.5 ms ± 448 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loo_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4 ms ± 449 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.loo(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326,)\n",
      "(326,)\n",
      "(326,)\n",
      "(316,)\n",
      "(326,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loo                                                    61.4893\n",
       "loo_se                                                 2.70297\n",
       "p_loo                                                 0.949409\n",
       "warning                                                      0\n",
       "loo_i        [9.77935763951731, 6.840747572952978, 7.726586...\n",
       "pareto_k     [0.4439131521000332, 0.3284878432072214, 0.529...\n",
       "loo_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loo                                                    61.4893\n",
       "loo_se                                                 2.70297\n",
       "p_loo                                                 0.949409\n",
       "warning                                                      0\n",
       "loo_i        [9.77935763951731, 6.840747572952978, 7.726586...\n",
       "pareto_k     [0.4439131521000332, 0.3284878432072214, 0.529...\n",
       "loo_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " az.stats.loo(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def summation(data):\n",
    "    x = 0\n",
    "    for i in range(0,len(data)):\n",
    "        x = x+data[i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.randn(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(summation(c), np.sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.1 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit summation(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.26 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.random.randn(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def logarithm(data):\n",
    "    log_arr = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        log_arr[i] = math.log(data[i])\n",
    "    return log_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(logarithm(data), np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51 ms ± 169 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit logarithm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 µs ± 16.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Not much improvement in performance of loo. Sometimes a bit slower as well'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Compare\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = load_arviz_data(\"centered_eight\")\n",
    "nschool = load_arviz_data(\"non_centered_eight\")\n",
    "compare_dict = {\"1\":school,\"2\":nschool}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>d_waic</th>\n",
       "      <th>weight</th>\n",
       "      <th>se</th>\n",
       "      <th>dse</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.3022</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.72729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.4296</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0.127437</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.68994</td>\n",
       "      <td>0.106882</td>\n",
       "      <td>0</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      waic    p_waic    d_waic weight       se       dse warning waic_scale\n",
       "2  61.3022  0.820067         0    0.5  2.72729         0       0   deviance\n",
       "1  61.4296  0.919548  0.127437    0.5  2.68994  0.106882       0   deviance"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(compare_dict, method='stacking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.088127 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: compare at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def compare(\n",
      "    26                                               dataset_dict,\n",
      "    27                                               ic=\"waic\",\n",
      "    28                                               method=\"BB-pseudo-BMA\",\n",
      "    29                                               b_samples=1000,\n",
      "    30                                               alpha=1,\n",
      "    31                                               seed=None,\n",
      "    32                                               scale=\"deviance\",\n",
      "    33                                           ):\n",
      "    34                                               r\"\"\"Compare models based on WAIC or LOO cross validation.\n",
      "    35                                           \n",
      "    36                                               WAIC is Widely applicable information criterion, and LOO is leave-one-out\n",
      "    37                                               (LOO) cross-validation. Read more theory here - in a paper by some of the\n",
      "    38                                               leading authorities on model selection - dx.doi.org/10.1111/1467-9868.00353\n",
      "    39                                           \n",
      "    40                                               Parameters\n",
      "    41                                               ----------\n",
      "    42                                               dataset_dict : dict[str] -> InferenceData\n",
      "    43                                                   A dictionary of model names and InferenceData objects\n",
      "    44                                               ic : str\n",
      "    45                                                   Information Criterion (WAIC or LOO) used to compare models. Default WAIC.\n",
      "    46                                               method : str\n",
      "    47                                                   Method used to estimate the weights for each model. Available options are:\n",
      "    48                                           \n",
      "    49                                                   - 'stacking' : stacking of predictive distributions.\n",
      "    50                                                   - 'BB-pseudo-BMA' : (default) pseudo-Bayesian Model averaging using Akaike-type\n",
      "    51                                                      weighting. The weights are stabilized using the Bayesian bootstrap\n",
      "    52                                                   - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n",
      "    53                                                      weighting, without Bootstrap stabilization (not recommended)\n",
      "    54                                           \n",
      "    55                                                   For more information read https://arxiv.org/abs/1704.02030\n",
      "    56                                               b_samples: int\n",
      "    57                                                   Number of samples taken by the Bayesian bootstrap estimation.\n",
      "    58                                                   Only useful when method = 'BB-pseudo-BMA'.\n",
      "    59                                               alpha : float\n",
      "    60                                                   The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n",
      "    61                                                   useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n",
      "    62                                                   on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n",
      "    63                                               seed : int or np.random.RandomState instance\n",
      "    64                                                   If int or RandomState, use it for seeding Bayesian bootstrap. Only\n",
      "    65                                                   useful when method = 'BB-pseudo-BMA'. Default None the global\n",
      "    66                                                   np.random state is used.\n",
      "    67                                               scale : str\n",
      "    68                                                   Output scale for IC. Available options are:\n",
      "    69                                           \n",
      "    70                                                   - `deviance` : (default) -2 * (log-score)\n",
      "    71                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "    72                                                   - `negative_log` : -1 * (log-score)\n",
      "    73                                           \n",
      "    74                                               Returns\n",
      "    75                                               -------\n",
      "    76                                               A DataFrame, ordered from lowest to highest IC. The index reflects the order in which the\n",
      "    77                                               models are passed to this function. The columns are:\n",
      "    78                                               IC : Information Criteria (WAIC or LOO).\n",
      "    79                                                   Smaller IC indicates higher out-of-sample predictive fit (\"better\" model). Default WAIC.\n",
      "    80                                                   If `scale == log` higher IC indicates higher out-of-sample predictive fit (\"better\" model).\n",
      "    81                                               pIC : Estimated effective number of parameters.\n",
      "    82                                               dIC : Relative difference between each IC (WAIC or LOO)\n",
      "    83                                               and the lowest IC (WAIC or LOO).\n",
      "    84                                                   It's always 0 for the top-ranked model.\n",
      "    85                                               weight: Relative weight for each model.\n",
      "    86                                                   This can be loosely interpreted as the probability of each model (among the compared model)\n",
      "    87                                                   given the data. By default the uncertainty in the weights estimation is considered using\n",
      "    88                                                   Bayesian bootstrap.\n",
      "    89                                               SE : Standard error of the IC estimate.\n",
      "    90                                                   If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n",
      "    91                                               dSE : Standard error of the difference in IC between each model and\n",
      "    92                                               the top-ranked model.\n",
      "    93                                                   It's always 0 for the top-ranked model.\n",
      "    94                                               warning : A value of 1 indicates that the computation of the IC may not be reliable. This could\n",
      "    95                                                   be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details.\n",
      "    96                                               scale : Scale used for the IC.\n",
      "    97                                               \"\"\"\n",
      "    98         1         19.0     19.0      0.0      names = list(dataset_dict.keys())\n",
      "    99         1         10.0     10.0      0.0      scale = scale.lower()\n",
      "   100         1         14.0     14.0      0.0      if scale == \"log\":\n",
      "   101                                                   scale_value = 1\n",
      "   102                                                   ascending = False\n",
      "   103                                               else:\n",
      "   104         1         13.0     13.0      0.0          if scale == \"negative_log\":\n",
      "   105                                                       scale_value = -1\n",
      "   106                                                   else:\n",
      "   107         1         18.0     18.0      0.0              scale_value = -2\n",
      "   108         1         11.0     11.0      0.0          ascending = True\n",
      "   109                                           \n",
      "   110         1         14.0     14.0      0.0      if ic == \"waic\":\n",
      "   111         1         12.0     12.0      0.0          ic_func = waic\n",
      "   112         1         13.0     13.0      0.0          df_comp = pd.DataFrame(\n",
      "   113         1         11.0     11.0      0.0              index=names,\n",
      "   114         1      21880.0  21880.0     24.8              columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
      "   115                                                   )\n",
      "   116         1         17.0     17.0      0.0          scale_col = \"waic_scale\"\n",
      "   117                                           \n",
      "   118                                               elif ic == \"loo\":\n",
      "   119                                                   ic_func = loo\n",
      "   120                                                   df_comp = pd.DataFrame(\n",
      "   121                                                       index=names,\n",
      "   122                                                       columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
      "   123                                                   )\n",
      "   124                                                   scale_col = \"loo_scale\"\n",
      "   125                                           \n",
      "   126                                               else:\n",
      "   127                                                   raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
      "   128                                           \n",
      "   129         1         12.0     12.0      0.0      if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
      "   130                                                   raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
      "   131                                           \n",
      "   132         1         15.0     15.0      0.0      ic_se = \"{}_se\".format(ic)\n",
      "   133         1         13.0     13.0      0.0      p_ic = \"p_{}\".format(ic)\n",
      "   134         1         14.0     14.0      0.0      ic_i = \"{}_i\".format(ic)\n",
      "   135                                           \n",
      "   136         1       2320.0   2320.0      2.6      ics = pd.DataFrame()\n",
      "   137         1         19.0     19.0      0.0      names = []\n",
      "   138         3         41.0     13.7      0.0      for name, dataset in dataset_dict.items():\n",
      "   139         2         19.0      9.5      0.0          names.append(name)\n",
      "   140         2      36703.0  18351.5     41.6          ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
      "   141         1       1483.0   1483.0      1.7      ics.index = names\n",
      "   142         1       2933.0   2933.0      3.3      ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
      "   143                                           \n",
      "   144         1         15.0     15.0      0.0      if method.lower() == \"stacking\":\n",
      "   145         1       4901.0   4901.0      5.6          rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   146         1         59.0     59.0      0.1          exp_ic_i = np.exp(ic_i_val / scale_value)\n",
      "   147         1         66.0     66.0      0.1          last_col = cols - 1\n",
      "   148                                           \n",
      "   149         1         14.0     14.0      0.0          def w_fuller(weights):\n",
      "   150                                                       return np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
      "   151                                           \n",
      "   152         1         13.0     13.0      0.0          def log_score(weights):\n",
      "   153                                                       w_full = w_fuller(weights)\n",
      "   154                                                       score = 0.0\n",
      "   155                                                       for i in range(rows):\n",
      "   156                                                           score += np.log(np.dot(exp_ic_i[i], w_full))\n",
      "   157                                                       return -score\n",
      "   158                                           \n",
      "   159         1         14.0     14.0      0.0          def gradient(weights):\n",
      "   160                                                       w_full = w_fuller(weights)\n",
      "   161                                                       grad = np.zeros(last_col)\n",
      "   162                                                       for k in range(last_col - 1):\n",
      "   163                                                           for i in range(rows):\n",
      "   164                                                               grad[k] += (exp_ic_i[i, k] - exp_ic_i[i, last_col]) / np.dot(\n",
      "   165                                                                   exp_ic_i[i], w_full\n",
      "   166                                                               )\n",
      "   167                                                       return -grad\n",
      "   168                                           \n",
      "   169         1         54.0     54.0      0.1          theta = np.full(last_col, 1.0 / cols)\n",
      "   170         1         22.0     22.0      0.0          bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
      "   171                                                   constraints = [\n",
      "   172         1         14.0     14.0      0.0              {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
      "   173         1         15.0     15.0      0.0              {\"type\": \"ineq\", \"fun\": np.sum},\n",
      "   174                                                   ]\n",
      "   175                                           \n",
      "   176         1         12.0     12.0      0.0          weights = minimize(\n",
      "   177         1       3503.0   3503.0      4.0              fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
      "   178                                                   )\n",
      "   179                                           \n",
      "   180         1        115.0    115.0      0.1          weights = w_fuller(weights[\"x\"])\n",
      "   181         1        441.0    441.0      0.5          ses = ics[ic_se]\n",
      "   182                                           \n",
      "   183                                               elif method.lower() == \"bb-pseudo-bma\":\n",
      "   184                                                   rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   185                                                   ic_i_val = ic_i_val * rows\n",
      "   186                                           \n",
      "   187                                                   b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
      "   188                                                   weights = np.zeros((b_samples, cols))\n",
      "   189                                                   z_bs = np.zeros_like(weights)\n",
      "   190                                                   for i in range(b_samples):\n",
      "   191                                                       z_b = np.dot(b_weighting[i], ic_i_val)\n",
      "   192                                                       u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
      "   193                                                       z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
      "   194                                                       weights[i] = u_weights / np.sum(u_weights)\n",
      "   195                                           \n",
      "   196                                                   weights = weights.mean(axis=0)\n",
      "   197                                                   ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
      "   198                                           \n",
      "   199                                               elif method.lower() == \"pseudo-bma\":\n",
      "   200                                                   min_ic = ics.iloc[0][ic]\n",
      "   201                                                   z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
      "   202                                                   weights = z_rv / np.sum(z_rv)\n",
      "   203                                                   ses = ics[ic_se]\n",
      "   204                                           \n",
      "   205         1        124.0    124.0      0.1      if np.any(weights):\n",
      "   206         1        293.0    293.0      0.3          min_ic_i_val = ics[ic_i].iloc[0]\n",
      "   207         3        213.0     71.0      0.2          for idx, val in enumerate(ics.index):\n",
      "   208         2       3017.0   1508.5      3.4              res = ics.loc[val]\n",
      "   209         2         20.0     10.0      0.0              if scale_value < 0:\n",
      "   210         2        507.0    253.5      0.6                  diff = res[ic_i] - min_ic_i_val\n",
      "   211                                                       else:\n",
      "   212                                                           diff = min_ic_i_val - res[ic_i]\n",
      "   213         2        330.0    165.0      0.4              d_ic = np.sum(diff)\n",
      "   214         2        992.0    496.0      1.1              d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
      "   215         2        663.0    331.5      0.8              std_err = ses.loc[val]\n",
      "   216         2         20.0     10.0      0.0              weight = weights[idx]\n",
      "   217                                                       df_comp.at[val] = (\n",
      "   218         2        217.0    108.5      0.2                  res[ic],\n",
      "   219         2        157.0     78.5      0.2                  res[p_ic],\n",
      "   220         2         18.0      9.0      0.0                  d_ic,\n",
      "   221         2         15.0      7.5      0.0                  weight,\n",
      "   222         2         14.0      7.0      0.0                  std_err,\n",
      "   223         2         14.0      7.0      0.0                  d_std_err,\n",
      "   224         2        159.0     79.5      0.2                  res[\"warning\"],\n",
      "   225         2       4283.0   2141.5      4.9                  res[scale_col],\n",
      "   226                                                       )\n",
      "   227                                           \n",
      "   228         1       2209.0   2209.0      2.5      return df_comp.sort_values(by=ic, ascending=ascending)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(compare)\n",
    "wrapper(compare_dict,'waic','stacking')\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.246776 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: compare at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def compare(\n",
      "    26                                               dataset_dict,\n",
      "    27                                               ic=\"waic\",\n",
      "    28                                               method=\"BB-pseudo-BMA\",\n",
      "    29                                               b_samples=1000,\n",
      "    30                                               alpha=1,\n",
      "    31                                               seed=None,\n",
      "    32                                               scale=\"deviance\",\n",
      "    33                                           ):\n",
      "    34                                               r\"\"\"Compare models based on WAIC or LOO cross validation.\n",
      "    35                                           \n",
      "    36                                               WAIC is Widely applicable information criterion, and LOO is leave-one-out\n",
      "    37                                               (LOO) cross-validation. Read more theory here - in a paper by some of the\n",
      "    38                                               leading authorities on model selection - dx.doi.org/10.1111/1467-9868.00353\n",
      "    39                                           \n",
      "    40                                               Parameters\n",
      "    41                                               ----------\n",
      "    42                                               dataset_dict : dict[str] -> InferenceData\n",
      "    43                                                   A dictionary of model names and InferenceData objects\n",
      "    44                                               ic : str\n",
      "    45                                                   Information Criterion (WAIC or LOO) used to compare models. Default WAIC.\n",
      "    46                                               method : str\n",
      "    47                                                   Method used to estimate the weights for each model. Available options are:\n",
      "    48                                           \n",
      "    49                                                   - 'stacking' : stacking of predictive distributions.\n",
      "    50                                                   - 'BB-pseudo-BMA' : (default) pseudo-Bayesian Model averaging using Akaike-type\n",
      "    51                                                      weighting. The weights are stabilized using the Bayesian bootstrap\n",
      "    52                                                   - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n",
      "    53                                                      weighting, without Bootstrap stabilization (not recommended)\n",
      "    54                                           \n",
      "    55                                                   For more information read https://arxiv.org/abs/1704.02030\n",
      "    56                                               b_samples: int\n",
      "    57                                                   Number of samples taken by the Bayesian bootstrap estimation.\n",
      "    58                                                   Only useful when method = 'BB-pseudo-BMA'.\n",
      "    59                                               alpha : float\n",
      "    60                                                   The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n",
      "    61                                                   useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n",
      "    62                                                   on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n",
      "    63                                               seed : int or np.random.RandomState instance\n",
      "    64                                                   If int or RandomState, use it for seeding Bayesian bootstrap. Only\n",
      "    65                                                   useful when method = 'BB-pseudo-BMA'. Default None the global\n",
      "    66                                                   np.random state is used.\n",
      "    67                                               scale : str\n",
      "    68                                                   Output scale for IC. Available options are:\n",
      "    69                                           \n",
      "    70                                                   - `deviance` : (default) -2 * (log-score)\n",
      "    71                                                   - `log` : 1 * log-score (after Vehtari et al. (2017))\n",
      "    72                                                   - `negative_log` : -1 * (log-score)\n",
      "    73                                           \n",
      "    74                                               Returns\n",
      "    75                                               -------\n",
      "    76                                               A DataFrame, ordered from lowest to highest IC. The index reflects the order in which the\n",
      "    77                                               models are passed to this function. The columns are:\n",
      "    78                                               IC : Information Criteria (WAIC or LOO).\n",
      "    79                                                   Smaller IC indicates higher out-of-sample predictive fit (\"better\" model). Default WAIC.\n",
      "    80                                                   If `scale == log` higher IC indicates higher out-of-sample predictive fit (\"better\" model).\n",
      "    81                                               pIC : Estimated effective number of parameters.\n",
      "    82                                               dIC : Relative difference between each IC (WAIC or LOO)\n",
      "    83                                               and the lowest IC (WAIC or LOO).\n",
      "    84                                                   It's always 0 for the top-ranked model.\n",
      "    85                                               weight: Relative weight for each model.\n",
      "    86                                                   This can be loosely interpreted as the probability of each model (among the compared model)\n",
      "    87                                                   given the data. By default the uncertainty in the weights estimation is considered using\n",
      "    88                                                   Bayesian bootstrap.\n",
      "    89                                               SE : Standard error of the IC estimate.\n",
      "    90                                                   If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n",
      "    91                                               dSE : Standard error of the difference in IC between each model and\n",
      "    92                                               the top-ranked model.\n",
      "    93                                                   It's always 0 for the top-ranked model.\n",
      "    94                                               warning : A value of 1 indicates that the computation of the IC may not be reliable. This could\n",
      "    95                                                   be indication of WAIC/LOO starting to fail see http://arxiv.org/abs/1507.04544 for details.\n",
      "    96                                               scale : Scale used for the IC.\n",
      "    97                                               \"\"\"\n",
      "    98         1          9.0      9.0      0.0      names = list(dataset_dict.keys())\n",
      "    99         1          6.0      6.0      0.0      scale = scale.lower()\n",
      "   100         1          4.0      4.0      0.0      if scale == \"log\":\n",
      "   101                                                   scale_value = 1\n",
      "   102                                                   ascending = False\n",
      "   103                                               else:\n",
      "   104         1          5.0      5.0      0.0          if scale == \"negative_log\":\n",
      "   105                                                       scale_value = -1\n",
      "   106                                                   else:\n",
      "   107         1          5.0      5.0      0.0              scale_value = -2\n",
      "   108         1          4.0      4.0      0.0          ascending = True\n",
      "   109                                           \n",
      "   110         1          5.0      5.0      0.0      if ic == \"waic\":\n",
      "   111         1          4.0      4.0      0.0          ic_func = waic\n",
      "   112         1          5.0      5.0      0.0          df_comp = pd.DataFrame(\n",
      "   113         1          4.0      4.0      0.0              index=names,\n",
      "   114         1      23367.0  23367.0      9.5              columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
      "   115                                                   )\n",
      "   116         1          7.0      7.0      0.0          scale_col = \"waic_scale\"\n",
      "   117                                           \n",
      "   118                                               elif ic == \"loo\":\n",
      "   119                                                   ic_func = loo\n",
      "   120                                                   df_comp = pd.DataFrame(\n",
      "   121                                                       index=names,\n",
      "   122                                                       columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
      "   123                                                   )\n",
      "   124                                                   scale_col = \"loo_scale\"\n",
      "   125                                           \n",
      "   126                                               else:\n",
      "   127                                                   raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
      "   128                                           \n",
      "   129         1          8.0      8.0      0.0      if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
      "   130                                                   raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
      "   131                                           \n",
      "   132         1          7.0      7.0      0.0      ic_se = \"{}_se\".format(ic)\n",
      "   133         1          6.0      6.0      0.0      p_ic = \"p_{}\".format(ic)\n",
      "   134         1          5.0      5.0      0.0      ic_i = \"{}_i\".format(ic)\n",
      "   135                                           \n",
      "   136         1      10880.0  10880.0      4.4      ics = pd.DataFrame()\n",
      "   137         1         19.0     19.0      0.0      names = []\n",
      "   138         3         41.0     13.7      0.0      for name, dataset in dataset_dict.items():\n",
      "   139         2         30.0     15.0      0.0          names.append(name)\n",
      "   140         2      40536.0  20268.0     16.4          ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
      "   141         1       1558.0   1558.0      0.6      ics.index = names\n",
      "   142         1       3423.0   3423.0      1.4      ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
      "   143                                           \n",
      "   144         1         54.0     54.0      0.0      if method.lower() == \"stacking\":\n",
      "   145                                                   rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   146                                                   exp_ic_i = np.exp(ic_i_val / scale_value)\n",
      "   147                                                   last_col = cols - 1\n",
      "   148                                           \n",
      "   149                                                   def w_fuller(weights):\n",
      "   150                                                       return np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
      "   151                                           \n",
      "   152                                                   def log_score(weights):\n",
      "   153                                                       w_full = w_fuller(weights)\n",
      "   154                                                       score = 0.0\n",
      "   155                                                       for i in range(rows):\n",
      "   156                                                           score += np.log(np.dot(exp_ic_i[i], w_full))\n",
      "   157                                                       return -score\n",
      "   158                                           \n",
      "   159                                                   def gradient(weights):\n",
      "   160                                                       w_full = w_fuller(weights)\n",
      "   161                                                       grad = np.zeros(last_col)\n",
      "   162                                                       for k in range(last_col - 1):\n",
      "   163                                                           for i in range(rows):\n",
      "   164                                                               grad[k] += (exp_ic_i[i, k] - exp_ic_i[i, last_col]) / np.dot(\n",
      "   165                                                                   exp_ic_i[i], w_full\n",
      "   166                                                               )\n",
      "   167                                                       return -grad\n",
      "   168                                           \n",
      "   169                                                   theta = np.full(last_col, 1.0 / cols)\n",
      "   170                                                   bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
      "   171                                                   constraints = [\n",
      "   172                                                       {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
      "   173                                                       {\"type\": \"ineq\", \"fun\": np.sum},\n",
      "   174                                                   ]\n",
      "   175                                           \n",
      "   176                                                   weights = minimize(\n",
      "   177                                                       fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
      "   178                                                   )\n",
      "   179                                           \n",
      "   180                                                   weights = w_fuller(weights[\"x\"])\n",
      "   181                                                   ses = ics[ic_se]\n",
      "   182                                           \n",
      "   183         1         15.0     15.0      0.0      elif method.lower() == \"bb-pseudo-bma\":\n",
      "   184         1       3529.0   3529.0      1.4          rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
      "   185         1         33.0     33.0      0.0          ic_i_val = ic_i_val * rows\n",
      "   186                                           \n",
      "   187         1       1273.0   1273.0      0.5          b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
      "   188         1         23.0     23.0      0.0          weights = np.zeros((b_samples, cols))\n",
      "   189         1         53.0     53.0      0.0          z_bs = np.zeros_like(weights)\n",
      "   190      1001       4738.0      4.7      1.9          for i in range(b_samples):\n",
      "   191      1000      98651.0     98.7     40.0              z_b = np.dot(b_weighting[i], ic_i_val)\n",
      "   192      1000      24211.0     24.2      9.8              u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
      "   193      1000       6224.0      6.2      2.5              z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
      "   194      1000      21669.0     21.7      8.8              weights[i] = u_weights / np.sum(u_weights)\n",
      "   195                                           \n",
      "   196         1         92.0     92.0      0.0          weights = weights.mean(axis=0)\n",
      "   197         1        999.0    999.0      0.4          ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
      "   198                                           \n",
      "   199                                               elif method.lower() == \"pseudo-bma\":\n",
      "   200                                                   min_ic = ics.iloc[0][ic]\n",
      "   201                                                   z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
      "   202                                                   weights = z_rv / np.sum(z_rv)\n",
      "   203                                                   ses = ics[ic_se]\n",
      "   204                                           \n",
      "   205         1         31.0     31.0      0.0      if np.any(weights):\n",
      "   206         1        118.0    118.0      0.0          min_ic_i_val = ics[ic_i].iloc[0]\n",
      "   207         3         79.0     26.3      0.0          for idx, val in enumerate(ics.index):\n",
      "   208         2       1317.0    658.5      0.5              res = ics.loc[val]\n",
      "   209         2         10.0      5.0      0.0              if scale_value < 0:\n",
      "   210         2        160.0     80.0      0.1                  diff = res[ic_i] - min_ic_i_val\n",
      "   211                                                       else:\n",
      "   212                                                           diff = min_ic_i_val - res[ic_i]\n",
      "   213         2         53.0     26.5      0.0              d_ic = np.sum(diff)\n",
      "   214         2        194.0     97.0      0.1              d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
      "   215         2        354.0    177.0      0.1              std_err = ses.loc[val]\n",
      "   216         2         11.0      5.5      0.0              weight = weights[idx]\n",
      "   217                                                       df_comp.at[val] = (\n",
      "   218         2         96.0     48.0      0.0                  res[ic],\n",
      "   219         2         85.0     42.5      0.0                  res[p_ic],\n",
      "   220         2         10.0      5.0      0.0                  d_ic,\n",
      "   221         2          8.0      4.0      0.0                  weight,\n",
      "   222         2          9.0      4.5      0.0                  std_err,\n",
      "   223         2         10.0      5.0      0.0                  d_std_err,\n",
      "   224         2         77.0     38.5      0.0                  res[\"warning\"],\n",
      "   225         2       1715.0    857.5      0.7                  res[scale_col],\n",
      "   226                                                       )\n",
      "   227                                           \n",
      "   228         1        937.0    937.0      0.4      return df_comp.sort_values(by=ic, ascending=ascending)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(compare)\n",
    "wrapper(compare_dict,'waic','bb-pseudo-bma')\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_new(\n",
    "    dataset_dict,\n",
    "    ic=\"waic\",\n",
    "    method=\"BB-pseudo-BMA\",\n",
    "    b_samples=1000,\n",
    "    alpha=1,\n",
    "    seed=None,\n",
    "    scale=\"deviance\",\n",
    "):\n",
    "    names = list(dataset_dict.keys())\n",
    "    scale = scale.lower()\n",
    "    if scale == \"log\":\n",
    "        scale_value = 1\n",
    "        ascending = False\n",
    "    else:\n",
    "        if scale == \"negative_log\":\n",
    "            scale_value = -1\n",
    "        else:\n",
    "            scale_value = -2\n",
    "        ascending = True\n",
    "\n",
    "    if ic == \"waic\":\n",
    "        ic_func = waic_new\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
    "        )\n",
    "        scale_col = \"waic_scale\"\n",
    "\n",
    "    elif ic == \"loo\":\n",
    "        ic_func = loo\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
    "        )\n",
    "        scale_col = \"loo_scale\"\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
    "\n",
    "    if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
    "        raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
    "\n",
    "    ic_se = \"{}_se\".format(ic)\n",
    "    p_ic = \"p_{}\".format(ic)\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
    "\n",
    "    if method.lower() == \"stacking\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        exp_ic_i = np.exp(ic_i_val / scale_value)\n",
    "        last_col = cols - 1\n",
    "        \n",
    "        def w_fuller(weights):\n",
    "            np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
    "\n",
    "        def log_score(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            score = 0.0\n",
    "            for i in range(rows):\n",
    "                k =  np.log(np.dot(exp_ic_i[i], w_full))\n",
    "                score = score+k\n",
    "            return -score\n",
    "\n",
    "        def gradient(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            grad = np.zeros(last_col)\n",
    "            for k in range(last_col - 1):\n",
    "                for i in range(rows):\n",
    "                    N = exp_ic_i[i, k] - exp_ic_i[i, last_col]\n",
    "                    D = np.dot(exp_ic_i[i], w_full)\n",
    "                    grad[k] += N/D\n",
    "            return -grad\n",
    "\n",
    "        theta = np.full(last_col, 1.0 / cols)\n",
    "        bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
    "        constraints = [\n",
    "            {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
    "            {\"type\": \"ineq\", \"fun\": np.sum},\n",
    "        ]\n",
    "\n",
    "        weights = minimize(\n",
    "            fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
    "        )\n",
    "\n",
    "        weights = w_fuller(weights[\"x\"])\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    elif method.lower() == \"bb-pseudo-bma\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        ic_i_val = ic_i_val * rows\n",
    "\n",
    "        b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
    "        @numba.jit\n",
    "        def weight_function(ic_i_val,b_weighting,scale_value,b_samples):\n",
    "            weights = np.zeros((b_samples, cols))\n",
    "            z_bs = np.zeros_like(weights)\n",
    "            for i in range(b_samples):\n",
    "                z_b = np.dot(b_weighting[i], ic_i_val)\n",
    "                u_weights = np.exp((z_b-np.min(z_b))/scale_value)\n",
    "                z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
    "                weights[i] = u_weights / np.sum(u_weights)\n",
    "            return weights,z_bs\n",
    "            \n",
    "        weights, z_bs = weight_function(ic_i_val, b_weighting,scale_value, b_samples)\n",
    "        weights = weights.mean(axis=0)\n",
    "        ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
    "\n",
    "    elif method.lower() == \"pseudo-bma\":\n",
    "        min_ic = ics.iloc[0][ic]\n",
    "        z_rv = np.exp((ics[ic] - min_ic) / scale_value)\n",
    "        weights = z_rv / np.sum(z_rv)\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    if np.any(weights):\n",
    "        min_ic_i_val = ics[ic_i].iloc[0]\n",
    "        for idx, val in enumerate(ics.index):\n",
    "            res = ics.loc[val]\n",
    "            if scale_value < 0:\n",
    "                diff = res[ic_i] - min_ic_i_val\n",
    "            else:\n",
    "                diff = min_ic_i_val - res[ic_i]\n",
    "            d_ic = np.sum(diff)\n",
    "            d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
    "            std_err = ses.loc[val]\n",
    "            weight = weights[idx]\n",
    "            df_comp.at[val] = (\n",
    "                res[ic],\n",
    "                res[p_ic],\n",
    "                d_ic,\n",
    "                weight,\n",
    "                std_err,\n",
    "                d_std_err,\n",
    "                res[\"warning\"],\n",
    "                res[scale_col],\n",
    "            )\n",
    "\n",
    "    return df_comp.sort_values(by=ic, ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the effect of loop jitting in bb-pseduo-mma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435 ms ± 12.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compare_new(compare_dict,'waic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.4 ms ± 582 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.compare(compare_dict,'waic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sheesh, it's 10 times slower.It was expected as nopython failed to compile.\n",
    "#Let's try to vectorize np.exp((z_b-np.min(z_b))/scale_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_new(\n",
    "    dataset_dict,\n",
    "    ic=\"waic\",\n",
    "    method=\"BB-pseudo-BMA\",\n",
    "    b_samples=1000,\n",
    "    alpha=1,\n",
    "    seed=None,\n",
    "    scale=\"deviance\",\n",
    "):\n",
    "    names = list(dataset_dict.keys())\n",
    "    scale = scale.lower()\n",
    "    if scale == \"log\":\n",
    "        scale_value = 1\n",
    "        ascending = False\n",
    "    else:\n",
    "        if scale == \"negative_log\":\n",
    "            scale_value = -1\n",
    "        else:\n",
    "            scale_value = -2\n",
    "        ascending = True\n",
    "\n",
    "    if ic == \"waic\":\n",
    "        ic_func = waic_new\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"waic\", \"p_waic\", \"d_waic\", \"weight\", \"se\", \"dse\", \"warning\", \"waic_scale\"],\n",
    "        )\n",
    "        scale_col = \"waic_scale\"\n",
    "\n",
    "    elif ic == \"loo\":\n",
    "        ic_func = loo\n",
    "        df_comp = pd.DataFrame(\n",
    "            index=names,\n",
    "            columns=[\"loo\", \"p_loo\", \"d_loo\", \"weight\", \"se\", \"dse\", \"warning\", \"loo_scale\"],\n",
    "        )\n",
    "        scale_col = \"loo_scale\"\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"The information criterion {} is not supported.\".format(ic))\n",
    "\n",
    "    if method.lower() not in [\"stacking\", \"bb-pseudo-bma\", \"pseudo-bma\"]:\n",
    "        raise ValueError(\"The method {}, to compute weights, is not supported.\".format(method))\n",
    "\n",
    "    ic_se = \"{}_se\".format(ic)\n",
    "    p_ic = \"p_{}\".format(ic)\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([ic_func(dataset, pointwise=True, scale=scale)])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=ascending)\n",
    "\n",
    "    if method.lower() == \"stacking\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        exp_ic_i = np.exp(ic_i_val / scale_value)\n",
    "        last_col = cols - 1\n",
    "        \n",
    "        def w_fuller(weights):\n",
    "            np.concatenate((weights, [max(1.0 - np.sum(weights), 0.0)]))\n",
    "\n",
    "        def log_score(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            score = 0.0\n",
    "            for i in range(rows):\n",
    "                k =  np.log(np.dot(exp_ic_i[i], w_full))\n",
    "                score = score+k\n",
    "            return -score\n",
    "\n",
    "        def gradient(weights):\n",
    "            w_full = w_fuller(weights)\n",
    "            grad = np.zeros(last_col)\n",
    "            for k in range(last_col - 1):\n",
    "                for i in range(rows):\n",
    "                    N = exp_ic_i[i, k] - exp_ic_i[i, last_col]\n",
    "                    D = np.dot(exp_ic_i[i], w_full)\n",
    "                    grad[k] += N/D\n",
    "            return -grad\n",
    "\n",
    "        theta = np.full(last_col, 1.0 / cols)\n",
    "        bounds = [(0.0, 1.0) for _ in range(last_col)]\n",
    "        constraints = [\n",
    "            {\"type\": \"ineq\", \"fun\": lambda x: 1.0 - np.sum(x)},\n",
    "            {\"type\": \"ineq\", \"fun\": np.sum},\n",
    "        ]\n",
    "\n",
    "        weights = minimize(\n",
    "            fun=log_score, x0=theta, jac=gradient, bounds=bounds, constraints=constraints\n",
    "        )\n",
    "\n",
    "        weights = w_fuller(weights[\"x\"])\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    elif method.lower() == \"bb-pseudo-bma\":\n",
    "        rows, cols, ic_i_val = _ic_matrix(ics, ic_i)\n",
    "        ic_i_val = ic_i_val * rows\n",
    "        \n",
    "        @numba.vectorize(nopython=True)\n",
    "        def z_b_weights(x,min_x,scale_value):\n",
    "            return np.exp((x-min_x)/scale_value)\n",
    "\n",
    "        b_weighting = st.dirichlet.rvs(alpha=[alpha] * rows, size=b_samples, random_state=seed)\n",
    "        weights = np.zeros((b_samples, cols))\n",
    "        z_bs = np.zeros_like(weights)\n",
    "        for i in range(b_samples):\n",
    "            z_b = np.dot(b_weighting[i], ic_i_val)\n",
    "            u_weights = z_b_weights(z_b, np.min(z_b), scale_value)\n",
    "            z_bs[i] = z_b  # pylint: disable=unsupported-assignment-operation\n",
    "            weights[i] = u_weights / np.sum(u_weights)\n",
    "            \n",
    "        weights = weights.mean(axis=0)\n",
    "        ses = pd.Series(z_bs.std(axis=0), index=names)  # pylint: disable=no-member\n",
    "\n",
    "    elif method.lower() == \"pseudo-bma\":\n",
    "        \n",
    "        @numba.vectorize(nopython=True)\n",
    "        def z_b_weights(x,min_x,scale_value):\n",
    "            return np.exp((x-min_x)/scale_value)\n",
    "        \n",
    "        min_ic = ics.iloc[0][ic]\n",
    "        z_rv = z_b_weights(ics[ic].values,min_ic,scale_value)\n",
    "        \n",
    "        weights = z_rv / np.sum(z_rv)\n",
    "        ses = ics[ic_se]\n",
    "\n",
    "    if np.any(weights):\n",
    "        min_ic_i_val = ics[ic_i].iloc[0]\n",
    "        for idx, val in enumerate(ics.index):\n",
    "            res = ics.loc[val]\n",
    "            if scale_value < 0:\n",
    "                diff = res[ic_i] - min_ic_i_val\n",
    "            else:\n",
    "                diff = min_ic_i_val - res[ic_i]\n",
    "            d_ic = np.sum(diff)\n",
    "            d_std_err = np.sqrt(len(diff) * np.var(diff))\n",
    "            std_err = ses.loc[val]\n",
    "            weight = weights[idx]\n",
    "            df_comp.at[val] = (\n",
    "                res[ic],\n",
    "                res[p_ic],\n",
    "                d_ic,\n",
    "                weight,\n",
    "                std_err,\n",
    "                d_std_err,\n",
    "                res[\"warning\"],\n",
    "                res[scale_col],\n",
    "            )\n",
    "\n",
    "    return df_comp.sort_values(by=ic, ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compare_new(compare_dict,'waic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.9 ms ± 1.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.compare(compare_dict,'waic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's 3 times slower. I don't think that much could be done to improve bb-pseudo-bma or pseduo-bma for that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 ms ± 4.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compare_new(compare_dict,'waic',method='pseudo-bma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 936 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.compare(compare_dict,'waic',method='pseudo-bma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 times slower. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I guess the best method is to use _waic_new and loo_new which provide a better speedup. jitting the different methods \n",
    "# under stacking slows down the overall method substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Summary\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary mainly involves ufuncs which is a  part of diagnostics. I'll pick that up when I take up diagnostics\n",
    "#in a couple of days. Open for suggestions and improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
