{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "from line_profiler import LineProfiler\n",
    "from arviz.data import load_arviz_data,from_dict,convert_to_inference_data\n",
    "from arviz.stats.stats import r2_score, hpd, _gpdfit, _gpinv,_ic_matrix, waic,psislw\n",
    "from arviz.stats.stats_utils import logsumexp as _logsumexp\n",
    "import scipy.stats as st\n",
    "import numba\n",
    "import math\n",
    "from numpy import sin, cos\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(10_000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'********************************************hpd********************************************************************'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''********************************************hpd********************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.36279 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 252\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   252                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   253                                               \"\"\"\n",
      "   254                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   255                                           \n",
      "   256                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   257                                               for unimodal distributions.\n",
      "   258                                           \n",
      "   259                                               Parameters\n",
      "   260                                               ----------\n",
      "   261                                               x : Numpy array\n",
      "   262                                                   An array containing posterior samples\n",
      "   263                                               credible_interval : float, optional\n",
      "   264                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   265                                               circular : bool, optional\n",
      "   266                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   267                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   268                                           \n",
      "   269                                               Returns\n",
      "   270                                               -------\n",
      "   271                                               np.ndarray\n",
      "   272                                                   lower and upper value of the interval.\n",
      "   273                                               \"\"\"\n",
      "   274      1001       1988.0      2.0      0.1      if ary.ndim > 1:\n",
      "   275         1          4.0      4.0      0.0          hpd_array = np.array(\n",
      "   276         1         13.0     13.0      0.0              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   277                                                   )\n",
      "   278         1          2.0      2.0      0.0          return hpd_array\n",
      "   279                                               # Make a copy of trace\n",
      "   280      1000     111598.0    111.6      4.7      ary = ary.copy()\n",
      "   281      1000       2119.0      2.1      0.1      n = len(ary)\n",
      "   282                                           \n",
      "   283      1000       1258.0      1.3      0.1      if circular:\n",
      "   284      1000     558298.0    558.3     23.6          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   285      1000      15540.0     15.5      0.7          ary = ary - mean\n",
      "   286      1000     701649.0    701.6     29.7          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   287                                           \n",
      "   288      1000     897646.0    897.6     38.0      ary = np.sort(ary)\n",
      "   289      1000       8817.0      8.8      0.4      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   290      1000       1520.0      1.5      0.1      n_intervals = n - interval_idx_inc\n",
      "   291      1000       8192.0      8.2      0.3      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   292                                           \n",
      "   293      1000       2096.0      2.1      0.1      if len(interval_width) == 0:\n",
      "   294                                                   raise ValueError(\n",
      "   295                                                       \"Too few elements for interval calculation. \"\n",
      "   296                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   297                                                   )\n",
      "   298                                           \n",
      "   299      1000      11767.0     11.8      0.5      min_idx = np.argmin(interval_width)\n",
      "   300      1000       2385.0      2.4      0.1      hdi_min = ary[min_idx]\n",
      "   301      1000       2842.0      2.8      0.1      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   302                                           \n",
      "   303      1000       1295.0      1.3      0.1      if circular:\n",
      "   304      1000       1563.0      1.6      0.1          hdi_min = hdi_min + mean\n",
      "   305      1000       1318.0      1.3      0.1          hdi_max = hdi_max + mean\n",
      "   306      1000      14513.0     14.5      0.6          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   307      1000       9265.0      9.3      0.4          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   308                                           \n",
      "   309      1000       7106.0      7.1      0.3      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_1, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.591736 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 252\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   252                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   253                                               \"\"\"\n",
      "   254                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   255                                           \n",
      "   256                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   257                                               for unimodal distributions.\n",
      "   258                                           \n",
      "   259                                               Parameters\n",
      "   260                                               ----------\n",
      "   261                                               x : Numpy array\n",
      "   262                                                   An array containing posterior samples\n",
      "   263                                               credible_interval : float, optional\n",
      "   264                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   265                                               circular : bool, optional\n",
      "   266                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   267                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   268                                           \n",
      "   269                                               Returns\n",
      "   270                                               -------\n",
      "   271                                               np.ndarray\n",
      "   272                                                   lower and upper value of the interval.\n",
      "   273                                               \"\"\"\n",
      "   274         1          7.0      7.0      0.0      if ary.ndim > 1:\n",
      "   275                                                   hpd_array = np.array(\n",
      "   276                                                       [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   277                                                   )\n",
      "   278                                                   return hpd_array\n",
      "   279                                               # Make a copy of trace\n",
      "   280         1     324942.0 324942.0     54.9      ary = ary.copy()\n",
      "   281         1          6.0      6.0      0.0      n = len(ary)\n",
      "   282                                           \n",
      "   283         1          2.0      2.0      0.0      if circular:\n",
      "   284         1      52718.0  52718.0      8.9          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   285         1       3433.0   3433.0      0.6          ary = ary - mean\n",
      "   286         1      73944.0  73944.0     12.5          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   287                                           \n",
      "   288         1     136196.0 136196.0     23.0      ary = np.sort(ary)\n",
      "   289         1         27.0     27.0      0.0      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   290         1          2.0      2.0      0.0      n_intervals = n - interval_idx_inc\n",
      "   291         1        202.0    202.0      0.0      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   292                                           \n",
      "   293         1          4.0      4.0      0.0      if len(interval_width) == 0:\n",
      "   294                                                   raise ValueError(\n",
      "   295                                                       \"Too few elements for interval calculation. \"\n",
      "   296                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   297                                                   )\n",
      "   298                                           \n",
      "   299         1        193.0    193.0      0.0      min_idx = np.argmin(interval_width)\n",
      "   300         1          4.0      4.0      0.0      hdi_min = ary[min_idx]\n",
      "   301         1          5.0      5.0      0.0      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   302                                           \n",
      "   303         1          2.0      2.0      0.0      if circular:\n",
      "   304         1          2.0      2.0      0.0          hdi_min = hdi_min + mean\n",
      "   305         1          1.0      1.0      0.0          hdi_max = hdi_max + mean\n",
      "   306         1         25.0     25.0      0.0          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   307         1         11.0     11.0      0.0          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   308                                           \n",
      "   309         1         10.0     10.0      0.0      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(data_2, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.120716 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: hpd at line 252\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   252                                           def hpd(ary, credible_interval=0.94, circular=False):\n",
      "   253                                               \"\"\"\n",
      "   254                                               Calculate highest posterior density (HPD) of array for given credible_interval.\n",
      "   255                                           \n",
      "   256                                               The HPD is the minimum width Bayesian credible interval (BCI). This implementation works only\n",
      "   257                                               for unimodal distributions.\n",
      "   258                                           \n",
      "   259                                               Parameters\n",
      "   260                                               ----------\n",
      "   261                                               x : Numpy array\n",
      "   262                                                   An array containing posterior samples\n",
      "   263                                               credible_interval : float, optional\n",
      "   264                                                   Credible interval to compute. Defaults to 0.94.\n",
      "   265                                               circular : bool, optional\n",
      "   266                                                   Whether to compute the hpd taking into account `x` is a circular variable\n",
      "   267                                                   (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n",
      "   268                                           \n",
      "   269                                               Returns\n",
      "   270                                               -------\n",
      "   271                                               np.ndarray\n",
      "   272                                                   lower and upper value of the interval.\n",
      "   273                                               \"\"\"\n",
      "   274       501       1271.0      2.5      1.1      if ary.ndim > 1:\n",
      "   275         1          4.0      4.0      0.0          hpd_array = np.array(\n",
      "   276         1         11.0     11.0      0.0              [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
      "   277                                                   )\n",
      "   278         1          4.0      4.0      0.0          return hpd_array\n",
      "   279                                               # Make a copy of trace\n",
      "   280       500       4368.0      8.7      3.6      ary = ary.copy()\n",
      "   281       500       1448.0      2.9      1.2      n = len(ary)\n",
      "   282                                           \n",
      "   283       500       1001.0      2.0      0.8      if circular:\n",
      "   284       500      33018.0     66.0     27.4          mean = st.circmean(ary, high=np.pi, low=-np.pi)\n",
      "   285       500       3560.0      7.1      2.9          ary = ary - mean\n",
      "   286       500       4450.0      8.9      3.7          ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
      "   287                                           \n",
      "   288       500      27646.0     55.3     22.9      ary = np.sort(ary)\n",
      "   289       500       4874.0      9.7      4.0      interval_idx_inc = int(np.floor(credible_interval * n))\n",
      "   290       500        991.0      2.0      0.8      n_intervals = n - interval_idx_inc\n",
      "   291       500       3171.0      6.3      2.6      interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
      "   292                                           \n",
      "   293       500       1213.0      2.4      1.0      if len(interval_width) == 0:\n",
      "   294                                                   raise ValueError(\n",
      "   295                                                       \"Too few elements for interval calculation. \"\n",
      "   296                                                       \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
      "   297                                                   )\n",
      "   298                                           \n",
      "   299       500       7751.0     15.5      6.4      min_idx = np.argmin(interval_width)\n",
      "   300       500       1753.0      3.5      1.5      hdi_min = ary[min_idx]\n",
      "   301       500       1837.0      3.7      1.5      hdi_max = ary[min_idx + interval_idx_inc]\n",
      "   302                                           \n",
      "   303       500        961.0      1.9      0.8      if circular:\n",
      "   304       500       1081.0      2.2      0.9          hdi_min = hdi_min + mean\n",
      "   305       500        949.0      1.9      0.8          hdi_max = hdi_max + mean\n",
      "   306       500       8501.0     17.0      7.0          hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
      "   307       500       6708.0     13.4      5.6          hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
      "   308                                           \n",
      "   309       500       4145.0      8.3      3.4      return np.array([hdi_min, hdi_max])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(hpd)\n",
    "wrapper(school, 0.94, True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBottlenecks:\\n    1)scipy.stats.circmean\\n    2)numpy arctan2\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bottlenecks:\n",
    "    1)scipy.stats.circmean\n",
    "    2)numpy arctan2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _circ_mean(ary, high, low):\n",
    "    ary = np.asarray(ary)\n",
    "    if ary.size==0:\n",
    "        return np.nan, np.nan\n",
    "    pi = np.pi\n",
    "    angles = (ary-low)*2*pi/(high-low)\n",
    "    S = sinusoidal(angles)\n",
    "    C = cosine(angles)\n",
    "    res = np.arctan2(S,C)\n",
    "    mask = res < 0\n",
    "    if mask.ndim > 0:\n",
    "        res[mask] += 2*pi\n",
    "    elif mask:\n",
    "        res += 2*pi\n",
    "    return res*(high - low)/2.0/pi + low\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def sinusoidal(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.sin(x[i])\n",
    "    return summ\n",
    "@numba.jit(nopython=True)\n",
    "def cosine(x):\n",
    "    summ = 0\n",
    "    x = x.flatten()\n",
    "    for i in range(0,len(x)):\n",
    "        summ = summ+math.cos(x[i])\n",
    "    return summ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.5 ms ± 181 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67 ms ± 102 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.8 ms ± 296 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sinusoidal(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58 ms ± 89.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372 ms ± 2.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.3 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.3 ms ± 900 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cosine(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.59 ms ± 81.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.cos(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_1, np.pi, -np.pi), st.circmean(data_1, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(data_2, np.pi, -np.pi), st.circmean(data_2, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_circ_mean(school, np.pi, -np.pi), st.circmean(school, np.pi, -np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 s ± 7.31 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 ms ± 4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_1, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.9 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.1 ms ± 364 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(data_2, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 µs ± 342 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _circ_mean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 µs ± 3.38 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit st.circmean(school, np.pi, -np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Performance of _circ_mean is a worse on larger datasets.\\n   On schools, the performance is much better.\\n   ¯\\\\_(ツ)_/¯\\n   Let's see the overall perfomance of hpd\\n\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance of _circ_mean is a worse on larger datasets.\n",
    "   On schools, the performance is much better.\n",
    "   ¯\\_(ツ)_/¯\n",
    "   Let's see the overall perfomance of hpd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpd_new(ary, credible_interval=0.94, circular=False):\n",
    "    if ary.ndim > 1:\n",
    "        hpd_array = np.array(\n",
    "            [hpd(row, credible_interval=credible_interval, circular=circular) for row in ary.T]\n",
    "        )\n",
    "        return hpd_array\n",
    "    # Make a copy of trace\n",
    "    ary = ary.copy()\n",
    "    n = len(ary)\n",
    "\n",
    "    if circular:\n",
    "        mean = _circ_mean(ary, high=np.pi, low=-np.pi)\n",
    "        ary = ary - mean\n",
    "        ary = np.arctan2(np.sin(ary), np.cos(ary))\n",
    "\n",
    "    ary = np.sort(ary)\n",
    "    interval_idx_inc = int(np.floor(credible_interval * n))\n",
    "    n_intervals = n - interval_idx_inc\n",
    "    interval_width = ary[interval_idx_inc:] - ary[:n_intervals]\n",
    "\n",
    "    if len(interval_width) == 0:\n",
    "        raise ValueError(\n",
    "            \"Too few elements for interval calculation. \"\n",
    "            \"Check that credible_interval meets condition 0 =< credible_interval < 1\"\n",
    "        )\n",
    "\n",
    "    min_idx = np.argmin(interval_width)\n",
    "    hdi_min = ary[min_idx]\n",
    "    hdi_max = ary[min_idx + interval_idx_inc]\n",
    "\n",
    "    if circular:\n",
    "        hdi_min = hdi_min + mean\n",
    "        hdi_max = hdi_max + mean\n",
    "        hdi_min = np.arctan2(np.sin(hdi_min), np.cos(hdi_min))\n",
    "        hdi_max = np.arctan2(np.sin(hdi_max), np.cos(hdi_max))\n",
    "\n",
    "    return np.array([hdi_min, hdi_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(school, 0.95,True), az.stats.hpd(school, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_1, 0.95,True), az.stats.hpd(data_1, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hpd_new(data_2, 0.95,True), az.stats.hpd(data_2, 0.95,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.1 ms ± 672 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.7 ms ± 418 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(school, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 s ± 13.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 s ± 6.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_1, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hpd_new(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 ms ± 2.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.hpd(data_2, 0.95,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As seen above, the performance of numba is better with school dataset.\\nThe performance with large datasets is a bit unstable.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''As seen above, the performance of numba is better with school dataset.\n",
    "The performance with large datasets is a bit unstable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'******************************************R2_SCORE***************************************************************'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''******************************************R2_SCORE***************************************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.random.randn(1000,1000)\n",
    "data_2 = np.random.randn(1_000_000)\n",
    "data_3 = np.random.randn(1000,1000)\n",
    "data_4 = np.random.randn(1_000_000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values\n",
    "n_school = load_arviz_data(\"non_centered_eight\").posterior[\"mu\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.198642 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 568\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   568                                           def r2_score(y_true, y_pred):\n",
      "   569                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   570                                           \n",
      "   571                                               Parameters\n",
      "   572                                               ----------\n",
      "   573                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   574                                                   Ground truth (correct) target values.\n",
      "   575                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   576                                                   Estimated target values.\n",
      "   577                                           \n",
      "   578                                               Returns\n",
      "   579                                               -------\n",
      "   580                                               Pandas Series with the following indices:\n",
      "   581                                               r2: Bayesian R²\n",
      "   582                                               r2_std: standard deviation of the Bayesian R².\n",
      "   583                                               \"\"\"\n",
      "   584         1          4.0      4.0      0.0      if y_pred.ndim == 1:\n",
      "   585                                                   var_y_est = np.var(y_pred)\n",
      "   586                                                   var_e = np.var(y_true - y_pred)\n",
      "   587                                               else:\n",
      "   588         1       1891.0   1891.0      1.0          var_y_est = np.var(y_pred.mean(0))\n",
      "   589         1     105350.0 105350.0     53.0          var_e = np.var(y_true - y_pred, 0)\n",
      "   590                                           \n",
      "   591         1         35.0     35.0      0.0      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   592                                           \n",
      "   593         1      91362.0  91362.0     46.0      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_1, data_3)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.024834 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: r2_score at line 568\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   568                                           def r2_score(y_true, y_pred):\n",
      "   569                                               \"\"\"R² for Bayesian regression models. Only valid for linear models.\n",
      "   570                                           \n",
      "   571                                               Parameters\n",
      "   572                                               ----------\n",
      "   573                                               y_true: : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   574                                                   Ground truth (correct) target values.\n",
      "   575                                               y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "   576                                                   Estimated target values.\n",
      "   577                                           \n",
      "   578                                               Returns\n",
      "   579                                               -------\n",
      "   580                                               Pandas Series with the following indices:\n",
      "   581                                               r2: Bayesian R²\n",
      "   582                                               r2_std: standard deviation of the Bayesian R².\n",
      "   583                                               \"\"\"\n",
      "   584         1          6.0      6.0      0.0      if y_pred.ndim == 1:\n",
      "   585         1      11631.0  11631.0     46.8          var_y_est = np.var(y_pred)\n",
      "   586         1      11826.0  11826.0     47.6          var_e = np.var(y_true - y_pred)\n",
      "   587                                               else:\n",
      "   588                                                   var_y_est = np.var(y_pred.mean(0))\n",
      "   589                                                   var_e = np.var(y_true - y_pred, 0)\n",
      "   590                                           \n",
      "   591         1          4.0      4.0      0.0      r_squared = var_y_est / (var_y_est + var_e)\n",
      "   592                                           \n",
      "   593         1       1367.0   1367.0      5.5      return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(r2_score)\n",
    "wrapper(data_2, data_4)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def _var_1d(data):\n",
    "    a,b = 0,0\n",
    "    for i in data:\n",
    "        a = a+i\n",
    "        b = b+i*i\n",
    "    return b/len(data)-((a/len(data))**2)\n",
    "\n",
    "@numba.jit\n",
    "def _var_2d(data):\n",
    "    a,b = data.shape\n",
    "    var = np.zeros(b)\n",
    "    for i in range(0,b):\n",
    "        var[i] = _var_1d(data[:,i])\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979012160953333"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_var_1d(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997901216095329"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2), np.var(data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_4), np.var(data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 ms ± 55.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 ms ± 62.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_1d(data_2-data_4),np.var(data_2-data_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.6 µs ± 4.07 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.8 µs ± 626 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(school,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 ms ± 39 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.58 ms ± 79.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.02 ms ± 52.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _var_2d(data_3-data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 ms ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.var(data_3-data_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(school), np.var(school,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1), np.var(data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3), np.var(data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_3-data_1), np.var(data_3-data_1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(_var_2d(data_1-data_3), np.var(data_1-data_3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is doing wonders with variance. Lets inspect mean'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is doing wonders with variance. Lets inspect mean'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def mean_1d(data):\n",
    "    summ = 0\n",
    "    for i in data:\n",
    "        summ = summ+i\n",
    "    return summ/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001399785322285794"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_1d(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013997853222859263"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.63 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mean_1d(data_2-data_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.89 ms ± 56.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.mean(data_2-data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numba is not suitable here. Lets see the overall r2_score performance'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Numba is not suitable here. Lets see the overall r2_score performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_new(y_true, y_pred):\n",
    "    if y_pred.ndim == 1:\n",
    "        var_y_est = _var_1d(y_pred)\n",
    "        var_e = _var_1d(y_true - y_pred)\n",
    "    else:\n",
    "        var_y_est = _var_1d(y_pred.mean(0))\n",
    "        var_e = _var_2d(y_true - y_pred)\n",
    "\n",
    "    r_squared = var_y_est / (var_y_est + var_e)\n",
    "\n",
    "    return pd.Series([np.mean(r_squared), np.std(r_squared)], index=[\"r2\", \"r2_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.03 ms ± 44.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.9 ms ± 443 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_2, data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.98 ms ± 67.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 1.68 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 µs ± 5.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit r2_score_new(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 µs ± 6.15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = r2_score_new(school, n_school)\n",
    "df_2 = az.stats.r2_score(school, n_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.219553\n",
       "r2_std    0.168670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        False\n",
       "r2_std    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1==df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000502\n",
       "r2_std    0.000023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_new(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r2        0.000502\n",
       "r2_std    0.000023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.r2_score(data_1, data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have not included the pandas test yet...will include them soon'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''I have not included the pandas test yet...will include them soon'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It can be clearly seen that numba greatly improves the performance of r2_score.\\nTBH, this was the function which I thought had the least scope of improvement\\nThank you mentors.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''It can be clearly seen that numba greatly improves the performance of r2_score.\n",
    "TBH, this was the function which I thought had the least scope of improvement\n",
    "Thank you mentors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpdfit'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpdfit\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.sort(np.random.randn(100000)))\n",
    "school = np.abs(np.sort((load_arviz_data(\"centered_eight\").posterior[\"mu\"].values)[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.479742 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 491\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   491                                           def _gpdfit(ary):\n",
      "   492                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   493                                           \n",
      "   494                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   495                                               distribution given the data.\n",
      "   496                                           \n",
      "   497                                               Parameters\n",
      "   498                                               ----------\n",
      "   499                                               ary : array\n",
      "   500                                                   sorted 1D data array\n",
      "   501                                           \n",
      "   502                                               Returns\n",
      "   503                                               -------\n",
      "   504                                               k : float\n",
      "   505                                                   estimated shape parameter\n",
      "   506                                               sigma : float\n",
      "   507                                                   estimated scale parameter\n",
      "   508                                               \"\"\"\n",
      "   509         1          3.0      3.0      0.0      prior_bs = 3\n",
      "   510         1          2.0      2.0      0.0      prior_k = 10\n",
      "   511         1          3.0      3.0      0.0      n = len(ary)\n",
      "   512         1          8.0      8.0      0.0      m_est = 30 + int(n ** 0.5)\n",
      "   513                                           \n",
      "   514         1         92.0     92.0      0.0      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   515         1         34.0     34.0      0.0      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   516         1         15.0     15.0      0.0      b_ary += 1 / ary[-1]\n",
      "   517                                           \n",
      "   518         1     436170.0 436170.0     90.9      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   519         1         72.0     72.0      0.0      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   520         1      42637.0  42637.0      8.9      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   521                                           \n",
      "   522                                               # remove negligible weights\n",
      "   523         1         57.0     57.0      0.0      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   524         1         30.0     30.0      0.0      if not np.all(real_idxs):\n",
      "   525         1          6.0      6.0      0.0          weights = weights[real_idxs]\n",
      "   526         1          3.0      3.0      0.0          b_ary = b_ary[real_idxs]\n",
      "   527                                               # normalise weights\n",
      "   528         1         17.0     17.0      0.0      weights /= weights.sum()\n",
      "   529                                           \n",
      "   530                                               # posterior mean for b\n",
      "   531         1         31.0     31.0      0.0      b_post = np.sum(b_ary * weights)\n",
      "   532                                               # estimate for k\n",
      "   533         1        555.0    555.0      0.1      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   534                                               # add prior for k_post\n",
      "   535         1          4.0      4.0      0.0      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   536         1          1.0      1.0      0.0      sigma = -k_post / b_post\n",
      "   537                                           \n",
      "   538         1          2.0      2.0      0.0      return k_post, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(data)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.001702 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpdfit at line 491\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   491                                           def _gpdfit(ary):\n",
      "   492                                               \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n",
      "   493                                           \n",
      "   494                                               Empirical Bayes estimate for the parameters of the generalized Pareto\n",
      "   495                                               distribution given the data.\n",
      "   496                                           \n",
      "   497                                               Parameters\n",
      "   498                                               ----------\n",
      "   499                                               ary : array\n",
      "   500                                                   sorted 1D data array\n",
      "   501                                           \n",
      "   502                                               Returns\n",
      "   503                                               -------\n",
      "   504                                               k : float\n",
      "   505                                                   estimated shape parameter\n",
      "   506                                               sigma : float\n",
      "   507                                                   estimated scale parameter\n",
      "   508                                               \"\"\"\n",
      "   509         1          4.0      4.0      0.2      prior_bs = 3\n",
      "   510         1          3.0      3.0      0.2      prior_k = 10\n",
      "   511         1          4.0      4.0      0.2      n = len(ary)\n",
      "   512         1         10.0     10.0      0.6      m_est = 30 + int(n ** 0.5)\n",
      "   513                                           \n",
      "   514         1        103.0    103.0      6.1      b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
      "   515         1         39.0     39.0      2.3      b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
      "   516         1         17.0     17.0      1.0      b_ary += 1 / ary[-1]\n",
      "   517                                           \n",
      "   518         1        977.0    977.0     57.4      k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
      "   519         1         48.0     48.0      2.8      len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
      "   520         1        206.0    206.0     12.1      weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
      "   521                                           \n",
      "   522                                               # remove negligible weights\n",
      "   523         1         41.0     41.0      2.4      real_idxs = weights >= 10 * np.finfo(float).eps\n",
      "   524         1         37.0     37.0      2.2      if not np.all(real_idxs):\n",
      "   525         1         11.0     11.0      0.6          weights = weights[real_idxs]\n",
      "   526         1          7.0      7.0      0.4          b_ary = b_ary[real_idxs]\n",
      "   527                                               # normalise weights\n",
      "   528         1         30.0     30.0      1.8      weights /= weights.sum()\n",
      "   529                                           \n",
      "   530                                               # posterior mean for b\n",
      "   531         1         38.0     38.0      2.2      b_post = np.sum(b_ary * weights)\n",
      "   532                                               # estimate for k\n",
      "   533         1        113.0    113.0      6.6      k_post = np.log1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
      "   534                                               # add prior for k_post\n",
      "   535         1          8.0      8.0      0.5      k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
      "   536         1          3.0      3.0      0.2      sigma = -k_post / b_post\n",
      "   537                                           \n",
      "   538         1          3.0      3.0      0.2      return k_post, sigma\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpdfit)\n",
    "wrapper(school)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bottleneck at 518'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bottleneck at 518\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def log_1p(data):\n",
    "    log_p = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        log_p[i] = math.log1p(data[i])\n",
    "    return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(data), np.log1p(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(log_1p(school), np.log1p(school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 ms ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 µs ± 5.56 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 µs ± 27.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit log_1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 µs ± 31.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.log1p(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\\nwhile in 10% of trials the performance comes down a notch w.r.t to numpy'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Performance is somewhat unstable, almost half the time it speeds up the process, sometimes its performance is\n",
    "while in 10% of trials the performance comes down a notch w.r.t to numpy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overall performance of gpdfit'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''overall performance of gpdfit'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpdfit_new(ary):\n",
    "    prior_bs = 3\n",
    "    prior_k = 10\n",
    "    n = len(ary)\n",
    "    m_est = 30 + int(n ** 0.5)\n",
    "\n",
    "    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n",
    "    b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
    "    b_ary += 1 / ary[-1]\n",
    "\n",
    "    k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1) # pylint: disable=no-member Using my own log_1p here throws an exception\n",
    "    len_scale = n * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n",
    "    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n",
    "\n",
    "    # remove negligible weights\n",
    "    real_idxs = weights >= 10 * np.finfo(float).eps\n",
    "    if not np.all(real_idxs):\n",
    "        weights = weights[real_idxs]\n",
    "        b_ary = b_ary[real_idxs]\n",
    "    # normalise weights\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b_post = np.sum(b_ary * weights)\n",
    "    # estimate for k\n",
    "    k_post = log_1p(-b_post * ary).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n",
    "    # add prior for k_post\n",
    "    k_post = (n * k_post + prior_k * 0.5) / (n + prior_k)\n",
    "    sigma = -k_post / b_post\n",
    "    return k_post, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 ms ± 5.33 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 ms ± 4.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 µs ± 7.52 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpdfit_new(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 µs ± 5.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpdfit(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similar performance on both the datasets. Up for discussion with mentors'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Similar performance on both the datasets. Up for discussion with mentors'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_gpvinv'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"_gpvinv\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(100000)\n",
    "school = load_arviz_data(\"centered_eight\").posterior[\"mu\"].values[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.004106 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 541\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   541                                           def _gpinv(probs, kappa, sigma):\n",
      "   542                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   543                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   544         1        908.0    908.0     22.1      x = np.full_like(probs, np.nan)\n",
      "   545         1          5.0      5.0      0.1      if sigma <= 0:\n",
      "   546                                                   return x\n",
      "   547         1        509.0    509.0     12.4      ok = (probs > 0) & (probs < 1)\n",
      "   548         1         44.0     44.0      1.1      if np.all(ok):\n",
      "   549                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   550                                                       x = -np.log1p(-probs)\n",
      "   551                                                   else:\n",
      "   552                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   553                                                   x *= sigma\n",
      "   554                                               else:\n",
      "   555         1         42.0     42.0      1.0          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   556                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   557                                                   else:\n",
      "   558         1       2233.0   2233.0     54.4              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   559         1        101.0    101.0      2.5          x *= sigma\n",
      "   560         1        160.0    160.0      3.9          x[probs == 0] = 0\n",
      "   561         1          4.0      4.0      0.1          if kappa >= 0:\n",
      "   562         1         99.0     99.0      2.4              x[probs == 1] = np.inf\n",
      "   563                                                   else:\n",
      "   564                                                       x[probs == 1] = -sigma / kappa\n",
      "   565         1          1.0      1.0      0.0      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(data, 5,2)\n",
    "lp.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000195 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _gpinv at line 541\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   541                                           def _gpinv(probs, kappa, sigma):\n",
      "   542                                               \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
      "   543                                               # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
      "   544         1         32.0     32.0     16.4      x = np.full_like(probs, np.nan)\n",
      "   545         1          2.0      2.0      1.0      if sigma <= 0:\n",
      "   546                                                   return x\n",
      "   547         1         27.0     27.0     13.8      ok = (probs > 0) & (probs < 1)\n",
      "   548         1         30.0     30.0     15.4      if np.all(ok):\n",
      "   549                                                   if np.abs(kappa) < np.finfo(float).eps:\n",
      "   550                                                       x = -np.log1p(-probs)\n",
      "   551                                                   else:\n",
      "   552                                                       x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
      "   553                                                   x *= sigma\n",
      "   554                                               else:\n",
      "   555         1         27.0     27.0     13.8          if np.abs(kappa) < np.finfo(float).eps:\n",
      "   556                                                       x[ok] = -np.log1p(-probs[ok])\n",
      "   557                                                   else:\n",
      "   558         1         54.0     54.0     27.7              x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
      "   559         1         10.0     10.0      5.1          x *= sigma\n",
      "   560         1          7.0      7.0      3.6          x[probs == 0] = 0\n",
      "   561         1          1.0      1.0      0.5          if kappa >= 0:\n",
      "   562         1          5.0      5.0      2.6              x[probs == 1] = np.inf\n",
      "   563                                                   else:\n",
      "   564                                                       x[probs == 1] = -sigma / kappa\n",
      "   565         1          0.0      0.0      0.0      return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_gpinv)\n",
    "wrapper(school, 5,2)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Log1p or expm is the bottleneck again'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Log1p or expm is the bottleneck again'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def expm(data):\n",
    "    expm = np.zeros_like(data)\n",
    "    for i in range(0,len(data)):\n",
    "        expm[i] = math.expm1(data[i])\n",
    "    return expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(expm(data), np.expm1(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.63 ms ± 23.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 µs ± 4.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 µs ± 18.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit expm(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3 µs ± 117 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.expm1(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numpy is a bit faster\\nLets see the overall performance'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numpy is a bit faster\n",
    "Lets see the overall performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gpinv_new(probs, kappa, sigma):\n",
    "    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n",
    "    # pylint: disable=unsupported-assignment-operation, invalid-unary-operand-type\n",
    "    x = np.full_like(probs, np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (probs > 0) & (probs < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x = -log_1p(-probs)\n",
    "        else:\n",
    "            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(kappa) < np.finfo(float).eps:\n",
    "            x[ok] = -log_1p(-probs[ok])\n",
    "        else:\n",
    "            x[ok] = np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n",
    "        x *= sigma\n",
    "        x[probs == 0] = 0\n",
    "        if kappa >= 0:\n",
    "            x[probs == 1] = np.inf\n",
    "        else:\n",
    "            x[probs == 1] = -sigma / kappa\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _gpinv_new(data,-5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan, ..., 0.39939703,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan, ..., 0.39939703,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ...,  0., nan, nan])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.72 ms ± 77 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpinv_new(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7 ms ± 37.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(data, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5 µs ± 409 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _gpinv_new(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.8 µs ± 382 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._gpinv(school, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numba _gpinv is similar in performance'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numba _gpinv is similar in performance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataset_dict,ic='waic'):\n",
    "    ic_i = \"{}_i\".format(ic)\n",
    "\n",
    "    ics = pd.DataFrame()\n",
    "    names = []\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        names.append(name)\n",
    "        ics = ics.append([waic(dataset, pointwise=True, scale=\"deviance\")])\n",
    "    ics.index = names\n",
    "    ics.sort_values(by=ic, inplace=True, ascending=True)\n",
    "    return ics, ic_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics, ic_i = generator({\"1\":load_arviz_data(\"centered_eight\"),\"2\":load_arviz_data(\"non_centered_eight\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waic</th>\n",
       "      <th>waic_se</th>\n",
       "      <th>p_waic</th>\n",
       "      <th>warning</th>\n",
       "      <th>waic_i</th>\n",
       "      <th>waic_scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.302151</td>\n",
       "      <td>2.727291</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.732429123207623, 6.813708378182436, 7.70616...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.429587</td>\n",
       "      <td>2.689941</td>\n",
       "      <td>0.919548</td>\n",
       "      <td>0</td>\n",
       "      <td>[9.76127241619229, 6.832600316056821, 7.725554...</td>\n",
       "      <td>deviance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        waic   waic_se    p_waic  warning  \\\n",
       "2  61.302151  2.727291  0.820067        0   \n",
       "1  61.429587  2.689941  0.919548        0   \n",
       "\n",
       "                                              waic_i waic_scale  \n",
       "2  [9.732429123207623, 6.813708378182436, 7.70616...   deviance  \n",
       "1  [9.76127241619229, 6.832600316056821, 7.725554...   deviance  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.002461 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: _ic_matrix at line 235\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   235                                           def _ic_matrix(ics, ic_i):\n",
      "   236                                               \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
      "   237         1         21.0     21.0      0.9      cols, _ = ics.shape\n",
      "   238         1        440.0    440.0     17.9      rows = len(ics[ic_i].iloc[0])\n",
      "   239         1          9.0      9.0      0.4      ic_i_val = np.zeros((rows, cols))\n",
      "   240                                           \n",
      "   241         3        102.0     34.0      4.1      for idx, val in enumerate(ics.index):\n",
      "   242         2       1853.0    926.5     75.3          ic = ics.loc[val][ic_i]\n",
      "   243                                           \n",
      "   244         2          7.0      3.5      0.3          if len(ic) != rows:\n",
      "   245                                                       raise ValueError(\"The number of observations should be the same across all models\")\n",
      "   246                                           \n",
      "   247         2         27.0     13.5      1.1          ic_i_val[:, idx] = ic\n",
      "   248                                           \n",
      "   249         1          2.0      2.0      0.1      return rows, cols, ic_i_val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(_ic_matrix)\n",
    "wrapper(ics,ic_i)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def loop_lifter(ics,rows,cols):\n",
    "    ic_i_val = np.zeros((rows, cols))\n",
    "    for idx, val in enumerate(ics.index):\n",
    "        ic = ics.loc[val][ic_i]\n",
    "\n",
    "        if len(ic) != rows:\n",
    "            raise ValueError(\"The number of observations should be the same across all models\")\n",
    "\n",
    "        ic_i_val[:, idx] = ic\n",
    "    return ic_i_val\n",
    "\n",
    "def _ic_matrix_new(ics, ic_i):\n",
    "    \"\"\"Store the previously computed pointwise predictive accuracy values (ics) in a 2D matrix.\"\"\"\n",
    "    cols, _ = ics.shape\n",
    "    rows = len(ics[ic_i].iloc[0])\n",
    "    return rows, cols, loop_lifter(ics,rows,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854 µs ± 151 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ic_matrix_new(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 µs ± 5.58 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.stats._ic_matrix(ics, ic_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\\n   Original _ic_matrix is better'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loop lifitng is not useful in this case. Numba fails at no nopython mode and falls back to pyobject mode.\n",
    "   Original _ic_matrix is better'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000,1000,20)\n",
    "sample_stats = {\"log_likelihood\":data}\n",
    "data = from_dict(sample_stats=sample_stats)\n",
    "school = load_arviz_data(\"centered_eight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.645326 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: waic at line 868\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   868                                           def waic(data, pointwise=False, scale=\"deviance\"):\n",
      "   869                                               \"\"\"Calculate the widely available information criterion.\n",
      "   870                                           \n",
      "   871                                               Also calculates the WAIC's standard error and the effective number of\n",
      "   872                                               parameters of the samples in trace from model. Read more theory here - in\n",
      "   873                                               a paper by some of the leading authorities on model selection\n",
      "   874                                               dx.doi.org/10.1111/1467-9868.00353\n",
      "   875                                           \n",
      "   876                                               Parameters\n",
      "   877                                               ----------\n",
      "   878                                               data : obj\n",
      "   879                                                   Any object that can be converted to an az.InferenceData object\n",
      "   880                                                   Refer to documentation of az.convert_to_dataset for details\n",
      "   881                                               pointwise: bool\n",
      "   882                                                   if True the pointwise predictive accuracy will be returned.\n",
      "   883                                                   Default False\n",
      "   884                                               scale : str\n",
      "   885                                                   Output scale for loo. Available options are:\n",
      "   886                                           \n",
      "   887                                                   - `deviance` : (default) -2 * (log-score)\n",
      "   888                                                   - `log` : 1 * log-score\n",
      "   889                                                   - `negative_log` : -1 * (log-score)\n",
      "   890                                           \n",
      "   891                                               Returns\n",
      "   892                                               -------\n",
      "   893                                               DataFrame with the following columns:\n",
      "   894                                               waic: widely available information criterion\n",
      "   895                                               waic_se: standard error of waic\n",
      "   896                                               p_waic: effective number parameters\n",
      "   897                                               var_warn: 1 if posterior variance of the log predictive\n",
      "   898                                                    densities exceeds 0.4\n",
      "   899                                               waic_i: and array of the pointwise predictive accuracy, only if pointwise True\n",
      "   900                                               waic_scale: scale of the waic results\n",
      "   901                                               \"\"\"\n",
      "   902         1         48.0     48.0      0.0      inference_data = convert_to_inference_data(data)\n",
      "   903         2         10.0      5.0      0.0      for group in (\"sample_stats\",):\n",
      "   904         1          7.0      7.0      0.0          if not hasattr(inference_data, group):\n",
      "   905                                                       raise TypeError(\n",
      "   906                                                           \"Must be able to extract a {group} group from data!\".format(group=group)\n",
      "   907                                                       )\n",
      "   908         1         18.0     18.0      0.0      if \"log_likelihood\" not in inference_data.sample_stats:\n",
      "   909                                                   raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
      "   910         1       1197.0   1197.0      0.2      log_likelihood = inference_data.sample_stats.log_likelihood\n",
      "   911                                           \n",
      "   912         1          8.0      8.0      0.0      if scale.lower() == \"deviance\":\n",
      "   913         1          5.0      5.0      0.0          scale_value = -2\n",
      "   914                                               elif scale.lower() == \"log\":\n",
      "   915                                                   scale_value = 1\n",
      "   916                                               elif scale.lower() == \"negative_log\":\n",
      "   917                                                   scale_value = -1\n",
      "   918                                               else:\n",
      "   919                                                   raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
      "   920                                           \n",
      "   921         1       1497.0   1497.0      0.2      n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
      "   922         1         60.0     60.0      0.0      new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
      "   923         1         45.0     45.0      0.0      log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
      "   924                                           \n",
      "   925         1     379063.0 379063.0     58.7      lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
      "   926                                           \n",
      "   927         1     261565.0 261565.0     40.5      vars_lpd = np.var(log_likelihood, axis=0)\n",
      "   928         1          3.0      3.0      0.0      warn_mg = 0\n",
      "   929         1         85.0     85.0      0.0      if np.any(vars_lpd > 0.4):\n",
      "   930         1          3.0      3.0      0.0          warnings.warn(\n",
      "   931                                                       \"\"\"For one or more samples the posterior variance of the log predictive\n",
      "   932                                                   densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "   933                                                   http://arxiv.org/abs/1507.04544 for details\n",
      "   934         1        245.0    245.0      0.0          \"\"\"\n",
      "   935                                                   )\n",
      "   936         1          3.0      3.0      0.0          warn_mg = 1\n",
      "   937                                           \n",
      "   938         1         18.0     18.0      0.0      waic_i = scale_value * (lppd_i - vars_lpd)\n",
      "   939         1        105.0    105.0      0.0      waic_se = (len(waic_i) * np.var(waic_i)) ** 0.5\n",
      "   940         1         16.0     16.0      0.0      waic_sum = np.sum(waic_i)\n",
      "   941         1         11.0     11.0      0.0      p_waic = np.sum(vars_lpd)\n",
      "   942                                           \n",
      "   943         1          1.0      1.0      0.0      if pointwise:\n",
      "   944         1         18.0     18.0      0.0          if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
      "   945                                                       warnings.warn(\n",
      "   946                                                           \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
      "   947                                                       the Observed RV in your model to make sure it returns element-wise logp.\n",
      "   948                                                       \"\"\"\n",
      "   949                                                       )\n",
      "   950         1          3.0      3.0      0.0          return pd.Series(\n",
      "   951         1          2.0      2.0      0.0              data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
      "   952         1       1290.0   1290.0      0.2              index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
      "   953                                                   )\n",
      "   954                                               else:\n",
      "   955                                                   return pd.Series(\n",
      "   956                                                       data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
      "   957                                                       index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
      "   958                                                   )\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:934: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(waic)\n",
    "wrapper(data,True)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_new(data, pointwise=False, scale=\"deviance\"):\n",
    "    inference_data = convert_to_inference_data(data)\n",
    "    for group in (\"sample_stats\",):\n",
    "        if not hasattr(inference_data, group):\n",
    "            raise TypeError(\n",
    "                \"Must be able to extract a {group} group from data!\".format(group=group)\n",
    "            )\n",
    "    if \"log_likelihood\" not in inference_data.sample_stats:\n",
    "        raise TypeError(\"Data must include log_likelihood in sample_stats\")\n",
    "    log_likelihood = inference_data.sample_stats.log_likelihood\n",
    "\n",
    "    if scale.lower() == \"deviance\":\n",
    "        scale_value = -2\n",
    "    elif scale.lower() == \"log\":\n",
    "        scale_value = 1\n",
    "    elif scale.lower() == \"negative_log\":\n",
    "        scale_value = -1\n",
    "    else:\n",
    "        raise TypeError('Valid scale values are \"deviance\", \"log\", \"negative_log\"')\n",
    "\n",
    "    n_samples = log_likelihood.chain.size * log_likelihood.draw.size\n",
    "    new_shape = (n_samples, np.product(log_likelihood.shape[2:]))\n",
    "    log_likelihood = log_likelihood.values.reshape(*new_shape)\n",
    "\n",
    "    lppd_i = _logsumexp(log_likelihood, axis=0, b_inv=log_likelihood.shape[0])\n",
    "\n",
    "    vars_lpd = _var_2d(log_likelihood)\n",
    "    warn_mg = 0\n",
    "    if np.any(vars_lpd > 0.4):\n",
    "        warnings.warn(\n",
    "            \"\"\"For one or more samples the posterior variance of the log predictive\n",
    "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
    "        http://arxiv.org/abs/1507.04544 for details\n",
    "        \"\"\"\n",
    "        )\n",
    "        warn_mg = 1\n",
    "\n",
    "    waic_i = scale_value * (lppd_i - vars_lpd)\n",
    "    waic_se = (len(waic_i) * _var_1d(waic_i)) ** 0.5\n",
    "    waic_sum = np.sum(waic_i)\n",
    "    p_waic = np.sum(vars_lpd)\n",
    "\n",
    "    if pointwise:\n",
    "        if np.equal(waic_sum, waic_i).all():  # pylint: disable=no-member\n",
    "            warnings.warn(\n",
    "                \"\"\"The point-wise WAIC is the same with the sum WAIC, please double check\n",
    "            the Observed RV in your model to make sure it returns element-wise logp.\n",
    "            \"\"\"\n",
    "            )\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, waic_i, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_i\", \"waic_scale\"],\n",
    "        )\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            data=[waic_sum, waic_se, p_waic, warn_mg, scale],\n",
    "            index=[\"waic\", \"waic_se\", \"p_waic\", \"warning\", \"waic_scale\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    19.9906\n",
       "waic_se                                               0.0118671\n",
       "p_waic                                                  19.9994\n",
       "warning                                                       1\n",
       "waic_i        [1.0020417664912404, 0.9963607756633119, 0.999...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:934: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waic                                                    19.9906\n",
       "waic_se                                               0.0118671\n",
       "p_waic                                                  19.9994\n",
       "warning                                                       1\n",
       "waic_i        [1.0020417664912467, 0.9963607756633732, 0.999...\n",
       "waic_scale                                             deviance\n",
       "dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 ms ± 18.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/Desktop/arviz/arviz/stats/stats.py:934: UserWarning: For one or more samples the posterior variance of the log predictive\n",
      "        densities exceeds 0.4. This could be indication of WAIC starting to fail see\n",
      "        http://arxiv.org/abs/1507.04544 for details\n",
      "        \n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641 ms ± 10.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73 ms ± 61.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  waic_new(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.01 ms ± 87.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  az.stats.waic(school,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psislw'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"psislw\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def max_elem(data):\n",
    "    max = data[0]\n",
    "    for i in range(0,len(data)):\n",
    "        if data[i]>max:\n",
    "            max = data[i]\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.77662 s\n",
      "File: /home/banzee/Desktop/arviz/arviz/stats/stats.py\n",
      "Function: psislw at line 420\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   420                                           def psislw(log_weights, reff=1.0):\n",
      "   421                                               \"\"\"\n",
      "   422                                               Pareto smoothed importance sampling (PSIS).\n",
      "   423                                           \n",
      "   424                                               Parameters\n",
      "   425                                               ----------\n",
      "   426                                               log_weights : array\n",
      "   427                                                   Array of size (n_samples, n_observations)\n",
      "   428                                               reff : float\n",
      "   429                                                   relative MCMC efficiency, `ess / n`\n",
      "   430                                           \n",
      "   431                                               Returns\n",
      "   432                                               -------\n",
      "   433                                               lw_out : array\n",
      "   434                                                   Smoothed log weights\n",
      "   435                                               kss : array\n",
      "   436                                                   Pareto tail indices\n",
      "   437                                               \"\"\"\n",
      "   438         1         11.0     11.0      0.0      rows, cols = log_weights.shape\n",
      "   439                                           \n",
      "   440         1      12780.0  12780.0      1.6      log_weights_out = np.copy(log_weights, order=\"F\")\n",
      "   441         1         22.0     22.0      0.0      kss = np.empty(cols)\n",
      "   442                                           \n",
      "   443                                               # precalculate constants\n",
      "   444         1         30.0     30.0      0.0      cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
      "   445         1         27.0     27.0      0.0      cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
      "   446         1          2.0      2.0      0.0      k_min = 1.0 / 3\n",
      "   447                                           \n",
      "   448                                               # loop over sets of log weights\n",
      "   449      1001       2713.0      2.7      0.3      for i, x in enumerate(log_weights_out.T):\n",
      "   450                                                   # improve numerical accuracy\n",
      "   451      1000      20899.0     20.9      2.7          x -= np.max(x)\n",
      "   452                                                   # sort the array\n",
      "   453      1000      91302.0     91.3     11.8          x_sort_ind = np.argsort(x)\n",
      "   454                                                   # divide log weights into body and right tail\n",
      "   455      1000       5118.0      5.1      0.7          xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
      "   456                                           \n",
      "   457      1000       6410.0      6.4      0.8          expxcutoff = np.exp(xcutoff)\n",
      "   458      1000      12142.0     12.1      1.6          tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
      "   459      1000       2783.0      2.8      0.4          x_tail = x[tailinds]\n",
      "   460      1000       1940.0      1.9      0.2          tail_len = len(x_tail)\n",
      "   461      1000       1642.0      1.6      0.2          if tail_len <= 4:\n",
      "   462                                                       # not enough tail samples for gpdfit\n",
      "   463                                                       k = np.inf\n",
      "   464                                                   else:\n",
      "   465                                                       # order of tail samples\n",
      "   466      1000      15732.0     15.7      2.0              x_tail_si = np.argsort(x_tail)\n",
      "   467                                                       # fit generalized Pareto distribution to the right tail samples\n",
      "   468      1000      11023.0     11.0      1.4              x_tail = np.exp(x_tail) - expxcutoff\n",
      "   469      1000     454569.0    454.6     58.5              k, sigma = _gpdfit(x_tail[x_tail_si])\n",
      "   470                                           \n",
      "   471      1000       2856.0      2.9      0.4              if k >= k_min:\n",
      "   472                                                           # no smoothing if short tail or GPD fit failed\n",
      "   473                                                           # compute ordered statistic for the fit\n",
      "   474                                                           sti = np.arange(0.5, tail_len) / tail_len\n",
      "   475                                                           smoothed_tail = _gpinv(sti, k, sigma)\n",
      "   476                                                           smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
      "   477                                                               smoothed_tail + expxcutoff\n",
      "   478                                                           )\n",
      "   479                                                           # place the smoothed tail into the output array\n",
      "   480                                                           x[tailinds[x_tail_si]] = smoothed_tail\n",
      "   481                                                           # truncate smoothed values to the largest raw weight 0\n",
      "   482                                                           x[x > 0] = 0\n",
      "   483                                                   # renormalize weights\n",
      "   484      1000     131717.0    131.7     17.0          x -= _logsumexp(x)\n",
      "   485                                                   # store tail index k\n",
      "   486      1000       2901.0      2.9      0.4          kss[i] = k\n",
      "   487                                           \n",
      "   488         1          1.0      1.0      0.0      return log_weights_out, kss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp = LineProfiler()\n",
    "wrapper = lp(psislw)\n",
    "wrapper(data, 0.66)\n",
    "lp.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lets use our own gpd funcs'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lets use our own gpd funcs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psislw_new(log_weights, reff=1.0):\n",
    "    rows, cols = log_weights.shape\n",
    "\n",
    "    log_weights_out = np.copy(log_weights, order=\"F\")\n",
    "    kss = np.empty(cols)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = -int(np.ceil(min(rows / 5.0, 3 * (rows / reff) ** 0.5))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)  # pylint: disable=no-member, assignment-from-no-return\n",
    "    k_min = 1.0 / 3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(log_weights_out.T):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(x[x_sort_ind[cutoff_ind]], cutoffmin)\n",
    "\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        x_tail = x[tailinds]\n",
    "        tail_len = len(x_tail)\n",
    "        if tail_len <= 4:\n",
    "            # not enough tail samples for gpdfit\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x_tail_si = np.argsort(x_tail)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            x_tail = np.exp(x_tail) - expxcutoff\n",
    "            k, sigma = _gpdfit_new(x_tail[x_tail_si])\n",
    "\n",
    "            if k >= k_min:\n",
    "                # no smoothing if short tail or GPD fit failed\n",
    "                # compute ordered statistic for the fit\n",
    "                sti = np.arange(0.5, tail_len) / tail_len\n",
    "                smoothed_tail = _gpinv_new(sti, k, sigma)\n",
    "                smoothed_tail = np.log(  # pylint: disable=assignment-from-no-return\n",
    "                    smoothed_tail + expxcutoff\n",
    "                )\n",
    "                # place the smoothed tail into the output array\n",
    "                x[tailinds[x_tail_si]] = smoothed_tail\n",
    "                # truncate smoothed values to the largest raw weight 0\n",
    "                x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= _logsumexp(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "\n",
    "    return log_weights_out, kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,k = psislw_new(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = az.stats.psislw(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(k,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 ms ± 9.15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit psislw_new(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543 ms ± 4.62 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit az.stats.psislw(data,0.84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not much improvement'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Not much improvement'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
